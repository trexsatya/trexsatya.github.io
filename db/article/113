{"name":"Unsupervised Learning Algorithms","id":113,"content":"<p>Unlike Supervised Learning algorithms, Linear Regression for example, there are some learning algorithms which do not required labelled training data. You just pass the examples consisting of features and these algorithms classify them into different clusters.</p>\n\n<p><strong>K-Means Clustering Algorithm</strong></p>\n\n<div class=\"math\">c_i = k ; \\text{min}_k || x^{(i)} - \\mu_k ||</div>\n\n<div class=\"math\">\\mu_k = \\text{avg}(\\text{x&#39;s assigned to k})</div>\n\n<p>Repeat these two steps. <span class=\"math\">\\mu_k </span> are initially assigned randomly.</p>\n\n<p>&nbsp;</p>\n\n<p>The cost function, which we want to minimize, of K-means algorithm is:</p>\n\n<div class=\"math\">\\displaystyle \\frac { 1}{ m}\\sum _{i=1}^m || x^{(i)} - \\mu_{c^{(i)}}||^2</div>\n\n<p>The first step in the above algorithm minimises the cost function w.r.t. <span class=\"math\">c^{(i)}</span>. The second step minimizes w.r.t. <span class=\"math\">\\mu_i</span></p>\n\n<p>&nbsp;</p>\n\n<p>If features are correlated, we can collapse them into one, hence reducing the dimensionality. Reducing the dimensionality also helps in visulaization.</p>\n\n<p><strong>Principal Component Analysis</strong></p>\n\n<p>Find a direction onto which to project the data so as to minimize the projection error i.e. choose a surface (1D or m-dimensional) such that it does not introduce huge difference from projected data points.</p>\n\n<p>PCA Algorithm</p>\n\n<p>Preprocessing: Feature scaling / mean normalization, just like we had in Supervised Learning.</p>\n\n<p>1. Compute Co-variance matrix</p>\n\n<div class=\"math\">\\Sigma = \\displaystyle \\frac{1}{m} \\sum_{i=1}^n ( x^{(i) } x^{(i)^T})</div>\n\n<p>2. Compute &quot;Eigenvectors&quot; of matrix <span class=\"math\">\\Sigma</span></p>\n\n<pre>\n<code class=\"language-python\">[U, S, V] = svd(Sigma)</code></pre>\n\n<p>svd = Singular Value Decomposition.</p>\n\n<p>svd reduces <span class=\"math\">U \\in {\\Bbb R}^{n\\text x n}</span> to <span class=\"math\">U \\in {\\Bbb R}^{n\\text x k}</span>. <span class=\"math\">U^T X \\in {\\Bbb R}^{k\\text x 1}</span></p>\n\n<p>&nbsp;</p>\n\n<p>It is possible to get back the data in original dimensions from the compressed data. You don&#39;t get the exact data, but an approximation.</p>\n\n<p>Typically you should choose&nbsp;<span class=\"math\">k</span> the number of principle components (number of dimensions after reducing) such that</p>\n\n<div class=\"math\">&nbsp;\\displaystyle \\frac{ \\frac{1}{m} \\Sigma_{i=1}^m || x^{(i)} - x_{\\text{approx}}^{(i)} ||^2 }{ \\frac{1}{m}\\Sigma_{i=1}^m || x^{(i)} ||^2 } \\le 0.01</div>\n\n<p>which can be paraphrased as &quot;99% of variance is retained&quot;.</p>\n\n<p>The svd function used above also returns <span class=\"math\">S \\in {\\Bbb R}^{n\\text x n}</span>, which can be used to compute how much variance is retained in a simpler way.</p>\n\n<div class=\"math\">\\displaystyle 1 - \\frac{ \\sum_{i=1}^k S_{ii}}{ \\sum_{i=1}^n S_{ii}}</div>\n\n<p>PCA might help in case of solving &quot;overfitting&quot;, but it is not a correct use of it, instead you should use Regularization for that purpose.</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n","authorId":null,"subject":"ai","tags":null,"img":null,"summary":null,"lastUpdated":null}