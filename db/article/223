{"name":"Distributed Cache","id":223,"content":"<p>Let&#39;s say you just loaded something from database and shown it to user. Another user also wants to see the same info at around the same time. Would you load the same data again from DB? - No, loading from DB is costly, time-consuming.</p>\n\n<p>So instead of loading again and again, we can store the loaded data into main memory for some time. How much time? - That depends on your use cases and the type of data you have loaded. If you are sure that the data loaded is not going to change for the whole day, you can keep it in memory for the whole day.</p>\n\n<p>Also, memory is not unlimited. There will come a time when there is no space left in memory to store the loaded data, so you might want to delete some previously loaded data. Which one? - That again depends on your scenarios and the type of data.</p>\n\n<h2>Local Cache vs Distributed Cache</h2>\n\n<p>Designing and implementing a local cache is an algorithimic problem. Designing and implementing a Distributed Cache is an architectural problem.</p>\n\n<p>Apart from being fast, a Distributed Cache has some additional non-functional requirements: Scalability, High Availability.</p>\n\n<p>Functional requirements of a cache are:</p>\n\n<ol>\n\t<li>Storing key-value in cache</li>\n\t<li>Retrieving value for given key</li>\n\t<li>Evicting entry when necessary. (There are many ways for this e.g. LRU (Least Recently Used), Time based eviction etc)</li>\n</ol>\n\n<p>We can implement a LRU local cache using HashMap with a Doubly Linked List.</p>\n\n<p>To make it distributed, we have to think about trade-offs and many options:</p>\n\n<p><strong>Where should the cache be located, on the same server (Co-Located) as application or different server?</strong> Co-located cache means flexibility to use optimized hardware for caching requirements, cache can itself be used as a service by many applications but at the cost of extra hardware and corresponding maintenance.</p>\n\n<p><strong>How many servers can host cache? (Sharding and Replication)</strong></p>\n\n<p>Sharding requires some form of Consistent Hashing.</p>\n\n<p>Cache Clients know (or compute) which shard (host) to call for an operation. Each cache client knows about the list of servers (shards). Clients need to know when server (shard) is added or removed. This needs to be consistent (every client must have same list of servers). So we need to have some sort of configuration service (e.g. ZooKeeper). Cache clients need to fetch list of servers available from configuration service (which will be run on each cache server).</p>\n\n<p>But even with Shards in place, cache system can be slow in case when most of the requests are going to the same shard. (For example if your cache stores URL shortening info like in Bit.ly and a URL has become popular that million people are clicking on the same URL at the same time)</p>\n\n<p>Also, at this point you don&#39;t have High Availability, if one of the shards is having high load, it can become unavailable for some queries which are in queue/timed-out.</p>\n\n<p>Replication can solve the problem of High Availability. You have to choose between Eventual Consistency (probabilistic protocols e.g <a href=\"https://en.wikipedia.org/wiki/Gossip_protocol\" target=\"_blank\">Gossip</a>, Epidemic Broadcast Trees, BiModal Multicast etc) vs Strong Consistency (e.g. Two or Three Phase Commit, RAFT protocols, <a href=\"https://en.wikipedia.org/wiki/Paxos_(computer_science)\" target=\"_blank\">Paxos</a>, Chain Replication etc) when you decide to implement replication.</p>\n\n<p>Master-Slave Configuration: Slaves are used only for reads, and master is used for both reads and writes. If master goes down, one of the slaves is promoted to the master position. Each write to master is replicated to all the slaves <strong><em>asynchronously</em></strong>. Who promotes them? - Configuration Service can do this task.</p>\n\n<p>But even with replication, we have points of failures. What if master goes down before data is replicated to all the slaves?</p>\n\n<p>Master-Slave replication introduces Consistency problem. So a read from master may return different result than a read from one of the slaves (We can make replication a synchronous operation at some cost). There can be inconsistency because of inconsistent lists of servers. It is possible that a client writes values that no other client can read (We can handle synchronization of this list at some cost).</p>\n\n<p>Cache Clients can have local view of cache (local cache) along with remote cache, so that remote cache is consulted only if local cache does not have data.</p>\n\n<p><strong>Other non-functional requirements:</strong></p>\n\n<p>Security: Security requirements are minimal for caching services, as they are generally used in secure environment.</p>\n\n<p>Monitoring and Logging.</p>\n\n<p>Consistent Hashing suffers from Domino Effect. Also the distribution of load among servers is complex task. We might need to consider other algorithms e.g Jump Hash Algorithm, Proportional Hashing etc.</p>\n\n<p>&nbsp;</p>\n","authorId":null,"subject":"architecture","tags":null,"img":null,"summary":null,"lastUpdated":null}