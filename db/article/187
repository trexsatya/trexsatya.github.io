{"name":"Probability","id":187,"content":"<p>Source: <a href=\"https://youtu.be/FJd_1H3rZGg?list=PLLVplP8OIVc8EktkrD3Q8td0GmId7DjW0\" target=\"_blank\">Harvard Youtube</a></p>\n\n<h3>History of Probability:</h3>\n\n<p>Regression Towards The Mean (RTTM) by Darwin&#39;s brother, analyzing the heights of people across generations.</p>\n\n<p>Daniel Kahneman&#39;s Quote on RTTM</p>\n\n<p>Gambling:</p>\n\n<p><a href=\"https://en.wikipedia.org/wiki/Problem_of_points\" target=\"_blank\">&nbsp; &nbsp;Fermat-Pascal Correspondence</a></p>\n\n<p>Sample Space: All possible outcomes of an &quot;experiment&quot; (anything that produces outcomes).</p>\n\n<p>Event: is a subset of Sample&nbsp;Space.</p>\n\n<blockquote>\n<p>In the domain of Probabilities, mostly intuitions turn out to be wrong.</p>\n</blockquote>\n\n<p>Binomial Coefficient nCk</p>\n\n<p>Probability of Full-House in cards, assuming the cards are randomly shuffled.</p>\n\n<h2>Counting</h2>\n\n<p><img alt=\"\" height=\"300/\" src=\"/images/prob_counting_quadrant.png\" width=\"100%\" /></p>\n\n<p>While doing research, or solving a problem, after listing down trivial examples, find and work on the <strong><em>&quot;simplest non-trivial example&quot;</em></strong>.</p>\n\n<p>&nbsp;</p>\n\n<p>Getting intuition for these counting formulae:</p>\n\n<p><strong><em>&quot;In how many ways can you choose k elements from n given items, if you are not allowed to replace the chosen item, that is you can choose one item only once, the order of choices matters&quot;</em></strong></p>\n\n<p>For the first time, you have n options (you can start from any item), after you have chosen one you have (n-1) options, after you have chosen 2 you have (n-2) options,.... and so on till you have chosen k items.</p>\n\n<p><strong><em>&quot;In how many ways can you choose k elements from n given items, if you are not allowed to replace the chosen item, that is you can choose one item only once, the order of choices does not matter&quot;</em></strong></p>\n\n<p>Similar to the above, but notice here that because the order of choice does not matter, you actually have overcounted k! times, because for a&nbsp;choice you could select any of the remaining, however since the order doesn&#39;t matter they will be counted only once.</p>\n\n<p><strong><em>&quot;In how many ways can you choose k elements from n given items, if you are allowed to replace the chosen item, that is you can choose one item only once, the order of choices does not matter&quot;</em></strong></p>\n\n<p>Let&#39;s take the simplest non-trivial example, if we have 2 items, and we are to choose k times. Let&#39;s say we choose the first item x times, and the second item (k-x) times. By visualizing it like x balls in one box, and (k-x) balls in the other box, we can transform the problem into another statement: &quot;how many ways can we choose the value of x?&quot;</p>\n\n<p>To extend it further for n = 4, k = 9; Let&#39;s visualize one possible choice, [] represents a box</p>\n\n<div style=\"padding: 5px; color: blue\">\n<p>[ o o o ] [ o o o o] [ ] [ o o ]</p>\n</div>\n\n<p>This can be encoded as&nbsp;&nbsp;</p>\n\n<div style=\"padding: 5px; color: blue\">\n<p>&nbsp;o o o | o o o o | | o o&nbsp;</p>\n</div>\n\n<p>Now we can rephrase the problem and ask &quot;in how many ways can choose the position of separators(|)&quot;?</p>\n\n<p>There are (n-1) separators, there are (n+k-1) total positions. Or, we could ask&nbsp;&quot;in how many ways can choose the position of dots(o)&quot;?</p>\n\n<p><strong>Proof by interpretation/story:-</strong></p>\n\n<p><span class=\"math\">\\dbinom{n}{k} = \\dbinom{n}{n-k}</span> Basically, when you are picking k out of n, you can also see it as picking other (n-k) of of n</p>\n\n<p><span class=\"math\">n\\dbinom{n-1}{k-1} = k\\dbinom{n}{k}</span> You can create a story like this: we have n people, we want to make a club of k people of which one would be the President. You can see it in two ways: Either choose k people and then you have k options for selecting the president. Or, choose the president for which you have n options, and then select remaining (k-1) club members from remaining (n-1) people.</p>\n\n<p><span class=\"math\">\\dbinom{m+n}{k} = \\sum_{j=0}^k \\dbinom{m}{j} \\dbinom{n}{k-j}</span> Imagine that you have two groups of people one containing m number of people and the other containing n number of people. Then, you can choose x from group1, and (k-x) from group2, and multiply those; now x can be from 0 to k and all these are disjoint scenarios, so we are not repeating anything or double-counting, so just sum them up. It is equal to saying that we chose k people from (m+n) people.</p>\n\n<p><strong>Definition of Probability:</strong></p>\n\n<p>A Probablility Sample consists of S&nbsp;and&nbsp;P; where S = Sample Space</p>\n\n<div class=\"math\">P: A \\subseteq S \\to [0,1] \\text{ such that}</div>\n\n<div class=\"math\">\\text{1. } P(\\phi)=0 \\text{ it&#39;s impossible}, P(S) = 1</div>\n\n<div class=\"math\">\\text{2. } P(\\cup_{n=1}^{\\infty}A_n) = \\sum_{n=1}^{\\infty}P(A_n) \\text{ if events are disjoint (non-overlapping)}</div>\n\n<p>A counter-intuitive case of probabilities: <a href=\"https://en.wikipedia.org/wiki/Birthday_problem\" target=\"_blank\">Birthday Problem</a>&nbsp;that it is very likely that if 23 or more people are in a group, at least two of them have the same birthday.</p>\n\n<p><a href=\"https://en.wikipedia.org/wiki/Inclusion%E2%80%93exclusion_principle\" target=\"_blank\">Inclusion-Exclusion Principle</a></p>\n\n<p><a href=\"https://www.encyclopediaofmath.org/index.php/Montmort_matching_problem\" target=\"_blank\">Montmort_matching_problem</a></p>\n\n<p>&nbsp;</p>\n\n<p><strong>Disjointness:</strong> If A occurs, B cannot occur =&gt; A,B are <em>disjoint</em>.</p>\n\n<p><strong>Independence:</strong> If A occurs, it tells nothing about whether B occurs or not =&gt; A,B are independent.</p>\n\n<div class=\"math\">P(A \\cap B \\cap C) = P(A) P(B) P(C) \\text{ if A,B,C are independent}</div>\n\n<p><a href=\"https://en.wikipedia.org/wiki/Newton%E2%80%93Pepys_problem\" target=\"_blank\">Newton_Pepys_problem</a></p>\n\n<p><strong>Conditional Probability:</strong></p>\n\n<p>Practically it can be used to update beliefs based on new evidences.</p>\n\n<div class=\"math\">P(A|B) = \\frac{P(A \\cap B)}{P(B)}, \\text{ if } P(B) \\gt 0</div>\n\n<p>When you perform an experiment many times (how many? Is it even possible to performs <em>same</em>&nbsp;experiment many times?), there will be times when you see B occurs, let&#39;s call those special, just take the fraction of how many times A occurs among those special experiments.&nbsp;</p>\n\n<p>Bayes Theorem just follows from this easily.</p>\n\n<div class=\"math\">P(A|B) = \\frac{P(A \\cap B)}{P(B)}, \\implies P(A \\cap B) = P(A|B) P(B) = P(B|A) P(A) \\implies P(A|B) = \\frac{P(B|A) P(A)}{P(B)}</div>\n\n<p><strong>Another example where probablities go against intuition:</strong></p>\n\n<p>Let&#39;s say there&#39;s a disease out there which afflicts 1% of the population. A man goes for the test, and is found positive in the test. The doctors have advertised that their tests has 95% accuracy. What are the chances that the patient actually has the disease?</p>\n\n<p>In a survey, even the doctors (more than 70% of them) in Harvard guessed that there should be around 90-95% chance that the patient has disease.</p>\n\n<p>Let&#39;s see what the mathematics says:-</p>\n\n<p>Lets, D = Patient has disease</p>\n\n<p>&nbsp; &nbsp; &nbsp; &nbsp; T = Patient has tested positive.</p>\n\n<div class=\"math\">\\text{The test has 95% accuracy } \\implies P(T|D) = 0.95 = P(T^c|D^c); P(T|D^c) = 0.05</div>\n\n<p>Now apply Bayes rule:-</p>\n\n<p><span class=\"math\">P(D) = \\frac{P(T|D)P(D)}{P(T)} </span></p>\n\n<div><a href=\"https://en.wikipedia.org/wiki/Law_of_total_probability\" target=\"_blank\">Law of total probability</a>&nbsp;<span class=\"math\">\\implies P(D) = \\frac{P(T|D)P(D)}{ P(T|D)P(D) + P(T|D^c) P(D^c) } \\implies 0.16</span> Which is far from what is guessed normally.</div>\n\n<p>When guessing, we focus on the accuracy of the test, we do not focus that the fact that only 1% of the population has disease will impact the chances, no matter how accurate the test is (except in the case that it is 100%)</p>\n\n<p>Biohazards:</p>\n\n<ol>\n\t<li>&nbsp;Confusing P(A | B) with P(B | A) Ex.&nbsp;<a href=\"https://understandinguncertainty.org/node/545\" target=\"_blank\">Prosecutor&#39;s Fallacy</a></li>\n\t<li>Confusing Prior P(A) with Posterior P(A | B)</li>\n\t<li>Confusing Independence with Conditional Independence</li>\n</ol>\n\n<p>More examples where probability goes against intuition:&nbsp;<a href=\"https://en.wikipedia.org/wiki/Monty_Hall_problem\" target=\"_blank\">Monty Hall Problem</a>, <a href=\"https://en.wikipedia.org/wiki/Simpson's_paradox\" target=\"_blank\">Simpson&#39;s Paradox</a></p>\n\n<p><a href=\"https://en.wikipedia.org/wiki/Gambler%27s_ruin\">Gambler&#39;s Ruin Problem</a></p>\n\n<p>&nbsp;</p>\n\n<div style=\"background-color: #f3f3f3; color: #17175f; padding: 2%; border-radius: 10px;\">\n<p>Probability of an event: How likely is a particular even to occur?</p>\n\n<p>Random Variable: To make trials more abstract, we classify them by assigning numbers to their outcomes. <strong><em>Random Variables</em></strong> are those functions which do this. Consider them as mathematical representation (in form of a function) of a single&nbsp;experiment.</p>\n\n<p>We can list the probability of such Random Variables one by one, OR we can create a function which can abstractly define these probabilities. Such functions are called Probability Mass Function&nbsp; <em><strong>PMF&nbsp;</strong></em>(in case outcomes of RVs are discrete i.e Integers), Probability Density Function <em><strong>PDF&nbsp;</strong></em>(in case outcomes of RVs are continuous i.e. Real Numbers).&nbsp;</p>\n\n<p>Sometimes, we are interested in likelihood of each possible outcome, not just a single event. The above functions PMF, PDF give us an abstract formula to describe probabilities of all possible outcomes. This is called Probability Distribution.</p>\n\n<p>Now, sometimes you may not be interested in the whole distribution, but only want a summary. Expectation Value (<strong><em>Mean</em></strong>) gives you a single number summary for a distribution. <em>&quot;On an average this is the <strong>likelihood</strong> of a random event, given that this event is one of those events distributed according to an assumed PDF/CDF&quot;</em></p>\n\n<p>Ok, maybe you have Mean (Expectation Value) summary, but you might also want to know how much can the likelihood vary from this mean. That is captured by <em><strong>Variance</strong></em>.</p>\n\n<p><strong><em>All of this is about getting some general idea about the whole population when empirically you only have a sample from the (large) population.</em></strong></p>\n</div>\n\n<p>&nbsp;</p>\n\n<p><a href=\"https://en.wikipedia.org/wiki/Random_walk\" target=\"_blank\">Random Walk</a>&nbsp;</p>\n\n<p>What is a random variable?</p>\n\n<p>&nbsp; &nbsp; &nbsp; - A function from the Sample Space (of random experiments)&nbsp;to the Reals (real numbers).</p>\n\n<p><strong>Indicator Random Variable:</strong></p>\n\n<div class=\"math\">I_A(\\omega) = I(\\omega \\in A) = \\begin{cases} 1 \\text{ if } \\omega \\in A \\\\ 0 \\text{ otherwise } \\end{cases}</div>\n\n<p>IRVs are helpful in some theorems.</p>\n\n<p>&nbsp;</p>\n\n<p><strong>Bernoulli Distribution</strong></p>\n\n<p>X is said to have B.D. iff X has only two possible values i.e. 0 and 1; and P(X=1) = p&nbsp;and P(X=0) = 1- p</p>\n\n<p><strong>Binomial Distribution</strong></p>\n\n<p>If we have n Bernoulli trials (experiments which can only have 2 possible outcomes), the distribution of number of successes (i.e&nbsp;ones if you consider 1 as success) is called Binomial Distribution.</p>\n\n<div class=\"math\">\\text{P} (X=k) = \\binom{n}{k} p^k (1-p)^{n-k}</div>\n\n<p>The above is called <strong>PMF (Probability Mass Function)</strong>. Just see that we have k trials where we had success, each with probability p, (n-k) trials with failure each with probability (1-p); but they can appear in any order, so count those arrangements. <em>(Note: For a PMF to be valid the numbers of sum must be non-negative, and they must sum up to 1)</em></p>\n\n<p>Why is it called Binomial Distribution? - If you add P(X=i) from i=0 to n it will give you the Binomial theorem expression.</p>\n\n<p>&nbsp;</p>\n\n<p><a href=\"https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables\" target=\"_blank\">Independent and Identically Distributed Random Variables</a></p>\n\n<p><i>&nbsp;</i></p>\n\n<p><strong>Cumulative Distribution Function</strong></p>\n\n<p>Like we said that Random Variables are functions from Samples Space to Reals viz &quot;assigning a numerical to trials/experiments outcomes&quot;. We can say that <span class=\"math\">(X=k) </span>&nbsp;is&nbsp;an event and it occurs whenever value k is assigned to a trial&#39;s outcome.</p>\n\n<p>Similarly&nbsp;<span class=\"math\">(X \\le k)</span> is event.</p>\n\n<div class=\"math\">\\text{CDF } F(x) = \\text{P}(X \\le x)</div>\n\n<p>Notice that PMF is for discrete variables, and CDF can be for any random variables (discrete or continuous).&nbsp;</p>\n\n<p>Any function&nbsp;<span class=\"math\">F: \\Bbb R \\to [0,1]</span> can be said to be CDF iff,&nbsp;</p>\n\n<div class=\"math\">\\begin{cases} x_1 \\le x_2 \\implies F(x_1) \\le F(x_2) \\\\ \\lim \\limits_{x \\to -\\infty} F(x) = 0,\\lim \\limits_{x \\to \\infty} F(x) = 1 \\\\F(x) = \\underset{ y \\gt x}{\\lim \\limits_{y \\to x}} F(y) \\end{cases}</div>\n\n<p><a href=\"https://en.wikipedia.org/wiki/Hypergeometric_distribution\" target=\"_blank\">Hypergeometric_distribution</a>&nbsp;: Take the sample space (N), tag some of them (T), choose some of those (n); find the distribution of events that (X=k) from the chosen ones. E.g. while studying animals in wild, you tagged some of those, later you could catch&nbsp;only few of those tagged to study. This is not binomial,&nbsp;<i>because the probability of success on each trial is not the same, as the size of the remaining population changes as we catch an animal.</i></p>\n\n<div class=\"math\">\\text{P}(X=k) = \\frac{ \\binom{T}{k} \\binom{N-T}{n-k} }{ \\binom{N}{n}}</div>\n\n<p>&nbsp;</p>\n\n<p><strong>Average of a discrete R.V</strong></p>\n\n<p>If we see this way finding averages of a normal set of numbers</p>\n\n<div class=\"math\">\\text{Avg}(1,1,1,1,2,2,2,3,3) = \\underbrace{ \\frac{4}{9} }_{\\text{weight of group of ones}} 1 + \\frac{3}{9}2 + \\frac{2}{9}3</div>\n\n<p>We can draw the analogy to average&nbsp;for R.Vs</p>\n\n<div class=\"math\">\\text{Avg } \\text{E}(x) = \\sum_x x \\text{P}(X=x)</div>\n\n<p>An interesting property of Expectations (Averages) is Linearity&nbsp;<span class=\"math\">\\text{E}(X+Y) = \\text{E}(X) + \\text{E}(Y)</span> even if X and Y are <em>dependent</em>.</p>\n\n<p>However, for multiplicity&nbsp;<span class=\"math\">\\text{E}(\\prod_{i=1}^nX_i) = \\prod_{i=1}^n \\text{E}(X_i)</span>&nbsp;only if RVs are <strong><em>independent</em></strong>.</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;To see how it is interesting, let&#39;s try to find Average of R.V which are Binomially distributed Bin(n, p)</p>\n\n<p>&nbsp; Binomial Distrbn B(n, p) means that there are n Bernoulli trials, and each one has probability p; So, E(X) = sum of Expected values of all Bernoulli trials</p>\n\n<p>&nbsp;</p>\n\n<div class=\"math\">\\text{And, for Bernoulli trials } \\text{E}(X) = 1 \\text{P}(X=1) + 0 \\text{P} (X=0) = \\underbrace{p}_{\\text{by definition}}</div>\n\n<p>Which means that for Binomial Distribn <span class=\"math\">\\text{Bin}(n, p)</span> &nbsp; <span class=\"math\">\\text{E}(X) = np</span></p>\n\n<p>&nbsp;</p>\n\n<p><strong>How is it that Linearity is true even for independent RVs?</strong></p>\n\n<p>To see how, let&#39;s see Averages and PMF from another perspective.</p>\n\n<p>Remember RVs are function from sample space to real number. Which means that we assign certain outcomes of trials number 1, certain outcomes of trials number 2, and so on. This is sort of classification.</p>\n\n<p>And we say&nbsp;<span class=\"math\">\\text{E}(X) = \\sum_x x\\text{P}(X=x)</span>&nbsp;and which is like finding the average by looking at the mass of classes; Instead we can look at the mass of individuals. And so, even if X,Y in E(X+Y) are dependent, the individuals can be seen as&nbsp;independent. We can say&nbsp;<span class=\"math\">\\text{E}(X) = \\sum_s X(s) \\text{P}({s})</span>&nbsp;s is individual trial, X(s) points to the numerical value of that trial since X is a function.&nbsp;</p>\n\n<div class=\"math\">\\text{E}(X+Y) = \\sum_s (X+Y)(s) \\text{P}({s}) = \\sum_s X(s) \\text{P}({s}) + \\sum_s Y(s) \\text{P}({s}) \\implies \\text{E}(X) + \\text{E}(Y)</div>\n\n<p>&nbsp;</p>\n\n<p><a href=\"https://en.wikipedia.org/wiki/Negative_binomial_distribution\" target=\"_blank\">Negative Binomial Distribution</a></p>\n\n<p>To find its Avg just see that what you actually have is a series of r number of (independent) <a href=\"https://en.wikipedia.org/wiki/Geometric_distribution\" target=\"_blank\">Geometric Distributions</a>, and since E(X) for G.D is n/p, E(X) for N.B.D would be r.n/p by the virtue of Linearity.</p>\n\n<p><a href=\"https://plato.stanford.edu/entries/paradox-stpetersburg/\" target=\"_blank\">St&nbsp;Petersburg Paradox</a></p>\n\n<p>&nbsp;</p>\n\n<div style=\"margin-left: 5%; color: grey;\">\n<p>Problem: Find the expected number of local maxima (i.e on average how many local maxima), if we permute numbers {1, 2, 3, ... 7} randomly.</p>\n\n<p>&nbsp;Let&#39;s see a possible permutation: 3, 2, 1, 4, 7, 5, 6</p>\n\n<p>&nbsp;Here 3, and 6 are local maxima because they have only one neighbour and that neighbor is smaller. 7 is local maxima as well, because it has two neighbours and both are smaller.</p>\n\n<p>&nbsp;Let&#39;s say&nbsp;<span class=\"math\">I_j</span>&nbsp;is <a href=\"https://en.wikipedia.org/wiki/Indicator_function\" target=\"_blank\">Indicator Random Variable</a> of position j having a local maxima i.e. it represents the event of position j having a local maxima, it will be 1 if it does have local maxima, otherwise will be equal to 0.</p>\n\n<p>What we want is&nbsp;<span class=\"math\">\\text{E}(I_1 + I_2 + I_3 + .... I_n)</span>&nbsp;Now for a given triplet in the permutation, the probability of a local maxima is&nbsp; <span class=\"math\">\\frac{2}{6}</span>&nbsp;E.g. in the above permutation&nbsp;3, 2, 1, 4, 7, 5, 6; taking the triplet = (4, 7, 5) 7 would have been a local maxima even if this triplet was like (5, 7, 4), and there are 6 possible arrangements for this triplet, hance 2/6.</p>\n\n<p>Expected value of a Indicator Random Variable is the probability of the event for which it is indicator. So, there are (7-2) possible candidates for local maxima with two neighbours each one with probability of 2/6. Also, there are two end-points which have only one neighbor, and in those cases the probability of the end-point being a local maxima is 50% each.&nbsp;</p>\n\n<p>So, by virtue of Linearity:&nbsp;<span class=\"math\">\\text{E}(I_1 + I_2 + ... + I_7) \\implies (7-2)\\frac{2}{6} + 2\\frac{1}{2}</span>&nbsp;</p>\n</div>\n\n<p>&nbsp;</p>\n\n<p>Don&#39;t confuse Random Variables with Expectations. Expectations are like <em>blueprint</em>, and RVs are like <em>houses</em> based on that blueprint.</p>\n\n<h2><a href=\"https://en.wikipedia.org/wiki/Poisson_distribution\" target=\"_blank\">Poisson Distribution</a>:</h2>\n\n<div class=\"math\">\\text{P}(X=k) = \\frac{\\text{e}^{-\\lambda} \\lambda^k }{k!} \\text{ ; } k = \\{0, 1, 2, 3, 4, ....\\}</div>\n\n<p>Find Expected value of this</p>\n\n<div class=\"math\">\\text{E}(X) = \\sum_{i=0}^{\\infty}k \\text{P}(X=k) \\implies \\text{e}^{-\\lambda } \\sum_{i=0}^{\\infty}k \\frac{\\lambda^k}{k!}</div>\n\n<p>Notice that k=0 makes whole term zero, so we can start the sum from 1 instead of 0. Also, one k in numerator cancels one term of the factorial.</p>\n\n<div class=\"math\">\\implies \\text{e}^{-\\lambda } \\sum_{i=1}^{\\infty} \\frac{\\lambda^k}{(k-1)!}</div>\n\n<p>Now, we can see the similarity with <a href=\"https://en.wikipedia.org/wiki/Taylor_series\" target=\"_blank\">Taylor Series</a>. Taking one lambda out, we get an exact Taylor Series expression on the right hand side.&nbsp;<span class=\"math\">\\implies \\lambda \\text{e}^{-\\lambda} \\text{e}^{\\lambda} \\implies \\lambda</span></p>\n\n<p>Applications of Poisson:</p>\n\n<p>In scenarios where we are counting number of successes; and the number of trials is large; and each trial has small possibility of success. Events are independent or <em>&quot;weakly dependent&quot;</em></p>\n\n<p>Events:&nbsp;<span class=\"math\">\\{ A_1, A_2, ... A_n \\}</span>&nbsp; <span class=\"math\">\\text{P}(A_j) = p_j</span>&nbsp;; n is large;&nbsp;<span class=\"math\">p_j</span>&nbsp;are small.</p>\n\n<p>In such scenarios the number of successes is <strong><em>approximately</em></strong> Poisson&nbsp; and&nbsp;<span class=\"math\">\\lambda = \\sum_{j=1}^{\\infty} p_j</span>&nbsp;</p>\n\n<p>Binomial Distribution converges to Poisson if n is large.</p>\n\n<div style=\"margin-left: 5%; color: grey;\">\n<p>Problem: Let&#39;s find the approximate probability that at least three people have common birthday in a large gathering not necessarily very very large.</p>\n\n<p>Notice that if n is even around 10, just like in Birthday Problem, it&#39;s not the n that matters, its n-choose-3 that matters; so even if n is around 10,&nbsp;<span class=\"math\">\\binom{n}{3}</span>&nbsp;is large.</p>\n\n<p>Let&#39;s see a possible triplet (i, j, k) of people. Let&#39;s choose an Indicator Random Variable&nbsp;<span class=\"math\">I_{ijk} \\text{ ; } i \\lt j \\lt k</span>&nbsp;. i &lt; j &lt;k, just to avoid counting multiple times. Expected Value of number of triple matches:-</p>\n\n<p>Choose a triplet, the first person can have any birthday, the second and third one would have to have that same birthday, so their probability has to be 1/365&nbsp;<span class=\"math\">\\implies \\binom{n}{3} \\frac{1}{365}^2</span>&nbsp;</p>\n\n<p>We can use Poisson approximation because n-choose-3 is large enough, triplets are &quot;weakly dependent&quot; (even though it doesn&#39;t matter much; weakly independent because if we have found a triplet {1, 2, 3} matching birthday, and then consider {2, 3, 4}, we do have some partial information that {2,3 } have same birthday already)</p>\n\n<p>So it is approximately Poisson with&nbsp;<span class=\"math\">\\lambda = \\binom{n}{3} \\frac{1}{365}^2</span>&nbsp;</p>\n\n<div class=\"math\">\\text{P}(X \\ge 1) = 1 - \\text{P}(X = 0) \\approx 1- \\text{e}^{-\\lambda} \\frac{\\lambda^0}{0! } \\implies 1 - e^{-\\lambda}</div>\n</div>\n\n<h3>Continuous Distributions</h3>\n\n<p>For continuous Random Variables f(x) is a PDF (Probability Density Function) if&nbsp;<span class=\"math\">\\text{P}(a \\le x \\le b) = \\int_a^b f(x) \\text{d}x \\text{ ; for all a,b}</span>&nbsp;&nbsp;The validity condition for CDF is similar to that of PMF, just that here is integral instead of sum.&nbsp;<span class=\"math\">f(x) \\ge 0; \\int_{-\\infty}^\\infty f(x) \\text{d}x = 1</span></p>\n\n<p>To think about it intuitively, imagine a continuous function f(x), then for an arbitrary point&nbsp;<span class=\"math\">x=x_0</span>&nbsp;<span class=\"math\">f(x_0) \\epsilon \\approx P(x \\in (x_0 - \\frac{\\epsilon}{2}, x_0 + \\frac{\\epsilon}{2})) \\text{ for } \\epsilon \\gt 0 \\text{ but very small length}</span>&nbsp;</p>\n\n<p>If f(x) is PDF, then the CDF would be&nbsp;<span class=\"math\">\\text{F}(x) = \\text{P}(X \\le x) = \\int_{-\\infty}^x f(t) dt</span>&nbsp;t is a dummy variable just to avoid any confusion with x.</p>\n\n<p>To find PDF from CDF, just take the derivative. (True by <a href=\"https://en.wikipedia.org/wiki/Fundamental_theorem_of_calculus\">Fundamental Theorem of calculus</a>) And by the same theorem, another relationship between PDF and CDF:</p>\n\n<div class=\"math\">\\text{P}(a \\le x \\le b) = \\int_a^b f(x) \\text{d}x = \\text{F}(b) - \\text{F}(a)</div>\n\n<p>Expected Value of a Continuous RV in general is:&nbsp;<span class=\"math\"> \\int_a^b x f(x) \\text{d}x </span></p>\n\n<p><strong><a href=\"https://en.wikipedia.org/wiki/Variance\" target=\"_blank\">Variance</a>:</strong></p>\n\n<p>Intuitively, it tells us &quot;on average, how far is x from its mean&quot;, it tells us about the spread of the RV.</p>\n\n<div class=\"math\">\\text{Var}(X) = \\text{E}((X - E(X))^2)</div>\n\n<p>Why square? - Because otherwise&nbsp;E(X - E(X)) would result in zero by virtue of Linearity.</p>\n\n<p>But, since we have squared it, we have also changed the metric system then. To compensate for that we take square root of it, and call it <a href=\"https://en.wikipedia.org/wiki/Standard_deviation\" target=\"_blank\">Standard Deviation</a>.</p>\n\n<p>Variance has following properties: if&nbsp;<span class=\"math\">\\mu</span>&nbsp;is the mean of RV</p>\n\n<ol>\n\t<li><span class=\"math\">\\text{Var}(X) = \\text{E}(X^2) - \\mu^2</span>&nbsp;</li>\n\t<li>If a, b are constants then&nbsp;<span class=\"math\">\\text{Var}(aX + b) = a^2 \\text{Var}(X)</span>&nbsp;</li>\n\t<li><span class=\"math\">\\text{Var}( \\sum_{i=1}^n a_iX_i ) = \\sum_{i=1}^n a_i^2\\text{Var}(X_i)</span></li>\n</ol>\n\n<p>&nbsp;</p>\n\n<p><a href=\"https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)\" target=\"_blank\">Uniform Distribution</a></p>\n\n<p>Let&#39;s consider that a line represents continuous probability density in the interval (a,b). Now, uniform means that&nbsp;<span class=\"math\">\\text{Prob } \\alpha \\text{ Length of a segment in interval}</span>&nbsp; i.e. if length of the segments is same, the probability will also be same.</p>\n\n<p>Let&#39;s find PDF for this &mdash;&nbsp;the probability outside the interval is zero, and inside the interval is constant (by virtue of <em>uniform</em>); also, note that to be a valid PDF it must be equal to one.</p>\n\n<div class=\"math\">f(x) = \\begin{cases} \\text{c} \\text{ if } a \\le x \\le b \\\\ 0 \\text{ otherwise} \\end{cases} \\implies 1 = \\int_a^b \\text{c } \\text{d}x \\implies c = \\frac{1}{b-a}</div>\n\n<p>Let&#39;s find CDF for this &mdash; Defintion of CDF tells us to integrate from -infinity to x, but we can avoid that and instead start integration from a beacuse we are only interested in interval (a,b), because if x &lt; a, it will be zero.</p>\n\n<div class=\"math\">\\text{F}(x) = \\int_a^x f(t) \\text{d}x = \\begin{cases} 0 \\text{ if } x \\lt a \\\\ 1 \\text{ if } x \\gt b \\text{ } \\\\ \\text{c} (x-a) \\text{ if } a \\le x \\le b\\end{cases}</div>\n\n<p>Take the value of c from above PDF.</p>\n\n<p>Let&#39;s find the Expected Value which will be integral of x times PDF.</p>\n\n<div class=\"math\">\\text{E}(X) = \\int_a^b x \\frac{1}{b-a} \\text{d}x = \\frac{x^2}{2(b-a)} \\bigm\\vert_a^b = \\frac{(b-a)^2}{2(b-a)} = \\frac{a+b}{2}</div>\n\n<p>Which means that the average is in the middle, makes sense because the distribution&nbsp;is uniform.</p>\n\n<p>Let&#39;s find its Variance:</p>\n\n<p>To find that we need to find <span class=\"math\">\\text{E}(X^2)</span>&nbsp;remember that X squared is also a function, since a function of a RV (which itself is a function) will also be a function.</p>\n\n<p><a href=\"https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician\" target=\"_blank\">Law Of The Unconscious Statistician</a>&nbsp;(LOTUS): It gives a handy method of finding E(g(X)) if we know E(X)</p>\n\n<p>Using the above LOTUS, we can easily find Variance.</p>\n\n<p>&nbsp;</p>\n\n<p>Universality of Uniform Distribution</p>\n\n<p>Let&#39;s take a CDF F, which is a continuous function, (and strictly increasing for the purpose of making our case easier);&nbsp;&nbsp;</p>\n\n<p>Then, if we let&nbsp;<span class=\"math\">\\text{X} = \\text{F}^{-1}(u)</span>&nbsp;then X will have CDF <span class=\"math\">\\text{F}</span>&nbsp;if u is Uniform Distribution&nbsp;<span class=\"math\">\\text{Unif}(0,1)</span>&nbsp;</p>\n\n<p>This allows us to simulate any Random Distribution by first generating a Uniform Distribution. Remember: Sometimes it might be hard to calculate inverse of F in practice.</p>\n\n<p>On the other hand, if we already have a Random Variable&nbsp;X, distributed by CDF F; then F(X) is Unif(0,1) Uniform Distribution.</p>\n\n<h2><a href=\"https://en.wikipedia.org/wiki/Normal_distribution\" target=\"_blank\">Normal Distribution</a></h2>\n\n<p>Standard Normal Distribution <span class=\"math\">\\mathcal{N}(0,1)</span>&nbsp; has mean = 0, variance = 1.&nbsp;</p>\n\n<p>Its PDF is&nbsp;<span class=\"math\">f(x) = \\text{ce}^{ \\frac{-x^2}{2}} </span>&nbsp;c is the normalizing constant, without which the integral of f(x) would not be equal to one. Let&#39;s find the value of c:-</p>\n\n<p>We need to find&nbsp;<span class=\"math\">\\int_{-\\infty}^{\\infty}\\text{e}^{ \\frac{-x^2}{2}} \\text{d}x</span>&nbsp;It has been proven that this integral cannot be solved in a closed-form (closed means finite terms of elementary functions).&nbsp;</p>\n\n<p>Someone came up with this bizzare/stupid&nbsp;way&nbsp;&nbsp;of solving this:-</p>\n\n<div style=\"color: green; margin-left: 5%;\">\n<p>Write the same integral&nbsp;twice,&nbsp;<span class=\"math\">\\int_{-\\infty}^{\\infty}\\text{e}^{ \\frac{-x^2}{2}} \\text{d}x \\int_{-\\infty}^{\\infty}\\text{e}^{ \\frac{-x^2}{2}} \\text{d}x</span></p>\n\n<p>The important thing to notice now is that x here is just a dummy variable, since this whole thing just represents an area under the curve, so we can rename it.</p>\n\n<div class=\"math\">\\int_{-\\infty}^{\\infty}\\text{e}^{ \\frac{-x^2}{2}} \\text{d}x \\int_{-\\infty}^{\\infty}\\text{e}^{ \\frac{-y^2}{2}} \\text{d}y</div>\n\n<p>&nbsp;Now, we can write this in form of double integral, it wouldn&#39;t change anything.</p>\n\n<div class=\"math\">\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}\\text{e}^{ \\frac{-x^2}{2}} \\text{e}^{ \\frac{-y^2}{2}} \\text{d}x\\text{ }\\text{d}y \\implies \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}\\text{e}^{ -\\frac{x^2 + y^2}{2}} \\text{d}x\\text{ }\\text{d}y</div>\n\n<p>Now, by the virtue of Pythogoras theorem, we can transform this into integral of polar co-ordinates:</p>\n\n<div class=\"math\">\\int_{0}^{2\\pi}\\int_{0}^{\\infty}\\text{e}^{ -\\frac{r^2}{2}} r \\text{ }\\text{d}r\\text{ }\\text{d}\\theta</div>\n\n<p>Notice that the limits have been changed, a Jacobian term (r) has been introduced during this transformation.</p>\n\n<div class=\"math\">\\implies \\int_{0}^{2\\pi} ( \\int_{0}^{\\infty}\\text{e}^{ -u} \\text{d}u)\\text{ }\\text{d}\\theta \\text{ by taking u=}r^2/2 \\implies 2\\pi</div>\n</div>\n\n<p>But, remember that we started with double terms, which means that to find the f(x) we would need to take square root of it&nbsp;<span class=\"math\">\\implies f(x) = \\sqrt{2\\pi}</span>&nbsp;</p>\n\n<p>Let&#39;s find Mean of N.D:</p>\n\n<p>Mean =&nbsp;<span class=\"math\">\\text{E}(X) = \\int_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi}} x \\text{ }\\text{e}^{ \\frac{-x^2}{2}} \\text{d}x</span>&nbsp; Notice that the term under integral is an <a href=\"https://en.wikipedia.org/wiki/Even_and_odd_functions\" target=\"_blank\">Odd Function</a>, and for odd functions integral turns out to be zero because by symmetry negative area represented by such functions is cancelled by the positive area; take sin(x) for an example.</p>\n\n<p>&nbsp;Let&#39;s find Variance now:</p>\n\n<p>Write formula for Variance, see that the second term is zero (we proved above), use LOTUS to find the other term.&nbsp;<span class=\"math\">\\text{E}(X^2) = \\int_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi}} x^2\\text{ }\\text{e}^{ \\frac{-x^2}{2}} \\text{d}x</span>&nbsp; Notice that here the term under integral is an Even Function which makes things a bit difficult. By symmetry, we can convert the limits to&nbsp;<span class=\"math\">2 \\int_0^{\\infty}</span>&nbsp;because the part before will be exactly same as the part after zero, and they will add up. We can then do integration by parts. Let&#39;s suppose&nbsp;<span class=\"math\">u = x, \\text{d}v = x \\text{e}^{-x^/2}</span>&nbsp;which implies&nbsp;<span class=\"math\">\\text{d}u = \\text{d}x, v = - \\text{e}^{-x^/2}</span>&nbsp;. Apply chain rule and solve for limits, it will give answer = 1.</p>\n\n<p>Notice that&nbsp;<span class=\"math\">\\text{If } X \\thicksim \\mathcal{N}(0,1) \\text{ then } -X \\thicksim \\mathcal{N}(0,1) \\text{ by symmetry}</span></p>\n\n<p>If we just multiply Standard N.D by a constant and add a constant, we get General N.D.</p>\n\n<p>If <span class=\"math\">Y = \\mu + \\sigma X \\text{ } ; \\sigma \\gt 0</span>, then <span class=\"math\">Y \\thicksim \\mathcal{N}(\\mu, \\sigma)</span>.&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>Marginal Densities</p>\n\n<p>For continuous R.V&#39;s X,Y the marginal densities are</p>\n\n<div class=\"math\">f_\\text{X} (x)= \\int f(x,y) \\text{d}y, \\text{ and } f_\\text{Y} (y)= \\int f(x,y) \\text{d}x</div>\n\n<p>The corresponding Marginal Distribution Functions are denoted by&nbsp;<span class=\"math\">F_\\text{X}</span> and&nbsp;<span class=\"math\">F_\\text{Y}</span> .</p>\n\n<p>&nbsp;</p>\n\n<p>Independent RVs</p>\n\n<p>Two RVs are independent if, for every A and&nbsp;B, we have</p>\n\n<div class=\"math\">\\text{P} ( \\text{X} \\in A, \\text{Y} \\in B) = \\text{P} (\\text{X} \\in A) \\text{P} (\\text{Y} \\in B)</div>\n\n<p>and we write&nbsp;<span class=\"math\">\\text{X} \\coprod \\text{Y}</span>&nbsp;if they are independent.</p>\n\n<p>&nbsp;</p>\n\n<p>If&nbsp;<span class=\"math\">X_1, X_2,... X_n</span> are independent and each has the same Marginal Distribution with CDF&nbsp;<span class=\"math\">F</span>&nbsp;, we say that&nbsp;<span class=\"math\">X_1, X_2,... X_n</span>&nbsp;are IID (Independent and Identically Distributed) RVs, and we write&nbsp;<span class=\"math\">X_1, X_2,... X_n \\thicksim F</span>&nbsp;, we also call&nbsp;<span class=\"math\">X_1, X_2,.. X_n</span>&nbsp;<strong>a random sample of size</strong>&nbsp;<span class=\"math\">n</span> <strong>from</strong> <span class=\"math\">F</span></p>\n\n<p>&nbsp;</p>\n\n<h3>Multi-Variate distributions</h3>\n\n<p>In case of Binomial Distribution there were multiple trials, and only two possible outcomes success/failure. If we extend this and say there are k possible outcomes&nbsp;<span class=\"math\">\\lgroup 1, 2, 3, ..k \\rgroup</span>&nbsp;with&nbsp;<span class=\"math\">p_j</span>&nbsp;being probability that outcome&nbsp;<span class=\"math\">j</span> happens.</p>\n\n<p>Now, if we consider RVs&nbsp;<span class=\"math\">\\lgroup X_1, X_2, X_3, .. X_k \\rgroup</span>&nbsp;where&nbsp;<span class=\"math\">X_j</span>&nbsp;represents number of times outcome&nbsp;<span class=\"math\">j</span>&nbsp;happens, we say that&nbsp;<span class=\"math\">X \\thicksim \\text{Multinomial}(n, p)</span>&nbsp;The probability function is,&nbsp;</p>\n\n<div class=\"math\">f(x) = \\frac{n!}{x_1! x_2!... x_k!} p_1^{x_1}...p_k^{x_k}</div>\n\n<p>The marginal distribution of&nbsp;<span class=\"math\">X_j</span>&nbsp;will be&nbsp;<span class=\"math\">\\text{Binomial}(n, p_j)</span>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p><strong>Multi-variate Normal Distribution</strong></p>\n\n<p>In univariate Normal parameters are single numbers, in Multivariate they become matrices.</p>\n\n<p>Let,</p>\n\n<div class=\"math\" style=\"border:1px solid #cccccc;padding:5px 10px;\">Z =\\begin{bmatrix} Z_1 \\\\ \\text{:} \\\\ Z_k \\end{bmatrix}</div>\n\n<p>&nbsp;Each of these are univariate standard Normal distributions. The density then is given by,</p>\n\n<p>&nbsp;</p>\n\n<div class=\"math\">\\displaystyle{ f(z) = \\prod_{i=1}^k f(z_i) = \\frac{1}{(2\\pi)^{k/2}}\\text{exp}\\large{\\lbrace}-\\frac{1}{2}\\sum_{j=1}^kz_j^2 \\large{\\rbrace} = \\frac{1}{(2\\pi)^{k/2}}\\text{exp}\\large{\\lbrace}-\\frac{1}{2}z^Tz \\large{\\rbrace}}</div>\n\n<p>We say that&nbsp;<span class=\"math\">Z</span>&nbsp;has standard Multivariate Normal Distribution, written&nbsp;<span class=\"math\">Z \\thicksim N(0, I)</span>&nbsp; where&nbsp;<span class=\"math\">0</span>&nbsp;is a vector of k zeroes, and&nbsp;<span class=\"math\">I</span>&nbsp;is&nbsp;<span class=\"math\">k \\times k</span>&nbsp;identity matrix. More generally, a vector&nbsp;<span class=\"math\">X</span>&nbsp;has a multivariate Normal distribution, denoted by&nbsp;<span class=\"math\">X \\thicksim N(\\mu, \\Sigma)</span>&nbsp;, if it has density</p>\n\n<div class=\"math\">\\displaystyle{ f(x; \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{k/2} \\lvert (\\Sigma) \\rvert^{1/2}} \\text{exp}\\large{\\lbrace}-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1} (x-\\mu) \\large{\\rbrace}}</div>\n\n<p>where&nbsp;<span class=\"math\">\\vert \\Sigma \\rvert</span>&nbsp;denotes determinant of matrix&nbsp;<span class=\"math\">\\Sigma</span>&nbsp;which is a&nbsp;<span class=\"math\">k \\times k</span>&nbsp;&nbsp;symmetric,&nbsp;positive definite matrix, and&nbsp;<span class=\"math\">\\mu</span>&nbsp;&nbsp;is a vector of length&nbsp;<span class=\"math\">k</span>&nbsp;</p>\n\n<p><strong>How to translate between standard and general multivariate Normal Distribution?</strong></p>\n\n<p>If <span class=\"math\">Z \\thicksim N(0,I)</span>, and&nbsp;<span class=\"math\">X = \\mu + \\Sigma^{1/2} Z</span>&nbsp;then&nbsp;<span class=\"math\">X \\thicksim N(\\mu, \\Sigma)</span>&nbsp;where&nbsp;<span class=\"math\">Sigma^{1/2}</span>&nbsp;is sqaure-root of the matrix.</p>\n\n<p>Conversely, if&nbsp;<span class=\"math\">X \\thicksim N(\\mu, \\Sigma)</span>&nbsp;then&nbsp;<span class=\"math\">\\Sigma^{-1/2}(X - \\mu) \\thicksim N(0,I)</span>&nbsp;</p>\n\n<p>If we have many normal RVs viz a Normal Vector we can partition it into two vectors and play with it which sometimes is helpful in problem solving</p>\n\n<p>If we partition a random Normal vector&nbsp;<span class=\"math\">X</span>&nbsp;as&nbsp;<span class=\"math\">X = (X_a, X_b)</span>&nbsp;We can similarly partition&nbsp;<span class=\"math\">\\mu = (\\mu_a, \\mu_b)</span>&nbsp;and&nbsp;<span class=\"math\">\\Sigma = \\begin{pmatrix} \\Sigma_{aa} \\; \\Sigma_{ab} \\\\ \\Sigma_{ba} \\; \\Sigma_{bb} \\end{pmatrix}</span>&nbsp;</p>\n\n<p>Then, marginal distribution of&nbsp;<span class=\"math\">X_a</span>&nbsp;is&nbsp;<span class=\"math\">X_a \\thicksim N(\\mu_a, \\Sigma_{aa})</span>&nbsp;</p>\n\n<p>And, conditional distribution of&nbsp;<span class=\"math\">X_b</span>&nbsp;given&nbsp;<span class=\"math\">X_a = x_a</span>&nbsp;is&nbsp;<span class=\"math\">N(\\mu_b + \\Sigma_{ba} \\Sigma_{aa}^{-1}(x_a - \\mu_a), \\Sigma_{bb} - \\Sigma_{ba} \\Sigma_{aa}^{-1} \\Sigma_{ab})</span>&nbsp;&nbsp;</p>\n\n<p>And, if&nbsp;<span class=\"math\">a</span>&nbsp;is a vector then&nbsp;<span class=\"math\">a^TX \\thicksim N(a^T \\mu, a^T \\Sigma a)</span>&nbsp;</p>\n\n<p>And,&nbsp;<span class=\"math\">V = (X-\\mu)^T \\Sigma^{-1} (X-\\mu) \\thicksim \\chi_k^2</span>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p><strong>Transformations of Random Variables</strong></p>\n\n<p>Let&nbsp;discrete RV is&nbsp;<span class=\"math\">X</span>&nbsp;CDF is&nbsp;<span class=\"math\">F_X</span> PDF is&nbsp;&nbsp;<span class=\"math\">f_X</span>&nbsp;</p>\n\n<p>if&nbsp;<span class=\"math\">Y = r(X)</span>&nbsp;is a function we can find its PDF&nbsp;<span class=\"math\">f_Y = \\text{P}( r(X) = y) = \\text{P}(\\{x; r(x) = y\\}) = \\text{P}(X \\in r^{-1} (y))</span>&nbsp;</p>\n\n<p>For continuous RV is a bit harder,</p>\n\n<p>To transform a continuous RV&nbsp;<span class=\"math\">X</span>&nbsp;</p>\n\n<ol>\n\t<li>For all&nbsp;<span class=\"math\">y</span>&nbsp;find set of related x&#39;s&nbsp;&nbsp;<span class=\"math\">A_y = \\{ x; r(x) \\le y \\}</span></li>\n\t<li>Find CDF&nbsp;<span class=\"math\">F_Y (y) = \\text{P}(\\{ x ; r(x) \\le y\\}) = \\int_{A_y}f_X(x)\\text{d}x</span></li>\n\t<li>PDF is the derivative of above&nbsp;</li>\n</ol>\n\n<p>Multiple RVs can be transformed in the same way:-</p>\n\n<p>Let,&nbsp;<span class=\"math\">Z = r(X,Y)</span>&nbsp;represents&nbsp;RVs X and&nbsp;Y&#39;s transformation</p>\n\n<ol>\n\t<li>For each&nbsp;<span class=\"math\">z</span>&nbsp;find the set of realted x,y pairs&nbsp;<span class=\"math\">A_z = \\{ (x,y); r(x,y) \\le z \\}</span>&nbsp;</li>\n\t<li>CDF&nbsp;<span class=\"math\">F_Z = \\int \\int_{A_z} f_{X,Y} (x,y) \\text{d}x \\text{d}y</span>&nbsp;</li>\n\t<li>PDF is the derivative of the above.</li>\n</ol>\n\n<p>&nbsp;</p>\n\n<p><strong>CoVariance and Correlation</strong></p>\n\n<p>Let&nbsp;<span class=\"math\">X, Y</span>&nbsp;be RVs with means&nbsp;<span class=\"math\">\\mu_X, \\mu_Y</span>&nbsp;and SDs&nbsp;<span class=\"math\">\\sigma_X, \\sigma_Y</span>&nbsp;</p>\n\n<div class=\"math\">\\displaystyle{\\text{Cov}(X,Y) = \\text{E}\\big( (X - \\mu_X)(Y - \\mu_Y) \\big) } ,\\text{and } \\text{Cor}(X,Y) = \\rho_{X,Y} = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}</div>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>Practical Application of Distributions:&nbsp;<a href=\"http://marathwadamathsociety.org/vol10-1/3-Kalyankar-Acharya.pdf\">http://marathwadamathsociety.org/vol10-1/3-Kalyankar-Acharya.pdf</a></p>\n\n<p>&nbsp;</p>\n","authorId":null,"subject":"ai","tags":["basic","intro"],"img":null,"summary":null,"lastUpdated":"2020-05-30T06:05:33.639+0000"}