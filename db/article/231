{"name":"Exploratory Data Analysis","id":231,"content":"<p>Source:&nbsp;<a href=\"https://www.itl.nist.gov/div898/handbook/index.htm\">https://www.itl.nist.gov/div898/handbook/index.htm</a>&nbsp;and&nbsp;<a href=\"https://learning.oreilly.com/library/view/practical-statistics-for/9781491952955/ch01.html#Location\">https://learning.oreilly.com/library/view/practical-statistics-for/9781491952955/ch01.html</a></p>\n\n<p>You have gathered some data. But you don&#39;t have insight into it. You have some assumptions (intuitions) about the data, but you are not sure.</p>\n\n<p>How would you go on getting insight and validating assumptions?</p>\n\n<p>What do we mean by insight? &mdash; Among many other things, what is the nature of data statistically, how are the variables related, which variables are important for our purposes, are there any exceptional cases from general trends&nbsp;etc.</p>\n\n<p><em>Everything, every technique, philosophy, approach&nbsp;that you apply to find these out is called EDA.</em></p>\n\n<p>There is also something&nbsp;called Classical Data Analysis (CDA).&nbsp; EDA is different from CDA in that it focuses primarily on data and models come out as a result. CDA focuses on assumed model.</p>\n\n<p>Classical techniques are generally quantitative in nature. They include ANOVA, t tests, chi-squared tests, and F tests.</p>\n\n<p>EDA techniques are generally graphical. They include scatter plots, character plots, box plots, histograms, bihistograms, probability plots, residual plots, and mean plots.</p>\n\n<p><em>[You can say that CDA is like Science and EDA is like Engineering.] CDA depends heavily on assumptions about data e.g. Normality.</em></p>\n\n<p>&nbsp;</p>\n\n<p>General Model for EDA:&nbsp;<span class=\"math\">\\text{response} = \\text{deterministic component} + \\text{random component}</span></p>\n\n<p><strong>Types of EDA Problems:</strong></p>\n\n<ol>\n\t<li>Univariate: Like you have a single column of numbers. You model the variable as&nbsp;<span class=\"math\">\\text{Constant } + \\text{ Error}</span></li>\n\t<li>Control: Same as above, but the interest is to find &quot;Is the system out of control? Yes or no&quot;. You model the variables as&nbsp;<span class=\"math\">y = f(x_1, x_2, ..., x_k) + \\text{error}</span>&nbsp;</li>\n\t<li>Comparative: A single response variable and k <em>independent variables</em>. The goal is to find &quot;Is this (one of the independent) variable significant?&quot;</li>\n\t<li>Screening: Same as above, but the you are interested in ranking the significance of variables.</li>\n\t<li>Optimization: Same as above, but the focus is on &quot;best&nbsp;settings for factor&nbsp;variables&quot;</li>\n\t<li>Regression:&nbsp;Same as above, but the focus is on &quot;finding a model/equation relating Y to the variables&quot;.</li>\n\t<li>Time-Series: Column of values which are time dependent,&nbsp; if the time&nbsp;is not equi-spaced you will have another column for the time. You model the variable as&nbsp;<span class=\"math\">y_t = f(t) + \\text{error}</span></li>\n\t<li>Multivariate: You don&#39;t have target/prediction variable, just&nbsp;factor variables.</li>\n</ol>\n\n<p>&nbsp;</p>\n\n<h2>Data Types for EDA</h2>\n\n<p>Continuous/Interval/Float/Numeric</p>\n\n<p>Discrete/Integer/Count</p>\n\n<p>Categorical/Enums/Enumerated/Factors/Nominal/Polychotomous</p>\n\n<p>Binary/Dichotomous/Logical/Indicator/Boolean</p>\n\n<p>Ordinal/Ordered Factor: Categorical data that has an explicit ordering.</p>\n\n<p>The data types are not important for statistical inference as much as they are to softwares used to work with data e.g. categories can signal software to index data for optimization.</p>\n\n<p>&nbsp;</p>\n\n<p><strong>Data Structure:</strong></p>\n\n<p>DataFrame/Table/Spreadsheet</p>\n\n<p>Feature/Column/Attribute/Predictor/Variable/Input</p>\n\n<p>Outcome/dependent variable/ response/&nbsp;target/&nbsp;output</p>\n\n<p>Record/case/&nbsp;example/&nbsp;instance/&nbsp;observation/&nbsp;pattern/&nbsp;sample</p>\n\n<p>In&nbsp;<code>pandas</code>, it is also possible to set multilevel/hierarchical indexes (into DataFrames) to improve the efficiency of certain operations.</p>\n\n<p>One synonym is particularly confusing: computer scientists will use the term&nbsp;<em>sample</em>&nbsp;for a single row; a&nbsp;<em>sample</em>&nbsp;to a statistician means a collection of rows.</p>\n\n<p>&nbsp;</p>\n\n<p><strong>Statistics:</strong></p>\n\n<p>Mean/Average</p>\n\n<p>Weighted Mea/Weighted Average</p>\n\n<p>Median / 50th Percentile</p>\n\n<p>Trimmed median/ Truncated Median</p>\n\n<p>Robust/ Resistant</p>\n\n<p>Outlier / Extreme Value</p>\n\n<p>&nbsp;</p>\n\n<p><strong>Location:</strong></p>\n\n<p>Statisticians often use the term&nbsp;<em>estimates</em>&nbsp;for values calculated from the data at hand, to draw a distinction between what we see from the data, and the theoretical true or exact state of affairs.<a data-primary=\"estimates\" data-type=\"indexterm\" id=\"idm45909263393576\"></a>&nbsp;Data scientists and business analysts are more likely to refer to such values as a&nbsp;<em>metric</em>.<a data-primary=\"metrics\" data-type=\"indexterm\" id=\"idm45909263392360\"></a>&nbsp;</p>\n\n<p>Apart from Mean, Median there are other Locations developed, but for a moderately sized dataset they don&#39;t have significant benefit, so we can just go ahead with these two.</p>\n\n<ul>\n\t<li>\n\t<p>The basic metric for location is the mean, but it can be sensitive to extreme values (outlier).</p>\n\t</li>\n\t<li>\n\t<p>Other metrics (median, trimmed mean) are more robust.</p>\n\t</li>\n</ul>\n\n<p>&nbsp;</p>\n\n<p><strong>Variability:</strong></p>\n\n<p>Deviations/&nbsp;errors/&nbsp;residuals</p>\n\n<p>Variance/&nbsp;mean-squared-error</p>\n\n<p>Standard Deviation/ L2-norm/&nbsp;Euclidean norm</p>\n\n<p>Mean Absolute Deviation/ L1-norm/&nbsp;Manhattan norm</p>\n\n<p>Percentile/ Quantile</p>\n\n<p>In S.D formula, you might see n or n-1, it doesn&#39;t really matter for a large dataset. Historically, n-1 is used because it leads to unbiased estimation of S.D from sample (For S.D there&#39;s only one constraint Mean, hence n-1 degrees of freedom).&nbsp;</p>\n\n<p>In statistical theory, <strong>location</strong> and <strong>variability</strong> are referred to as the first and second&nbsp;<em>moments</em>&nbsp;of a distribution.<a data-primary=\"statistical moments\" data-type=\"indexterm\" id=\"idm45909268579528\"></a><a data-primary=\"moments\" data-type=\"indexterm\" id=\"idm45909268578792\"></a>&nbsp;The third and fourth moments are called&nbsp;<strong><em>skewness</em></strong>&nbsp;and&nbsp;<strong><em>kurtosis</em></strong>. Skewness refers to whether the data is skewed to larger or smaller values and kurtosis indicates the propensity of the data to have extreme values.<a data-primary=\"skewness\" data-type=\"indexterm\" id=\"idm45909268577000\"></a><a data-primary=\"kurtosis\" data-type=\"indexterm\" id=\"idm45909268576296\"></a>&nbsp;Generally, metrics are not used to measure skewness and kurtosis; instead, these are discovered through visual displays.</p>\n\n<p>&nbsp;</p>\n\n<p><strong>Exploring Binary and Categorical data:</strong></p>\n\n<p><strong><em>Mode:</em></strong>&nbsp;The most commonly&nbsp;<a data-primary=\"mode\" data-type=\"indexterm\" id=\"idm45909268466168\"></a>occurring category (categories in case of a tie)&nbsp;or value in a data set.</p>\n\n<dl>\n\t<dt><strong><em>Expected value</em></strong>: When the categories can be associated with a numeric value, this gives an average value based on a category&rsquo;s probability of occurrence. Multiply each outcome by its probability of occuring and then sum it up. (Is really a form of weighted mean, weight being probability)</dt>\n\t<dt>&nbsp;</dt>\n\t<dt>&nbsp;</dt>\n\t<dt><strong>Correlation:</strong>&nbsp;Correlation Coefficient, Correlation Matrix, Scatter plots.</dt>\n</dl>\n\n<p>Statisticians have long ago proposed other types of correlation coefficients, such as&nbsp;<em>Spearman&rsquo;s rho</em>&nbsp;or&nbsp;<em>Kendall&rsquo;s tau</em>.<a data-primary=\"correlation coefficient\" data-secondary=\"other types of\" data-type=\"indexterm\" id=\"idm45909268233848\"></a>&nbsp;These are correlation coefficients based on the rank of the data. Since they work with ranks rather than values, these estimates are robust to outliers and can handle certain types of nonlinearities. However, data scientists can generally stick to <strong>Pearson&rsquo;s</strong> correlation coefficient, and its robust alternatives, for exploratory analysis. The appeal of rank-based estimates is mostly for smaller data sets and specific hypothesis tests.</p>\n\n<ul>\n\t<li>\n\t<p>The correlation coefficient is a standardized metric so that it always ranges from &ndash;1 (perfect negative correlation) to +1 (perfect positive correlation).</p>\n\t</li>\n\t<li>\n\t<p>A correlation coefficient of 0 indicates no correlation, but be aware that random arrangements of data will produce both positive and negative values for the correlation coefficient just by chance.</p>\n\t</li>\n</ul>\n\n<p>&nbsp;</p>\n\n<dl>\n\t<dt><strong><em>Standard error</em></strong></dt>\n\t<dd>\n\t<p>The variability (standard deviation) of a sample&nbsp;<em>statistic</em>&nbsp;over many&nbsp;<a data-primary=\"standard error\" data-type=\"indexterm\" id=\"idm45909267345160\"></a>samples (not to be confused with&nbsp;<em>standard deviation</em>, which, by itself, refers to variability of individual data&nbsp;<em>values</em>).</p>\n\t</dd>\n</dl>\n\n<p>It is important to distinguish between the distribution of the individual data points, known as&nbsp;<em>the data distribution</em>, and the distribution of a sample statistic, known as the&nbsp;<em>sampling distribution</em>.</p>\n\n<p>The distribution of a sample statistic such as the mean is likely to be more regular and bell-shaped than the distribution of the data itself. The larger the sample that the statistic is based on, the more this is true. Also, the larger the sample, the narrower the distribution of the sample statistic.</p>\n\n<p>The&nbsp;<em>standard error</em>&nbsp;is a single metric that sums up the variability in the sampling distribution for a statistic.<a data-primary=\"sampling distribution\" data-secondary=\"standard error\" data-type=\"indexterm\" id=\"idm45909267169688\"></a><a data-primary=\"standard error\" data-secondary=\"formula for calculating\" data-type=\"indexterm\" id=\"idm45909267168712\"></a>&nbsp;The standard error can be estimated using a statistic based on the standard deviation&nbsp;<em>s</em>&nbsp;of the sample values, and the sample size&nbsp;<em>n</em>:</p>\n\n<p>Standarderror=SE=&nbsp;<span class=\"math\">\\frac{s}{\\sqrt{n}}</span></p>\n\n<p>Do not confuse standard deviation (which measures the variability of individual data points) with standard error (which measures the variability of a sample metric).</p>\n\n<p>In practice, this approach of collecting new samples to estimate the standard error is typically not feasible (and statistically very wasteful). Fortunately, it turns out that it is not necessary to draw brand new samples; instead, you can use&nbsp;<em>bootstrap</em>&nbsp;resamples (see&nbsp;<a data-type=\"xref\" href=\"https://learning.oreilly.com/library/view/practical-statistics-for/9781491952955/ch02.html#bootstrap\">&ldquo;The Bootstrap&rdquo;</a>). In modern statistics, the bootstrap has become the standard way to to estimate standard error. It can be used for virtually any statistic and does not rely on the central limit theorem or other distributional assumptions.</p>\n\n<p>Sometimes the term&nbsp;<em>resampling</em>&nbsp;is used synonymously with the term&nbsp;<em>bootstrapping</em>, as just outlined.<a data-primary=\"resampling\" data-secondary=\"bootstrapping vs.\" data-type=\"indexterm\" id=\"idm45909267053592\"></a><a data-primary=\"bootstrap\" data-secondary=\"resampling vs. bootstrapping\" data-type=\"indexterm\" id=\"idm45909267052584\"></a>&nbsp;More often, the term&nbsp;<em>resampling</em>&nbsp;also includes permutation procedures (see&nbsp;<a data-type=\"xref\" href=\"https://learning.oreilly.com/library/view/practical-statistics-for/9781491952955/ch03.html#Permutation\">&ldquo;Permutation Test&rdquo;</a>), where multiple samples are combined and the sampling may be done without replacement.</p>\n\n<p>Frequency tables, histograms, boxplots, and standard errors are all ways to understand the potential error in a sample estimate.<a data-primary=\"sampling\" data-secondary=\"confidence intervals\" data-type=\"indexterm\" id=\"ix_sampci\"></a><a data-primary=\"confidence intervals\" data-type=\"indexterm\" id=\"ix_confint\"></a>&nbsp;Confidence intervals are another.</p>\n\n<dl>\n\t<dt><strong><em>Confidence level</em></strong></dt>\n\t<dd>\n\t<p>The percentage of confidence intervals, constructed in the same way from the same population, expected to contain the statistic of interest.</p>\n\t</dd>\n</dl>\n\n<p>Given a sample of size&nbsp;<em>n</em>, and a sample statistic of interest, the algorithm for a<a data-primary=\"confidence intervals\" data-secondary=\"generating with bootstrap\" data-type=\"indexterm\" id=\"idm45909266890712\"></a><a data-primary=\"bootstrap\" data-secondary=\"confidence interval generation\" data-type=\"indexterm\" id=\"idm45909266889672\"></a>&nbsp;bootstrap confidence interval is as follows:</p>\n\n<ol>\n\t<li>\n\t<p>Draw a random sample of size&nbsp;<em>n</em>&nbsp;with replacement from the data (a resample).</p>\n\t</li>\n\t<li>\n\t<p>Record the statistic of interest for the resample.</p>\n\t</li>\n\t<li>\n\t<p>Repeat steps 1&ndash;2 many (<em>R</em>) times.</p>\n\t</li>\n\t<li>\n\t<p>For an&nbsp;<em>x</em>% confidence interval, trim [(100-<em>x</em>) / 2]% of the&nbsp;<em>R</em>&nbsp;resample results from either end of the distribution.</p>\n\t</li>\n\t<li>\n\t<p>The trim points are the endpoints of an&nbsp;<em>x</em>% bootstrap confidence interval.</p>\n\t</li>\n</ol>\n\n<p>The bootstrap is a general tool that can be used to generate confidence intervals for most statistics, or model parameters. Statistical textbooks and software, with roots in over a half-century of computerless statistical analysis, will also reference confidence intervals generated by formulas, especially the t-distribution (see&nbsp;<a data-type=\"xref\" href=\"https://learning.oreilly.com/library/view/practical-statistics-for/9781491952955/ch02.html#t-distribution\">&ldquo;Student&rsquo;s t-Distribution&rdquo;</a>).</p>\n\n<p>The probability question associated with a confidence interval starts out with the phrase &ldquo;Given a sampling procedure and a population, what is the probability that&hellip;&rdquo; To go in the opposite direction, &ldquo;Given a sample result, what is the probability that (something is true about the population),&rdquo; involves more complex calculations and deeper imponderables.</p>\n\n<p>For a data scientist, a confidence interval is a tool to get an idea of how variable a sample result might be. Data scientists would use this information not to publish a scholarly paper or submit a result to a regulatory agency (as a researcher might), but most likely to communicate the potential error in an estimate, and, perhaps, learn whether a larger sample is needed.</p>\n\n<p>&nbsp;</p>\n\n<p>It is a common misconception that the normal distribution is called that because most data follows a normal distribution&mdash;that is, it is the normal thing. Most of the variables used in a typical data science project&mdash;in fact most raw data as a whole&mdash;are&nbsp;<em>not</em>&nbsp;normally distributed: see&nbsp;<a data-type=\"xref\" href=\"https://learning.oreilly.com/library/view/practical-statistics-for/9781491952955/ch02.html#LongTailedData\">&ldquo;Long-Tailed Distributions&rdquo;</a>. The utility of the normal distribution derives from the fact that many statistics&nbsp;<em>are</em>&nbsp;normally distributed in their sampling distribution. Even so, assumptions of normality are generally a last resort, used when empirical probability distributions, or bootstrap distributions, are not available.</p>\n\n<p>&nbsp;</p>\n\n<p><strong>Standard Normal and QQ-Plots</strong></p>\n\n<p>A&nbsp;<em>standard normal</em>&nbsp;distribution is one in which the units on the x-axis are expressed in terms of standard deviations away from the mean.<a data-primary=\"standard normal distribution\" data-type=\"indexterm\" id=\"idm45909266821656\"></a><a data-primary=\"normal distribution\" data-secondary=\"standard normal and QQ-Plots\" data-type=\"indexterm\" id=\"idm45909266820936\"></a><a data-primary=\"QQ-Plots\" data-secondary=\"standard normal and\" data-type=\"indexterm\" id=\"idm45909266819976\"></a>&nbsp;To compare data to a standard normal distribution, you subtract the mean then divide by the standard deviation; this is also called&nbsp;<em>normalization</em>&nbsp;or&nbsp;<em>standardization</em>&nbsp;(see&nbsp;<a data-type=\"xref\" href=\"https://learning.oreilly.com/library/view/practical-statistics-for/9781491952955/ch06.html#Standardization\">&ldquo;Standardization (Normalization, Z-Scores)&rdquo;</a>).<a data-primary=\"z-score\" data-secondary=\"converting data to\" data-type=\"indexterm\" id=\"idm45909266817064\"></a><a data-primary=\"standardization\" data-type=\"indexterm\" id=\"idm45909266816088\"></a><a data-primary=\"normalization\" data-type=\"indexterm\" id=\"idm45909266815416\"></a>&nbsp;Note that &ldquo;standardization&rdquo; in this sense is unrelated to database record standardization (conversion to a common format). The transformed value is termed a&nbsp;<em>z-score</em>, and the normal distribution is sometimes called the&nbsp;<em>z-distribution</em>.</p>\n\n<p>A QQ-Plot is used to visually determine how close a sample is to the normal distribution. The QQ-Plot orders the&nbsp;<em>z</em>-scores from low to high, and plots each value&rsquo;s&nbsp;<em>z</em>-score on the y-axis; the x-axis is the corresponding quantile of a normal distribution for that value&rsquo;s rank. Since the data is normalized, the units correspond to the number of standard deviations away of the data from the mean. If the points roughly fall on the diagonal line, then the sample distribution can be considered close to normal.</p>\n\n<p>Note:&nbsp;Converting data to&nbsp;<em>z</em>-scores (i.e., standardizing or normalizing the data) does&nbsp;<em>not</em>&nbsp;make the data normally distributed. It just puts the data on the same scale as the standard normal distribution, often for comparison purposes.</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<h2>Testing the assumptions for EDA that&nbsp;the data behaves like:</h2>\n\n<ol>\n\t<li>random drawings;</li>\n\t<li>from a fixed distribution;</li>\n\t<li>with the distribution having fixed location; and</li>\n\t<li>with the distribution having fixed variation.</li>\n</ol>\n\n<p>There are 4 plots:</p>\n\n<ol>\n\t<li>\n\t<p>run sequence plot (<span class=\"math\">Y_i</span> versus <span class=\"math\">i</span>): will be flat and non-drifting if fixed location assumption is true. Vertical spread will be. approximately the same with entire horizontal axis.</p>\n\t</li>\n\t<li>\n\t<p>lag plot (<span class=\"math\">Y_i</span> versus <span class=\"math\">Y_{i-1}</span>): will be structureless if randomness assumption is true.</p>\n\t</li>\n\t<li>\n\t<p>histogram (counts versus subgroups of <span class=\"math\">Y</span>): will&nbsp;be approximately. bell-shaped if&nbsp;Normal assumption is true.</p>\n\t</li>\n\t<li>\n\t<p>normal probability plot (ordered&nbsp;<span class=\"math\">Y</span> versus theoretical ordered Y): will be linear if&nbsp;Normal assumption is true.</p>\n\t</li>\n</ol>\n\n<p>&nbsp;</p>\n\n<p>Non-randomness is most dangerous, so its assumption must be tested. One reason of non-radomness is auto-correlation (current value of variable dependent on past value of itself) which can be detected via&nbsp;autocorrelation plot or a lag plot. Or maybe because adjacent data values are related, maybe due to undetected junk-outliers.&nbsp;</p>\n\n<p><strong>What to do if these assumptions do not hold? ......</strong></p>\n\n<p>&nbsp;</p>\n\n<p><strong>Auto-Correlation Plots:</strong></p>\n\n<p><span class=\"math\">\\frac{C_h}{C_0} \\text{ on y-axis against } h \\text { on x-axis}</span>&nbsp;where&nbsp;<span class=\"math\">C_h = \\frac{1}{N}\\sum_{t=1}^{n-h}(Y_{t} - \\overline{Y})(Y_{t+h}- \\overline{Y}) \\in [-1,1]</span>&nbsp;</p>\n\n<p>If the data are random, the plot&#39;s y-values should be near zero. We fix a confidence band to allow some fluctuations and still consider it random.</p>\n\n<p>Also, if the data are random, you cannot look at a y-value on the plot and predict the next y-value (i.e. adjacent values are not related).</p>\n\n<p>If you find that data are moderately auto-correlated, you can assume a regression&nbsp;model and find the parameters of it:-&nbsp;<span class=\"math\">Y_i = c_0 + c_1*Y_{i-1} + E_i</span>&nbsp;After fitting&nbsp;<span class=\"math\">Y_i</span>&nbsp;against&nbsp;<span class=\"math\">Y_{i-1}</span>&nbsp;it should become random most probably.</p>\n\n<p>You could also use <a href=\"https://www.itl.nist.gov/div898/handbook/eda/section3/eda33f1.htm\" target=\"_blank\">Lag Plot</a> instead.</p>\n\n<p>&nbsp;</p>\n\n<p>Sometimes, a <strong>Bihistogram plot</strong> can be easier (in comparision to t-test, ANOVA etc) in comparing location, variation, distribution before and after an engineering modification for example. Bihistograams can reveal truth about three out of four assumptions mentioned above (all execpt for randomness).</p>\n\n<p>&nbsp;</p>\n\n<p><strong>Block Plot</strong>&nbsp;can reveal whether the variable of interest has a significant effect over response variable (prediction variable) while comparing two experiments/designs etc. It can be used in place of ANOVA.</p>\n\n<p>This plots Response Variable against all combinations secondary factors, boxes are drawn on plot with upper and lower parts of box representing the value of Response Variable for Primary Variable in two situations. Then we can ask, &quot;does the primary variable has significantly more (in comparision to secondary variables) impact on Response Variable?&quot;</p>\n\n<p>If you see that for 10 out of 12 combinations, design one is better than other, then ask &quot;what is the chance that this was by chance?&quot; It is equivalent to getting 10 successes out of 12 trials in Binomial Distribution.</p>\n\n<p>See an example plot and its explanation on this <a href=\"https://www.itl.nist.gov/div898/handbook/eda/section3/eda333.htm\" target=\"_blank\">link</a>.</p>\n\n<p>&nbsp;</p>\n\n<p><strong>Bootstrap Plot</strong>&nbsp;in cases where it&#39;s hard to calculate mathematically&nbsp;the confidence interval for statistic, these plots come in handy. In Boostrap method, subsample of size equal to or less than the size of dataset is chosen and statistic is computed. This is repeated again and again, typically at least 500 times. The plot draws value of statistic against subsample.</p>\n\n<p>&nbsp;</p>\n\n<p><strong>Box-Cox Linearity Plot</strong>: If you want to model linear relationship of variables, you can use this plot to see whether a transformation of variable increases the model fitness (i.e. the linear relationship now approximates better).</p>\n\n<p>This plots the value of correlation b/w Y and f(X)&nbsp;against a parameter lambda.&nbsp;<span class=\"math\">f(X) = (X^\\lambda - 1)/\\lambda; \\lambda \\ne 0</span>&nbsp;. For lambda = 0, natural log of X is taken instead.</p>\n\n<p><strong>Box-Cox Normality Plot</strong>: If the data that you have is not approximately Normal, most of the statistical techniques simply do not work. Then you ask the question, &quot;Is there a way my data could be converted into a Normal distributed?&quot; This plot can help you find such transformation of data.</p>\n\n<p>See the details of this plot <a href=\"https://www.itl.nist.gov/div898/handbook/eda/section3/eda336.htm\" target=\"_blank\">here</a>.</p>\n\n<p><strong>Box Plots:</strong>&nbsp;</p>\n\n<p>it plots Response Variable against variables of interest. The box (b/w lower and upper quartile) represents 50% of the data, line from lower part of the box extends to minimum values in observation, Line from upper part of the box extends to the maximum value in observation. A symbol inside box represents median.&nbsp;Calculate the interquartile range (the difference between the upper and lower quartile) and call it IQ.</p>\n\n<p>Calculate the following points:</p>\n\n<p>L1 = lower quartile - 1.5*IQ , L2 = lower quartile - 3.0*IQ</p>\n\n<p>U1 = upper quartile + 1.5*IQ , U2 = upper quartile + 3.0*IQ</p>\n\n<p>In the plot there&#39;ll be a line from lower quartile to the smallest point greater than L1. A line from upper quartile to largest point less than U1. Points b/w L1 and L2 or b/w U1 and U2 are drawn in small&nbsp;circles. Points less than L2, or larger than U2 are drawn in big circles.</p>\n\n<p>The plot can answer following questions:</p>\n\n<p>1. Is a factor significant?<br />\n2. Does the location differ between subgroups?<br />\n3. Does the variation differ between subgroups?<br />\n4. Are there any outliers?</p>\n\n<p>&nbsp;</p>\n\n<p><strong>Contour Plots:</strong></p>\n\n<p>A contour plot is a graphical technique for representing a 3- dimensional surface by plotting constant z slices, called contours, on a 2-dimensional format. That is, given a value for z, lines are drawn for connecting the (x,y) coordinates where that z value occurs.</p>\n\n<p>The plot answers the question, &quot;How does Z change as a function of X and Y?&quot;</p>\n\n<p>&nbsp;</p>\n\n<p><strong>Histogram:</strong></p>\n\n<p>Plots frequency against (equal-sized) bin/buckets.</p>\n\n<p>It reveals mean, variance, Normality, outliers.&nbsp;</p>\n\n<p>There are terms commonly used with histograms: Short-tailed, Long-tailed etc. The tail length tells how quickly the value on y-axis approaches zero while moving from location. For Normal distributions it is moderately long. For uniform distribution its shortest. For Cauchy distribution it&#39;s quite long.</p>\n\n<p>The optimal (unbiased and most precise) estimator for location for the center of a distribution is heavily dependent on the tail length of the distribution. The common choice of taking N observations and using the calculated sample mean as the best estimate for the center of the distribution is a good choice for the normal distribution (moderate tailed), a poor choice for the uniform distribution (short tailed), and a horrible choice for the Cauchy distribution (long tailed). Although for the normal distribution the sample mean is as precise an estimator as we can get, for the uniform and Cauchy distributions, the sample mean is not the best estimator.</p>\n\n<p>For the uniform distribution, the midrange</p>\n\n<ul>\n\t<li>midrange = (smallest + largest) / 2</li>\n</ul>\n\n<p>is the best estimator of location. For a Cauchy distribution, the&nbsp;<a href=\"https://www.itl.nist.gov/div898/handbook/eda/section3/eda351.htm#MEDIAN\">median</a>&nbsp;is the best estimator of location.</p>\n\n<p>Unimodel vs BiModal Histogram: Unimodal means single peak e.g. in Normal distribution. BiModal means two peaks.</p>\n\n<p><strong>What to do if you find that there are two peaks?</strong> - Find the cause. Read <a href=\"https://www.itl.nist.gov/div898/handbook/eda/section3/eda33e4.htm\" target=\"_blank\">here</a>.</p>\n\n<p>See if the bimodality is because of two Normals with same S.D. but different Mean, then see if&nbsp;data can be fit with mixture of two Normals.</p>\n\n<p><strong>Symmetric vs Skewness</strong>: Distribution is said to be skewed when&nbsp;the plot has longer tail on left or right side.</p>\n\n<p>If the histogram indicates a right-skewed data set, the recommended next steps are to:</p>\n\n<ol>\n\t<li>Quantitatively summarize the data by computing and reporting the sample mean, the sample median, and the sample mode.</li>\n\t<li>Determine the best-fit distribution (skewed-right) from the\n\t<ul>\n\t\t<li><a href=\"https://www.itl.nist.gov/div898/handbook/eda/section3/eda3668.htm\">Weibull family</a>&nbsp;(for the maximum)</li>\n\t\t<li><a href=\"https://www.itl.nist.gov/div898/handbook/eda/section3/eda366b.htm\">Gamma family</a></li>\n\t\t<li><a href=\"https://www.itl.nist.gov/div898/handbook/eda/section3/eda3666.htm\">Chi-square family</a></li>\n\t\t<li><a href=\"https://www.itl.nist.gov/div898/handbook/eda/section3/eda3669.htm\">Lognormal family</a></li>\n\t\t<li><a href=\"https://www.itl.nist.gov/div898/handbook/eda/section3/eda366e.htm\">Power lognormal family</a></li>\n\t</ul>\n\t</li>\n\t<li>Consider a normalizing transformation such as the&nbsp;<a href=\"https://www.itl.nist.gov/div898/handbook/eda/section3/boxcoxno.htm\">Box-Cox transformation</a>.</li>\n</ol>\n\n<p>If the histogram shows outliers, confirm it with Box Plots, or. by&nbsp;<a href=\"https://www.itl.nist.gov/div898/handbook/eda/section3/eda35h.htm\">Grubbs test</a>&nbsp;which indicates how many sample standard deviations away from the sample mean are the data in question. Large values indicate outliers.</p>\n\n<p>&nbsp;</p>\n\n<p><strong>Normal Probability Plot:</strong></p>\n\n<p>To test whether data departs from Normality and how.</p>\n\n<p>See <a href=\"https://www.itl.nist.gov/div898/handbook/eda/section3/eda33l.htm\" target=\"_blank\">here</a>. All data points approximately on the reference line&nbsp;&rArr; Normal, All data poinsts below &rArr; Right Skewed, All data points above&nbsp;&rArr; Left Skewed. Kind of S-shape in middle around reference line and departure around extreme ends&nbsp;&rArr; Short-tailed. Inverted S-shape&nbsp;&rArr; Long-tailed.</p>\n\n<p>&nbsp;</p>\n\n<p><strong>Probability Plot:</strong>&nbsp;To verify the underlying assumption that data comes from a particular distriution (e.g.&nbsp;Weibull, lognormal, exponentional in case of reliability applications). If not what kind distribution is good, what are the estimates of statistics.</p>\n\n<p>&nbsp;</p>\n\n<p><strong>Hexagonal Plots:</strong></p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<hr />\n<h2>Case Studies</h2>\n\n<p>EDA Case Studies:&nbsp;<a href=\"https://www.itl.nist.gov/div898/handbook/eda/section4/eda42.htm\">https://www.itl.nist.gov/div898/handbook/eda/section4/eda42.htm</a></p>\n\n<p>We&#39;d like to have this kind of statistics for the samples set of Random Numbers in this case study:-</p>\n\n<pre>\nAnalysis of 500 normal random numbers\n  \n 1: Sample Size                           = 500\n  \n 2: Location\n    Mean                                  = -0.00294\n    Standard Deviation of Mean            = 0.045663\n    95% Confidence Interval for Mean      = (-0.09266,0.086779)\n    Drift with respect to location?       = NO\n  \n 3: Variation\n    Standard Deviation                    = 1.021042\n    95% Confidence Interval for SD        = (0.961437,1.088585)\n    Drift with respect to variation?\n    (based on Bartletts test on quarters\n    of the data)                          = NO\n  \n 4: Data are Normal?\n      (as tested by Normal PPCC)         = YES\n      (as tested by Anderson-Darling)    = NO\n  \n 5: Randomness\n    Autocorrelation                       = 0.045059\n    Data are Random?\n      (as measured by autocorrelation)    = YES\n  \n 6: Statistical Control\n    (i.e., no drift in location or scale,\n    data are random, distribution is \n    fixed, here we are testing only for\n    fixed normal)\n    Data Set is in Statistical Control?   = YES\n  \n 7: Outliers?\n    (as determined by Grubbs&#39; test)       = NO</pre>\n\n<p>&nbsp;</p>\n\n<p>Here&#39;s a great explanation of Linear Regression: <a href=\"https://www.oreilly.com/library/view/practical-statistics-for/9781491952955/ch04.html\">Oreilly Linear Regression</a>. Remember, Linear Regression is not only used to predict data, it can also be used to explain data.</p>\n\n<p><strong>How can Linear Regression help in explaining the data?</strong></p>\n\n<p>We can use Correlation to find whether two variables are related and if they are related how strong is their relationship. Then why L.R? &mdash;&nbsp; Because Regression does something more, it. quantifies the nature of relationship b/w two variables.</p>\n\n<p>&nbsp;</p>\n\n<p>Hardly any sample is represents its population (especially reviews on websites: the only motivated people might be those with bad experience, good ones might be baught or something, hence introducing bias). Note that while self-selection samples can be unreliable indicators of the true state of affairs, they may be more reliable in simply comparing one establishment to a similar one; the same self-selection bias might apply to each.</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<hr />\n<h2>EDA in Python</h2>\n\n<p><strong>Get the statistics of data quickly:</strong></p>\n\n<pre>\n<code class=\"language-python\">#Gives count, min, max, std deviation, quantiles (25%, 50%, 75%) for each column\npandas.read_csv(\"..\").describe()</code></pre>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n","authorId":null,"subject":"ai","tags":null,"img":null,"summary":null,"lastUpdated":"2020-06-07T10:51:32.750+0000"}