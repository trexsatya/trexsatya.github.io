{"name": "Evolution Of Application/Systems", "id": 257, "content": "<p>You create an <em>application/system</em> to support a business. <em>Business</em> means that your are providing some <em>service</em> to people(<em>users</em>).&nbsp;</p>\n\n<p>What your application/system&nbsp;does largely depends upon what kind of business it is supporting. But in general, it will provide assistance to the users of the system, make things easier for them, make things automatic for them.</p>\n\n<p>How the system does this thing again depends upon what kind of business it is, but in general your system will provide an interface to your users (e.g. a web app, a mobile app, SMS facility, voice call facility, a desktop app, or even a hardware device, etc), your users will interact with this interface and the interface will make some requests to your system, your system will do some things like parse and understand the request, store some data, fetch some data, do some calculations, and then give response back the requester interface.</p>\n\n<p>So, in summary, and from a higher perspective, your system just accepts some requests, stores data, and makes some computations, and serves data.</p>\n\n<p>Simple Case:</p>\n\n<p>&nbsp;</p>\n\n<hr />\n<h3>&nbsp;</h3>\n\n<h3>A single application server + A single DB instance</h3>\n\n<p>How much throughput with a reasonable response time can this setup give?&nbsp;</p>\n\n<p>There&#39;s no accurate answer, because it depends upon the webserver implementation <em>(whether it uses event-drive/asynchronous sockets or one hardware thread per request)</em>, web-server configuration, hardware of the server, how much resources your application/DB gets from the hardware (because there&#39;d always be other processes running on the machine, also if you are using VMs from cloud service providers it will be shared with other users).</p>\n\n<p>Nginx (with default config) can handle 300,000 or more open connections. But this many connections, doesn&#39;t mean this many requests can be served simultaneously because the processing of requests will also take time.</p>\n\n<p>It&#39;s better to do stress test on the system to see how much it can handle.</p>\n\n<p><a href=\"http://highscalability.com/blog/2013/5/13/the-secret-to-10-million-concurrent-connections-the-kernel-i.html\">10M concurrent connections with one server</a></p>\n\n<blockquote>\n<p>If we were designing a kernel for handling one application per server we would design it very differently than for a multi-user kernel.&nbsp;</p>\n\n<p><strong>Don&rsquo;t let the kernel do all the heavy lifting</strong>. Take packet handling, memory management, and processor scheduling out of the kernel and put it into the application, where it can be done efficiently. Let Linux handle the control plane and let the the application handle the data plane.</p>\n\n<ul>\n\t<li dir=\"ltr\">\n\t<p dir=\"ltr\">Servers could not handle 10K concurrent connections because of O(n^2) algorithms used in the kernel.</p>\n\n\t<ul>\n\t\t<li>\n\t\t<p dir=\"ltr\">Two basic problems in the kernel:</p>\n\n\t\t<ul>\n\t\t\t<li>\n\t\t\t<p dir=\"ltr\">Connection = thread/process. As a packet came in it would walk down all 10K processes in the kernel to figure out which thread should handle the packet</p>\n\t\t\t</li>\n\t\t\t<li>\n\t\t\t<p dir=\"ltr\">Connections = select/poll (single thread). Same scalability problem. Each packet had to walk a list of sockets.</p>\n\t\t\t</li>\n\t\t</ul>\n\t\t</li>\n\t\t<li>\n\t\t<p dir=\"ltr\">Solution: fix the kernel to make lookups in constant time</p>\n\n\t\t<ul>\n\t\t\t<li>\n\t\t\t<p dir=\"ltr\">Threads now constant time context switch regardless of number of threads.</p>\n\t\t\t</li>\n\t\t\t<li>\n\t\t\t<p dir=\"ltr\">Came with a new scalable epoll()/IOCompletionPort constant time socket lookup.</p>\n\t\t\t</li>\n\t\t</ul>\n\t\t</li>\n\t</ul>\n\t</li>\n\t<li dir=\"ltr\">\n\t<p dir=\"ltr\">Thread scheduling still didn&rsquo;t scale so servers scaled using epoll with sockets which led to the asynchronous programming model embodied in Node and Nginx.&nbsp;What Nginx says it don&rsquo;t use thread scheduling as the packet scheduler. Do the packet scheduling yourself. Use select to find the socket, we know it has data so we can read immediately and it won&rsquo;t block, and then process the data.</p>\n\t</li>\n</ul>\n\n<p dir=\"ltr\"><strong>Packet Scaling - Write Your Own Custom Driver To Bypass The Stack:</strong>&nbsp;The problem with packets is they go through the Unix kernel. The network stack is complicated and slow. The path of packets to your application needs to be more direct. Don&rsquo;t let the OS handle the packets.</p>\n\n<p dir=\"ltr\"><strong>Multi-core scalibility:</strong>&nbsp;Multi-threading is not same as multi-core. Sometimes, with more number of cores performance can go down, if the code is not writtent properly. Use wait-free algorithms and data-structures. Instead of Linux doing thread-handling handle threads on your own, as per your requirements.</p>\n\n<p dir=\"ltr\"><strong>Memory Scalability:</strong>&nbsp;The problem is if you have 20gigs of RAM and let&rsquo;s say you use 2k per connection, then if you only have 20meg L3 cache, none of that data will be in cache. It costs 300 clock cycles to go out to main memory, at which time the CPU isn&rsquo;t doing anything. Solutions: Co-locate data, Paging,&nbsp;Memory pools (pre-allocate memory&nbsp;on a per object, per thread, and per socket basis)</p>\n\n<ul>\n\t<li dir=\"ltr\">\n\t<p dir=\"ltr\">The paging table for 32gigs require 64MB of paging tables which doesn&rsquo;t fit in cache. So you have two caches misses, one for the paging table and one for what it points to. This is detail we can&rsquo;t ignore for scalable software.</p>\n\t</li>\n\t<li dir=\"ltr\">\n\t<p dir=\"ltr\">Solutions: compress data; use cache efficient structures instead of binary search tree that has a lot of memory accesses.</p>\n\t</li>\n\t<li dir=\"ltr\">\n\t<p dir=\"ltr\">Reduces page table size. Reserve memory from the start and then your application manages the memory.</p>\n\t</li>\n\t<li dir=\"ltr\">\n\t<p dir=\"ltr\">Controlling process affinity with something like libnuma/numactl.</p>\n\t</li>\n</ul>\n</blockquote>\n\n<p>&nbsp;</p>\n\n<p>Even with a single DB there are mulitple things to consider. E.g. <strong>Storage Engines</strong> can affect the performance of the application especially under high load. There are 2 fundamental implementations of storage engines:</p>\n\n<ol>\n\t<li>LSM (Log Structured Merge) Tree Based. May be better suited for situations where write throughput requirement is more than read-throughput</li>\n\t<li>B-Tree Based Storage Engines</li>\n</ol>\n\n<p>You&#39;d also have to consider whether your application is read-heavy or write-heavy or both. You&#39;d also have to consider how much consistency will be enough for your application, in some cases Eventual Consistency might be enough, in other cases full consistency is required.</p>\n\n<p><em>(Eventual Consistency is feasible only if the operations are commutative i.e. they can be run in any order without affecting the end result. It&#39;s interesting that with more progress, there&#39;ll be very few scenarios left where EC will be acceptable.)</em></p>\n\n<p>Remember: <strong>Denormalising</strong> (basically having duplicate data) can speed up the read performance, but it introduces places for inconsistency, and will make writes slower if you want to solve that. Denormalised data will also have less requirements for Joins, so that also improves read performance.</p>\n\n<p><strong>Indexes</strong> also basically duplicate some data, so they may also decrease write throughput (because everytime something is written index needs to be updated) but provide faster reads. Clustered Index store whole row with the index itself, so they don&#39;t cause extra delay (but you can have only one clustered index).</p>\n\n<p>Type of Indexes matters as well. Hashtable&nbsp;Index is much faster than B-Tree indexes (but Hash-based Index can be used only for key-value storage, and doesn&#39;t perform well for range queries).</p>\n\n<p>Also, different sort of storage engines can fit different scenarios e.g Column Storage is more fitting for analytics (OLAP) work.</p>\n\n<p>&nbsp;</p>\n\n<hr />\n<p>So, the simple setup above can handle fairly large number of users. But what are the problems that can occur:-</p>\n\n<ol>\n\t<li>What if the application server goes down, or the DB server goes down?</li>\n\t<li>What if a single machine (with all its resources) is no longer enough for your app server, or the amount of disk storage is not enough for your DB.</li>\n</ol>\n\n<p><em><strong>Can we run multiple instances of the app server?</strong></em></p>\n\n<p><strong><em>Can we run multiple instances of DB server?&nbsp;</em></strong></p>\n\n<p>Yes, we can. Although that would make things more complicated.&nbsp;</p>\n\n<p>If we are using multiple app servers, we somehow need to figure out a way to redirect client requests to one of the app servers. There could be many ways: simplest one is that client knows the addresses of the servers, chooses one of them randomly, a server can redirect to other server if it is very busy.</p>\n\n<p>But, all of this is a logic that can be encapsulated at one place:<strong> Load-Balancers</strong>. But Load-Balancers can also fail, so we need multiple instances of them as well. We can use <a href=\"https://aws.amazon.com/elasticloadbalancing/?elb-whats-new.sort-by=item.additionalFields.postDateTime&amp;elb-whats-new.sort-order=desc\">Elastic Load Balancer</a>.</p>\n\n<p>You have to also implement/configure some logic to decide how to route requests to different servers. It can be done by inspecting HTTP requests themselves (e.g. by inspecting content, request path etc).</p>\n\n<p>Remember, contention to a single resource (server, in this case) will lead to O(N^2) growth in time for completion of all requests. <a href=\"https://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/\">Backoff algorithms</a> help there.&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>You can extend it as per your liking and after considering the trade-offs. E.g. AWS ELB has Application Load Balancer which is like (ELBs &rarr; Proxies(Zuul/Envoy/Nginx/HAProxy) &rarr; ELBs&nbsp;&rarr; WebServers). The benefit here is that you can handle cross-cutting concerns at sinlge layer.</p>\n\n<p>&nbsp;</p>\n\n<p>To run multiple instances of DB, we need to figure out how because there are two different ways to do this: 1. You can run multiple DB instances each with a subset of total data (<strong>DB Sharding</strong>), 2. You can run multiple instances of DBs each having complete data (<strong>DB Clustering</strong>), 3. A combination of both, because even with clustering it&#39;s possible that all your data doesn&#39;t fit in a single machine.</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n<style type=\"text/css\">hr {\n    overflow: visible; /* For IE */\n    height: 30px;\n    border-style: solid;\n    border-color: black;\n    border-width: 1px 0 0 0;\n    border-radius: 20px;\n    width: 25%;\n}\nhr:before { /* Not really supposed to work, but does */\n    display: block;\n    content: \"\";\n    height: 30px;\n    margin-top: -31px;\n    border-style: solid;\n    border-color: black;\n    border-width: 0 0 1px 0;\n    border-radius: 20px;\n}\n</style>\n", "authorId": 1, "subject": "architecture", "tags": [], "img": "", "summary": "string", "lastUpdated": "2021-01-29 13:54:46.727420"}