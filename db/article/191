{"name":"Scalability Examples","id":191,"content":"<p>Here are some real world examples of how big companies scaled their applications.</p>\n\n<h2>DropBox</h2>\n\n<p>Requirements: <strong><em>Get Your Files Anywhere, Anytime</em></strong></p>\n\n<ul>\n\t<li>User should be able to login and upload files from their system.</li>\n\t<li>Further read/writes should be synced automatically</li>\n\t<li>Users should be able to recover their files previous version</li>\n\t<li>Should be able to work in low badnwidths</li>\n</ul>\n\n<p><strong>Challenges:</strong><br />\n&nbsp;&nbsp; &nbsp;Write Scale: This is not like most of the other services where reads are a lot more than writes. Read to write ratio is roughly one-to-one here.<br />\n&nbsp;&nbsp; &nbsp;ACIDity Requirements: Atomicity, Consistency, Isolation, Durability</p>\n\n<p>&nbsp;&nbsp; &nbsp;Deduplication<br />\n&nbsp;&nbsp; &nbsp;MetaData handling</p>\n\n<p><strong>How They Started and How They Scaled?</strong></p>\n\n<p>In year 2007, they started with a single server (with web server and MySQL) and a client app. They first faced the problem that data had consumed all disk space, they had to move data to some other place. As the load increased, they put the data (files) on Amazon S3, and metadata in MySQL DB on a separate server than web server. Next they faced the problem that downloading of files would jam other tasks like uploading due to high load. So they chose to push notifications instead of polling using a Notification Server, they split the web server into two, one running in managed hosting and other running in AWS. Server running in AWS is hosting all file contents and accepting all the uploads. Server in managed hosting is managing metadata and is backed by memcache. After this DropBox was able to easily handle around 50k users in year 2008.</p>\n\n<p>After this the architecture has remained kind of same. But they had to scale horizontally, modify memcache library for consistency. All Load balancers have hot backup. &nbsp;&nbsp;&nbsp;</p>\n\n<p><strong>Evolution of architecture:&nbsp;</strong></p>\n\n<div style=\"background:#eeeeee;border:1px solid #cccccc;padding:5px 10px;\">\n<p>Single Server (WebServer+MySQL)&nbsp;&rarr; +AmazonS3&nbsp;&rarr; +NotificationServer&nbsp;&rarr; 2 WebServers + Memcache</p>\n</div>\n\n<h2><br />\nGoogle</h2>\n\n<p><br />\n&nbsp;Storage of indexes, and documents themselves (optionally)<br />\n&nbsp;Index processing; hundreds of GBs of data<br />\n&nbsp;Queries; &nbsp;thousands per second<br />\n&nbsp;<br />\n&nbsp;People are inclined to look only a few top results;&nbsp;<br />\n&nbsp;<br />\n&nbsp;Google chose to store documents themselves, for various purposes; research (academic or otherwise); the database generated (by web crawling) can be used for various purposes.<br />\n&nbsp;<br />\n&nbsp;Page Rank: <a href=\"https://www.youtube.com/watch?v=P8Kt6Abq_rM\" target=\"_blank\">See the video description</a><br />\n&nbsp;&nbsp; &nbsp;How many citations or backlinks for the given link<br />\n&nbsp;&nbsp; &nbsp;We assume page <code>A </code>has pages <code>T1...Tn</code> which point to it (i.e., are citations). The parameter d is a damping factor which can be set between 0 and 1. We usually set d to 0.85. There are more details about d in the next section. Also <code>C(A)</code> is defined as the number of links going out of page A. The PageRank of a page A is given as follows:<br />\n<code>PR(A) = (1-d) + d (PR(T1)/C(T1) + ... + PR(Tn)/C(Tn))</code></p>\n\n<p>Note that the PageRanks form a probability distribution over web pages, so the sum of all web pages&#39; PageRanks will be one.</p>\n\n<p>PageRank or <code>PR(A)</code> can be calculated using a simple iterative algorithm, and corresponds to the principal eigenvector of the normalized link matrix of the web. Also, a PageRank for 26 million web pages can be computed in a few hours on a medium size workstation. There are many other details which are beyond the scope of this paper.</p>\n\n<p><strong>2.1.2 Intuitive Justification</strong><br />\nPageRank can be thought of as a model of user behavior. We assume there is a &quot;random surfer&quot; who is given a web page at random and keeps clicking on links, never hitting &quot;back&quot; but eventually gets bored and starts on another random page. The probability that the random surfer visits a page is its PageRank. And, the d damping factor is the probability at each page the &quot;random surfer&quot; will get bored and request another random page. One important variation is to only add the damping factor d to a single page, or a group of pages. This allows for personalization and can make it nearly impossible to deliberately mislead the system in order to get a higher ranking. We have several other extensions to PageRank, again see [Page 98].<br />\nAnother intuitive justification is that a page can have a high PageRank if there are many pages that point to it, or if there are some pages that point to it and have a high PageRank. Intuitively, pages that are well cited from many places around the web are worth looking at. Also, pages that have perhaps only one citation from something like the Yahoo! homepage are also generally worth looking at. If a page was not high quality, or was a broken link, it is quite likely that Yahoo&#39;s homepage would not link to it. PageRank handles both these cases and everything in between by recursively propagating weights through the link structure of the web.</p>\n\n<p>The text of links is treated in a special way in our search engine.&nbsp;<br />\nPropagating anchor text to the page it refers to (useful for images, media files, DBs).</p>\n\n<p>If a user issues a query like &quot;Bill Clinton&quot; they should get reasonable results since there is a enormous amount of high quality information available on this topic. Given examples like these, we believe that the standard information retrieval work (like vector space model) needs to be extended to deal effectively with the web.</p>\n\n<p>Examples of external meta information include things like reputation of the source, update frequency, quality, popularity or usage, and citations.<br />\nExternal meta information is useful.&nbsp;</p>\n\n<p><strong>Architecture:</strong><br />\n(1) URL-Server sends list to crawl; to WebCrawler(s) distributed crawlers<br />\n(2) WebCrawler fetches the web page; compresses and stores that into repository.<br />\n(3) Indexer reads the repository, uncompresses the documents, and parses them. Each document is converted into a set of word occurrences called hits [The hits record the word, position in document, an approximation of font size, and capitalization].&nbsp;<br />\n&nbsp; &nbsp; &nbsp;The indexer distributes these hits into a set of &quot;barrels&quot;, creating a partially sorted forward index.&nbsp;<br />\n&nbsp; &nbsp; &nbsp;It parses out all the links in every web page and stores important information about them in an anchors file. This file contains enough information to determine where each link points from and to, and the text of the link.<br />\n(4) The URLresolver reads the anchors file and converts relative URLs into absolute URLs and in turn into docIDs. It puts the anchor text into the forward index, associated with the docID that the anchor points to.&nbsp;<br />\n&nbsp; &nbsp; &nbsp;It also generates a database of links which are pairs of docIDs.<br />\n(5) The sorter takes the barrels, which are sorted by docID (in simple words), and resorts them by wordID to generate the inverted index<br />\n&nbsp;&nbsp; &nbsp;The sorter also produces a list of wordIDs and offsets into the inverted index.<br />\n(6) A program called DumpLexicon takes this list together with the lexicon produced by the indexer and generates a new lexicon to be used by the searcher.<br />\n(7) The searcher is run by a web server and uses the lexicon built by DumpLexicon together with the inverted index and the PageRanks to answer queries.</p>\n\n<p>BigFiles are virtual files spanning multiple file systems and are addressable by 64 bit integers. The allocation among multiple file systems is handled automatically. The BigFiles package also handles allocation and deallocation of file descriptors, since the operating systems do not provide enough for our needs. BigFiles also support rudimentary compression options.</p>\n\n<p>The repository requires no other data structures to be used in order to access it.&nbsp;<br />\nIt has memory layout like: Sync|Lenthe|Packet<br />\nPacket has this format: docId|ecode|urlLen|pageLen|url|page</p>\n\n<p><strong>Document Index:</strong><br />\nThe document index keeps information about each document. It is a fixed width ISAM (Index sequential access mode) index, ordered by docID.&nbsp;<br />\nThe information stored in each entry includes the current document status, a pointer into the repository, a document checksum, and various statistics. If the document has been crawled, it also contains a pointer into a variable width file called docinfo which contains its URL and title. Otherwise the pointer points into the URLlist which contains just the URL. This design decision was driven by the desire to have a reasonably compact data structure, and the ability to fetch a record in one disk seek during a search<br />\nAdditionally, there is a file which is used to convert URLs into docIDs. It is a list of URL checksums with their corresponding docIDs and is sorted by checksum. In order to find the docID of a particular URL, the URL&#39;s checksum is computed and a binary search is performed on the checksums file to find its docID. URLs may be converted into docIDs in batch by doing a merge with this file. This is the technique the URLresolver uses to turn URLs into docIDs. This batch mode of update is crucial because otherwise we must perform one seek for every link which assuming one disk would take more than a month for our 322 million link dataset.</p>\n\n<p><a href=\"http://infolab.stanford.edu/~backrub/google.html#ref\" target=\"_blank\">Source: Standford paper</a></p>\n\n<p><a href=\"https://www.youtube.com/watch?v=Wf6HbY2PQDw\" target=\"_blank\">To understand Inverted Index</a>, <a href=\"https://www.youtube.com/watch?v=5KbynCj7yRQ&amp;list=PLaZQkZp6WhWwoDuD6pQCmgVyDbUWl_ZUi&amp;index=4\">Querying an Inverted Index for complex queries</a></p>\n\n<div style=\"background:#eeeeee;border:1px solid #cccccc;padding:5px 10px;\">\n<p>Summary of Inverted Index: Given a list of documents containing words (after linguistic transformations, normalizations), Create a map of (Word, Frequency) :-&gt; SortedLinkedList of DocumentIds containing the word<br />\n<br />\nFrequency, and Sorted list help in faster algorithm for querying.</p>\n</div>\n\n<h2>Instagram</h2>\n\n<p><br />\nChose PostgreSQL.&nbsp;<br />\nChallenges:&nbsp; How would you uniquely identify a photo, and put into correct DB for Sharding?<br />\nRequirements:<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;Ids should be sortable by time e.g. Photo IDs sortable without fetching photo information<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;IDs should ideally be 64 bits&nbsp;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;<br />\nID Generation challenges:</p>\n\n<ul>\n\t<li>Uniqueness</li>\n\t<li>Contention</li>\n</ul>\n\n<p>Viable Options / Choices:</p>\n\n<ul>\n\t<li>MongoDB ObjectId</li>\n\t<li>UUID</li>\n\t<li>Generate IDs through dedicated serviceEx: Twitter&rsquo;s Snowflake, a Thrift service that uses Apache ZooKeeper to coordinate nodes and then generates 64-bit unique IDs</li>\n\t<li>DB Ticket Servers: Uses the database&rsquo;s auto-incrementing abilities to enforce uniqueness. Flickr uses this approach, but with two ticket DBs (one on odd numbers, the other on even) to avoid a single point of failure.</li>\n</ul>\n\n<p><strong>Solution:</strong><br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;Custom SQL function<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;</p>\n\n<pre>\n<code class=\"language-sql\">CREATE OR REPLACE FUNCTION insta5.next_id(OUT result bigint) AS $$\n            DECLARE\n                our_epoch bigint := 1314220021721;\n                seq_id bigint;\n                now_millis bigint;\n                shard_id int := 5;\n            BEGIN\n                SELECT nextval('insta5.table_id_seq') %% 1024 INTO seq_id;\n                SELECT FLOOR(EXTRACT(EPOCH FROM clock_timestamp()) * 1000) INTO now_millis;\n                result := (now_millis - our_epoch) &lt;&lt; 23;\n                result := result | (shard_id &lt;&lt; 10);\n                result := result | (seq_id);\n            END;\n                $$ LANGUAGE PLPGSQL;</code></pre>\n\n<p><br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;<br />\nAnd when creating the table, we do:</p>\n\n<pre>\n<code class=\"language-sql\"> CREATE TABLE insta5.our_table (\n                \"id\" bigint NOT NULL DEFAULT insta5.next_id(),\n                ...rest of table schema...\n              )</code></pre>\n\n<p>&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;<br />\nCan easily get shard_id from PK itself.<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;<br />\n1024 uniqueIds per ShardId per milliseconds<br />\n&nbsp;&nbsp; &nbsp;<br />\nLet&rsquo;s walk through an example: let&rsquo;s say it&rsquo;s September 9th, 2011, at 5:00pm and our &lsquo;epoch&rsquo; begins on January 1st, 2011. There have been 1387263000 milliseconds since the beginning of our epoch, so to start our ID, we fill the left-most 41 bits with this value with a left-shift:</p>\n\n<p><code>id = 1387263000 &lt;&lt;(64-41)</code><br />\nNext, we take the shard ID for this particular piece of data we&rsquo;re trying to insert. Let&rsquo;s say we&rsquo;re sharding by user ID, and there are 2000 logical shards; if our user ID is 31341, then the shard ID is 31341 % 2000 -&gt; 1341. We fill the next 13 bits with this value:</p>\n\n<p><code>id |= 1341 &lt;&lt;(64-41-13)</code><br />\nFinally, we take whatever the next value of our auto-increment sequence (this sequence is unique to each table in each schema) and fill out the remaining bits. Let&rsquo;s say we&rsquo;d generated 5,000 IDs for this table already; our next value is 5,001, which we take and mod by 1024 (so it fits in 10 bits) and include it too:</p>\n\n<p><code>id |= (5001 % 1024)</code><br />\nWe now have our ID, which we can return to the application server using the RETURNING keyword as part of the INSERT.</p>\n\n<h2><br />\nFlickr</h2>\n\n<p>Why not GUIDs?<br />\nMostly because GUIDs are big, and they index badly in MySQL. One of the ways we keep MySQL fast is we index everything we want to query on, and we only query on indexes. So index size is a key consideration.</p>\n\n<p>Consistent Hashing?<br />\nSome projects like Amazon&rsquo;s Dynamo provide a consistent hashing ring on top of the datastore to handle the GUID/sharding issue. This is better suited for write-cheap environments (e.g. LSMTs), while MySQL is optimized for fast random reads.</p>\n\n<p>Final Solution:<br />\nFlickr ticket server is a dedicated database server, with a single database on it, and in that database there are tables like Tickets32 for 32-bit IDs, and Tickets64 for 64-bit IDs.</p>\n\n<pre>\n<code class=\"language-sql\">CREATE TABLE `Tickets64` (\n  `id` bigint(20) unsigned NOT NULL auto_increment,\n  `stub` char(1) NOT NULL default '',\n  PRIMARY KEY  (`id`),\n  UNIQUE KEY `stub` (`stub`)\n) ENGINE=MyISAM</code></pre>\n\n<p><code>SELECT * from Tickets64</code> returns a single row that looks something like:</p>\n\n<p>When I need a new globally unique 64-bit ID I issue the following SQL:<br />\n+-----------------------------+------+<br />\n| id&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| stub |<br />\n+-----------------------------+------+<br />\n| 72157623227190423 | &nbsp; &nbsp;a&nbsp; &nbsp;|<br />\n+-----------------------------+------+</p>\n\n<p>&nbsp;</p>\n\n<pre>\n<code class=\"language-sql\">REPLACE INTO Tickets64 (stub) VALUES ('a');\nSELECT LAST_INSERT_ID();</code></pre>\n\n<p>Two ticket servers for HA.<br />\nTo avoid contention: they are for even and odd.<br />\nTicketServer1:<br />\nauto-increment-increment = 2<br />\nauto-increment-offset = 1</p>\n\n<p>TicketServer2:<br />\nauto-increment-increment = 2<br />\nauto-increment-offset = 2</p>\n\n<p>We round robin between the two servers to load balance and deal with down time. The sides do drift a bit out of sync, I think we have a few hundred thousand more odd number objects then evenly numbered objects at the moment, but this hurts no one.</p>\n\n<p>We have a sequences for Photos, for Accounts, for OfflineTasks, and for Groups, etc.<br />\nGroups, and Accounts get their own sequence because we get comparatively so few of them. Photos have their own sequence that we made sure to sync to our old auto-increment table when we cut over because its nice to know how many photos we&rsquo;ve had uploaded, and we use the ID as a short hand for keeping track.Pinterest:<br />\n......................................................<br />\nClustering vs Sharding: They chose sharding<br />\nClustering Examples: Cassandra, MemBase, HBase<br />\nSharding:<br />\n&nbsp;&nbsp; &nbsp;Cons: Joins are performed in the application layer.<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;Schema changes require more planning.<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;Constraint handling moves to application logic.<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;Lost all transaction capabilities. A write to one database may fail when a write to another succeeds.<br />\n&nbsp;&nbsp; &nbsp;Added tons of cache; Basically every query has to be cached.<br />\nSharding mechanisms:<br />\n&nbsp;&nbsp; &nbsp;Looked at Cassandra&rsquo;s ring model. Looked at Membase. And looked at Twitter&rsquo;s Gizzard.</p>\n\n<h2>Pinterest</h2>\n\n<p>&nbsp;&nbsp;<br />\n<a href=\"https://medium.com/@Pinterest_Engineering/sharding-pinterest-how-we-scaled-our-mysql-fleet-3f341e96ca6f\" target=\"_blank\">Sharding on Pinterest</a></p>\n\n<blockquote>\n<p>&quot; Once you&rsquo;re sharded, there&rsquo;s generally no advantage to interacting with a slave in production. &quot;</p>\n</blockquote>\n\n<p>(1) Generate UUIDs</p>\n\n<p>Chose MySQL instead of auto-scaling solutions like MongoDB, Cassandra, Membase especially because they were not mature enough at that time.</p>\n\n<blockquote>\n<p><em class=\"id\">Aside: I still recommend startups avoid the fancy new stuff &mdash; try really hard to just use MySQL. Trust me. I have the scars to prove it. (<strong>In March, 2015</strong>)</em></p>\n</blockquote>\n\n<p>Started with one MySQL instances each on eight EC2 servers. Each MySQL server is master-master replicated onto a backup host in case the primary fails. Each MySQL instance can have multiple DBs (with unique names).</p>\n\n<p>A configuration table has {Range, Master_Instance_ID, Slave_Instance_ID}. This config is in ZooKeeper service.</p>\n\n<p>Each shard contains the same set of tables: pins, boards, users_has_pins, users_likes_pins, pin_liked_by_user, etc.</p>\n\n<p>Initially created a lot of virtual shards. 8 physical servers, each with 512 DBs. All the databases have all the tables.<br />\nFor high availability they always run in multi-master replication mode. Each master is assigned to a different availability zone. On failure the switch to the other master and bring in a new replacement node.</p>\n\n<p>64 bits:<br />\n&nbsp;&nbsp; &nbsp;shard ID: 16 bits<br />\n&nbsp;&nbsp; &nbsp;type : 10 bits - Pin, Board, User, or any other object type<br />\n&nbsp;&nbsp; &nbsp;local ID - rest of the bits for the ID within the table. Uses MySQL auto increment.</p>\n\n<p class=\"hm hn eq bh ho b hp hq hr hs ht hu hv hw hx hy hz\" data-selectable-paragraph=\"\" id=\"7d38\">Given this Pin: <a class=\"aq cc iw ix iy iz\" href=\"https://www.pinterest.com/pin/241294492511762325/?source=post_page---------------------------\">https://www.pinterest.com/pin/241294492511762325/</a>, let&rsquo;s decompose the Pin ID 241294492511762325:</p>\n\n<pre class=\"jj jk jl jm jn fx do jy\">\n<span class=\"ie if eq bh jz b dg ka kb n kc\" data-selectable-paragraph=\"\" id=\"33d8\">Shard ID = (241294492511762325 &gt;&gt; 46) &amp; 0xFFFF = 3429\nType ID  = (241294492511762325 &gt;&gt; 36) &amp; 0x3FF = 1\nLocal ID = (241294492511762325 &gt;&gt;  0) &amp; 0xFFFFFFFFF = 7075733</span></pre>\n\n<p class=\"hm hn eq bh ho b hp hq hr hs ht hu hv hw hx hy hz\" data-selectable-paragraph=\"\" id=\"7943\">So this Pin object lives on shard 3429. It&rsquo;s type is 1 (i.e. &lsquo;Pin&rsquo;), and it&rsquo;s in the row 7075733 in the pins table. For an example, let&rsquo;s assume this shard is on MySQL012A. We can get to it as follows:</p>\n\n<pre class=\"jj jk jl jm jn fx do jy\">\n<span class=\"ie if eq bh jz b dg ka kb n kc\" data-selectable-paragraph=\"\" id=\"7b0d\">conn = MySQLdb.connect(host=&rdquo;MySQL012A&rdquo;)\nconn.execute(&ldquo;SELECT data FROM db03429.pins where local_id=7075733&rdquo;)</span></pre>\n\n<p class=\"hm hn eq bh ho b hp hq hr hs ht hu hv hw hx hy hz\" data-selectable-paragraph=\"\" id=\"8ef2\">There are two types of data: objects and mappings. Objects contain details, such as Pin data. Object tables, such as Pins, users, boards and comments, have an ID (the local ID, an auto-incrementing primary key) and a blob of data that contains a JSON with all the object&rsquo;s data.</p>\n\n<pre class=\"jj jk jl jm jn fx do jy\">\n<span class=\"ie if eq bh jz b dg ka kb n kc\" data-selectable-paragraph=\"\" id=\"e1a2\">CREATE TABLE pins (\n  local_id INT PRIMARY KEY AUTO_INCREMENT,\n  data TEXT,\n  ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n) ENGINE=InnoDB;</span></pre>\n\n<p class=\"hm hn eq bh ho b hp hq hr hs ht hu hv hw hx hy hz\" data-selectable-paragraph=\"\" id=\"2ade\">To edit a Pin, we read-modify-write the JSON under a <a class=\"aq cc iw ix iy iz\" href=\"http://dev.mysql.com/doc/refman/5.7/en/innodb-locking-reads.html?source=post_page---------------------------\">MySQL transaction</a>:</p>\n\n<pre class=\"jj jk jl jm jn fx do jy\">\n<span class=\"ie if eq bh jz b dg ka kb n kc\" data-selectable-paragraph=\"\" id=\"3b8c\">&gt; BEGIN\n&gt; SELECT blob FROM db03429.pins WHERE local_id=7075733 FOR UPDATE\n[Modify the json blob]\n&gt; UPDATE db03429.pins SET blob=&rsquo;&lt;modified blob&gt;&rsquo; WHERE local_id=7075733\n&gt; COMMIT</span></pre>\n\n<p class=\"hm hn eq bh ho b hp hq hr hs ht hu hv hw hx hy hz\" data-selectable-paragraph=\"\" id=\"e9f3\">To delete a Pin, you can delete its row in MySQL.</p>\n\n<p class=\"ie if eq bh bg fj ig ih ii ij ik il im in io ip iq\" data-selectable-paragraph=\"\" id=\"08cc\">Mapping Tables</p>\n\n<p class=\"hm hn eq bh ho b hp ir hr is ht it hv iu hx iv hz\" data-selectable-paragraph=\"\" id=\"8997\">A mapping table links one object to another, such as a board to the Pins on it. The MySQL table for a mapping contains three columns: a 64 bit &lsquo;from&rsquo; ID, a 64 bit &lsquo;to&rsquo; ID and a sequence ID. There are index keys on the (from, to, sequence) triple, and they live on the shard of the &lsquo;from&rsquo; ID.</p>\n\n<pre class=\"jj jk jl jm jn fx do jy\">\n<span class=\"ie if eq bh jz b dg ka kb n kc\" data-selectable-paragraph=\"\" id=\"6b4d\">CREATE TABLE board_has_pins (\n  board_id INT,\n  pin_id INT,\n  sequence INT,\n  INDEX(board_id, pin_id, sequence)\n) ENGINE=InnoDB;</span></pre>\n\n<p class=\"hm hn eq bh ho b hp hq hr hs ht hu hv hw hx hy hz\" data-selectable-paragraph=\"\" id=\"4ef0\">Mapping tables are unidirectional, such as a board_has_pins table. If you need the opposite direction, you&rsquo;ll need a separate pin_owned_by_board table.</p>\n\n<p class=\"hm hn eq bh ho b hp hq hr hs ht hu hv hw hx hy hz\" data-selectable-paragraph=\"\" id=\"4ef0\">The sequence ID gives an ordering (our ID&rsquo;s can&rsquo;t be compared across shards as the new local ID offsets diverge). We usually insert new Pins into a new board with a sequence ID = unix timestamp. The sequence can be any numbers, but a unix timestamp is a convenient way to force new stuff always higher since time monotonically increases. You can look stuff up in the mapping table like this:</p>\n\n<pre class=\"jj jk jl jm jn fx do jy\">\n<span class=\"ie if eq bh jz b dg ka kb n kc\" data-selectable-paragraph=\"\" id=\"a8a5\">SELECT pin_id FROM board_has_pins \nWHERE board_id=241294561224164665 ORDER BY sequence \nLIMIT 50 OFFSET 150</span></pre>\n\n<p class=\"hm hn eq bh ho b hp hq hr hs ht hu hv hw hx hy hz\" data-selectable-paragraph=\"\" id=\"aa9d\">This will give you up to 50 pin_ids, which you can then use to look up Pin objects.</p>\n\n<p>What we&rsquo;ve just done is an application layer join (board_id -&gt; pin_ids -&gt; pin objects). One awesome property of application layer joins is that you can cache the mapping separate from the object. We keep pin_id -&gt; pin object cache in a memcache cluster, but we keep board_id -&gt; pin_ids in a redis cluster. This allows us to choose the right technology to best match the object being cached.</p>\n\n<p>&nbsp;</p>\n\n<p>Twitter uses a mapping table to map IDs to a physical host. Which requires a lookup. Since Pinterest is on AWS and MySQL queries took about 3ms, they decided this extra level of indirection would not work. They build the location into the ID.</p>\n\n<p>New users are randomly distributed across shards.</p>\n\n<p>All data (pins, boards, etc) for a user is collocated on the same shard. Huge advantage. Rendering a user profile, for example, does not take multiple cross shard queries. It&rsquo;s fast.</p>\n\n<p>Every application has a configuration file that maps a shard range to a physical host.</p>\n\n<p>&ldquo;sharddb001a&rdquo;: : (1, 512)</p>\n\n<p>&ldquo;sharddb001b&rdquo;: : (513, 1024) - backup hot master</p>\n\n<p>If you want to look up a User whose ID falls into sharddb003a:</p>\n\n<p>Decompose the ID into its parts.</p>\n\n<p>Perform the lookup in the shard map</p>\n\n<p>Connect to the shard, go to the database for the type, and use the local ID to find the right user and return the serialized data.</p>\n\n<p>&nbsp;&nbsp; &nbsp;<br />\nAll data is either an object (pin, board, user, comment) or a mapping (user has boards, pins has likes).<br />\nFor objects a Local ID maps to a MySQL blob. The blob format started with JSON but is moving to serialized thrift.</p>\n\n<p>For mappings there&rsquo;s a mapping table. &nbsp;You can ask for all the boards for a user. The IDs contain a timestamp so you can see the order of events.</p>\n\n<p>There&rsquo;s a reverse mapping, many to many table, to answer queries of the type give me all the users who like this pin.</p>\n\n<p>Schema naming scheme is noun_verb_noun: user_likes_pins, pins_like_user.</p>\n\n<p>Queries are primary key or index lookups (no joins).</p>\n\n<p>Data doesn&rsquo;t move across database as it does with clustering. Once a user lands on shard 20, for example, and all the user data is collocated, it will never move. The 64 bit ID has contains the shard ID so it can&rsquo;t be moved. You can move the physical data to another database, but it&rsquo;s still associated with the same shard.</p>\n\n<p>All tables exist on all shards. No special shards, not counting the huge user table that is used to detect user name conflicts.</p>\n\n<p>No schema changes required and a new index requires a new table.</p>\n\n<p>Since the value for a key is a blob, you can add fields without destroying the schema. There&rsquo;s versioning on each blob so applications will detect the version number and change the record to the new format and write it back. All the data doesn&rsquo;t need to change at once, it will be upgraded on reads.</p>\n\n<p>Huge win because altering a table takes a lock for hours or days. &nbsp;If you want a new index you just create a new table and start populating it. When you don&rsquo;t want it anymore just drop it. (no mention of how these updates are transaction safe).</p>\n\n<p>&nbsp;</p>\n\n<h2>URL Shortening Service</h2>\n\n<p>f(URL) = shortURL<br />\ng(shortURL) = f<br />\nNo collision</p>\n\n<p>base62(md5(url))</p>\n\n<p>For a new URL:<br />\nGet auto-incremented key (base-10 number); Convert it into base-62; map the digits of this to characters.<br />\nWhy 62?&nbsp;<br />\nBecause [a-zA-Z0-9] would cover all URLs; and this set has only 62 possible values.</p>\n\n<p>For reverse lookup, Convert base-62 to id and find in table.</p>\n\n<p>&nbsp;</p>\n\n<h2>Twitter</h2>\n\n<p><br />\nEarlier sharded temporarily e.g. daily tweets. But it filled up machines quickly, because people are often interested in latest tweets, so old machines were not getting work much.</p>\n\n<p>Typical database server config: HP DL380, 72GB RAM, 24 disk RAID10. Good balance of memory and disk.</p>\n\n<p>Tech Stack:<br />\nGizzard built on top of MySQL; distributed datastore<br />\nSnowflake for unique ID generation.<br />\nCassandra<br />\nHadoop -&nbsp;<br />\nVertika - for analytics<br />\n&nbsp;</p>\n\n<h2>Medium</h2>\n\n<p><br />\n&nbsp;&nbsp; &nbsp;Why?<br />\n&nbsp;&nbsp; &nbsp;- Certain parts of the app were I/O heavy not good for Node.js, suffering performance; but couldn&#39;t change tech stack so easily.<br />\n&nbsp;&nbsp; &nbsp;- Features tighltly coupled; deployment process slow<br />\n&nbsp;&nbsp; &nbsp;- Scaling up certain tasks, or isolating resource concerns for different types of task; wasn&#39;t easy.<br />\n&nbsp;&nbsp; &nbsp;<br />\n&nbsp;&nbsp; &nbsp;From Monolith to Microservice:<br />\n&nbsp;&nbsp; &nbsp;- New service for something only if it has product value/engineering value.<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;Faster shipping, performance, efficiency of new features is a product value.<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;Helpful for engineering team is engineering value.<br />\n&nbsp;&nbsp; &nbsp;<br />\n&nbsp;&nbsp; &nbsp;Why not shared storage for services?<br />\n&nbsp;&nbsp; &nbsp;- What if you decide that some other DB is suitable for some microservice, how much change will it trigger?<br />\n&nbsp;&nbsp; &nbsp;- What if you decide data model for some service will change?<br />\n&nbsp;&nbsp; &nbsp;- Violates loose coupling<br />\n&nbsp;&nbsp; &nbsp;<br />\n&nbsp;&nbsp; &nbsp;Modification in one service behavior through persistence changes, it should not propagate to other services.<br />\n&nbsp;&nbsp; &nbsp;<br />\n&nbsp;&nbsp; &nbsp;Networking:<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;e.g., service discovery, routing, load balancing, traffic routing, etc<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;At Medium, we use Istio and Envoy as sidecar proxy.&nbsp;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;<br />\n&nbsp;&nbsp; &nbsp;Communication Protocol:<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;At Medium, we chose gRPC.</p>\n\n<p>&nbsp;</p>\n\n<p><br />\n&nbsp;</p>\n","authorId":null,"subject":"architecture","tags":["basic","intro"],"img":null,"summary":"A summary of how large companies have scaled their popular products","lastUpdated":"2020-12-05T16:33:56.397+0000"}