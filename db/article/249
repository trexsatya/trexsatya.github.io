{"name":"MySQL Cluster (NDB) Architecture","id":249,"content":"<p>&nbsp;</p>\n\n<div style=\"font-size: smaller;\">\n<pre>\n/**\n  Description of NDB Software Architecture\n  ----------------------------------------\n\n  The NDB software architecture has two foundations, blocks and signals.\n  The base object for the blocks is the below SimulatedBlock class and\n  the signal object is the base class Signal defined in VMSignal.hpp.\n\n  Blocks are intended as software units that owns its own data and it\n  communicates with other blocks only through signals. Each block owns\n  its own set of data which it entirely controls. There has been some\n  optimisations where blocks always executing in the same thread can do\n  some shortcuts by calling functions in a different block directly.\n  There is even some code to call functions in a block in a different\n  thread, in this case however some mutex is required to protect the\n  data.\n\n  Blocks are gathered together in threads. Threads are gathered into nodes.\n  So when sending a signal you need to send it to an address. The address is\n  a 32-bit word. It is a bit similar to IPv4 addresses. The address is\n  setup in the following manner:\n\n  -- Bit 0-8 ------- Bit 9-15 ------ Bit 16-31 ------\n  | Block number  | Thread id   |       NodeId      |\n  ---------------------------------------------------\n\n  So when delivering a signal we start by checking the node id. If the node\n  id is our own node id, then we will continue checking thread id. If it\n  is destined to another node, then we move the signal sending to the module\n  that takes care of transporting the signal to another node in the cluster.\n  \n  Each other node is found using a socket over TCP/IP. The architecture\n  supports also other ways to transport signals such as using some form\n  of shared memory between processes on the same or different machines.\n  It would also be possible to extend the architecture such that we\n  might use different sockets for different threads in the node.\n\n  If the signal is destined for a different thread then we transport the\n  signal to that thread, we use a separate memory buffer for each two\n  threads that communicate such that the communication between threads is\n  completely lock-free.\n\n  One block number can be used in several threads. So e.g. the LDM threads\n  all contain its own instance of the DBLQH block. The method instance()\n  gets the instance number of the currently executing block. The method\n  reference() gets the block reference of the currently executing block.\n\n  If we send to ourselves we put the signal in the memory buffer for\n  communication with our own thread.\n\n  The current limits of the architecture is a maximum of 512 block numbers.\n  We currently use less than 25 of those numbers. The maximum number of\n  threads are 128 threads. We currently can use at most 72 threads.\n  The current node limit is 255 nodes and node id 0 is a special case.\n\n  So there is still a lot of space in the addressing mechanism for growth\n  in terms of number of threads, blocks and nodes and even for introduction\n  of new addressable units like subthreads or similar things.\n\n  The software architecture also contains a structure for how signals are\n  structured. Each signal is sent at a certain priority level. Finally also\n  there is a concept of sending delayed signals to blocks within the same\n  thread.\n\n  Priority level on signals\n  -------------------------\n  So starting with priority level a signal can be sent on high priority\n  (JBA) and normal priority (JBB). The priority level can be used also when\n  sending to other nodes. The priority will however not be used to prioritise\n  the signal in sending it over the socket to the receiving node.\n\n  Each thread has its own buffer for A-priority signals. In the scheduler\n  we will always execute all signals in the A-priority buffer first. If\n  new A-priority signals are sent during these signals, then they will also\n  be executed until no more signals exist on A-priority level. So it&#39;s not\n  allowed to have a flow of signals all executing at A-level. We always have\n  to insert a signal in the flow that either goes down to B-level or use some\n  form of delayed signal.\n\n  If an A-level signal is sent from a B-level signal it will be handled\n  differently in the single threaded ndbd and the multi-threaded ndbmtd. In\n  ndbmtd it will be executed after executing up to 128 B-level signals. In\n  ndbd it will be executed as the next signal. So one cannot assume that an\n  A-level signal will be executed before a specific B-level signal. A B-level\n  signal can even be executed before an A-level signal although it was sent\n  after the A-level signal.\n\n  Delayed signals\n  ---------------\n  Delayed signals are used to create threads of activities that execute without\n  consuming too much CPU activity. Delayed signals can only be sent internally\n  within the same thread. When the signal has been delayed and is taken out of\n  its timer queue its inserted into the priority A buffer.\n\n  Bounded delay signals\n  ---------------------\n  A special form of delayed signal also exists, this is sent with delay equal to\n  the constant BOUNDED_DELAY. This means that the signal will be executed as a\n  priority A task as soon as the current set of B-level tasks are done. This is\n  similar to sending an A-level signal from a B-level job in ndbmtd. However for\n  ndbd it&#39;s not the same thing and also when sending an A-level signal from an\n  A-level signal it is also not the same thing.\n\n  So a delayed signal with delay BOUNDED_DELAY is a special type of signal\n  with a bounded delay. The bound is that no more than 100 B-level signals will\n  be executed before this signal is executed. Given our design requirements\n  a B-level signal should mostly be executed within at most 5-10 microseconds\n  or so, mostly much shorter than this even, so a normal execution time of\n  a signal would be below 1 microsecond. So 100 signals should almost never\n  execute for more than 1000 microseconds and rarely go beyond even 100\n  microseconds.\n\n  So these bounded delay signals are a good tool to ensure that activitites\n  such as backups, checkpoints, node recovery activities, altering of tables\n  and similar things gets executed at a certain rate. Without any possibility\n  of bounded delay signals it is very hard to implement an activity that gets\n  executed at a certain rate.\n\n  So in a sense we&#39;re using the bounded delay signals to implement a form of\n  time-sharing priority, certain activities are allowed to use a proportion\n  of the available CPU resources, not too much, but also not too little. If\n  an LCP gets bogged down by user transactions then the system will eventually\n  run out of REDO log space. If a node recovery activity gets bogged down by\n  user transactions then we will run for too long with only one replica in the\n  node group which is bad for system availability.\n\n  Execute direct signals\n  ----------------------\n  If the receiving block is within the same thread, then it is possible to\n  send the signal using the method EXECUTE_DIRECT. This execution will\n  happen immediately and won&#39;t be scheduled for later, it will be done in\n  the same fashion as a function call.\n\n  Signals\n  -------\n  Signals are carried with a certain structure:\n  1) Each signal has a signal number. This number also is mapped to a name.\n     When executing a signal with a certain number which e.g. has the name\n     TCKEYREQ, then this signal is implemented by a method called\n     execTCKEYREQ in the receiving block. More than one block could have\n     such a method since a signal is not tied to a certain block.\n\n  2) Each signal has 4 areas that can be sent in the signal. The first is\n     always sent in the signal, this is the fixed part. The fixed part\n     consists of at least 1 and at most 25 32-bit words. Many signals have\n     a class that defines this part of the signal. This is however not\n     absolutely necessary. Then there are up to 3 sections that can carry\n     longer information bits. So e.g. a TCKEYREQ has one section that contains\n     the primary key and another part that contains the attribute information.\n     The attribute information could be seen as a program sent to MySQL\n     Cluster data nodes to read, update the row specified in the key\n     section. The attribute information could also contain interpreted\n     programs that can do things like increment, decrement, conditional\n     update and so forth.\n\n   3) As mentioned above each signal carries a certain priority level to\n      execute it on. It is currently not possible to check the prio\n      level you&#39;re currently executing on, but it would be real simple\n      to add this capability if necessary.\n\n   4) When executing a certain signal it gets a signal id, this id is\n      local to the thread and is incremented by one each new signal that\n      is executed. This signal id is available in the Signal class and\n      can e.g. be used to deduce if the thread is currently at high load.\n\n   A signal is sent over the socket using a special protocol that is called\n   Protocol6. This is not discussed more here, it is a feature of the\n   transport mechanisms of the NDB Software Architecture.\n\n   CONTINUEB\n   ---------\n   CONTINUEB is a special signal used by almost all blocks. This signal is\n   used to handle background thread activities. Often the CONTINUEB signals\n   are used as part of implementing a more complex action. One example is\n   when DBDIH starts up a new LCP. It sends various forms of CONTINUEB\n   signals to itself to move ahead through the LCP actions it needs to do\n   as part of starting up a new LCP. The first word contains the type of\n   CONTINUEB signal, so this is in a sense a bit like a second level of\n   signal number. Based on this number the CONTINUEB data is treated\n   differently.\n\n   Common patterns of signals\n   --------------------------\n   There is no absolute patterns for how signal data looks like. But it is\n   very common that a signal at least contains the destination object id,\n   the senders object id and the senders block reference. The senders block\n   reference is actually also available in the Signal class when executing\n   a signal. But we can use various forms of software routing of the\n   signal, so the senders block reference is the originator of the signal,\n   not necessarily the same as the sender of the signal since it could be\n   routed through several blocks on the way.\n\n   The basic data type in the NDB signals are unsigned 32-bit integers. So\n   all addressing is using a special form of pointers. The pointers always\n   refers to a special class of objects and the pointer is the index in an\n   array of objects of this kind. So we can have up to 4 billion objects of\n   most kinds. If one needs to send strings and 64-bit integers one follows\n   various techniques to do this. Signals are sent in the endian order of\n   the machine they were generated, so machines in a cluster has to be\n   of the same type of endian.\n\n   ROUTE_SIGNAL\n   ------------\n   ROUTE_SIGNAL is a special signal that can be used to carry a signal\n   in a special path to ensure that it arrives in the correct order to\n   the receiving block.\n\n   Signal order guarantees\n   -----------------------\n   The following signal order guarantees are maintained.\n\n   1) Sending signals at the same priority level to the same block reference\n      from one block will arrive in the order they were sent.\n\n      It is not guaranteed if the priority level is different for the signals,\n      it is also not guaranteed if they are sent through different paths.\n      Not even sending in the following pattern has a guarantee on the\n      delivery order. Signal 1: Block A -&gt; Block B, Signal 2: Block A -&gt;\n      Block C -&gt; Block B. Although the signal 2 uses a longer path and is\n      destined to the same block it can still arrive before signal 1 at\n      Block B. The reason is that we execute signals from one sender at a\n      time, so we might be executing in Block C very quickly whereas the\n      thread executing Block B might be stalled and then when Block C has\n      sent its signal the thread executing Block B wakes up and decides\n      to execute signals from Block C before signals from Block A.\n\n   So as can be seen there is very little support for signal orders in the\n   NDB software architecture and so most protocols have to take into\n   account that signals can arrive in many different orders.\n\n   Fragmented signals\n   ------------------\n   It is possible to send really long signals. These signals cannot be\n   sent as one signal though. They are sent as one signal, then they will\n   be split up into multiple signals. The fixed part is the same in all\n   signals. What mainly differs is that they each contain a part of each\n   segment.\n\n   When receiving such a signal one should always call assembleFragments()\n   at first to see if the entire signal has arrived first. The signal\n   executor method is executed once for each signal fragment that is sent.\n   When all fragments have arrived then they will contain the full signal\n   with up to 3 sections that can much longer than the normal sized signals\n   that have limitations on the size of the signals.\n\n   Tracing infrastructure\n   ----------------------\n   All signals are sent through memory buffers. At crashes these memory\n   buffers can be used to print the last executed signals in each thread.\n   This will aid in looking for reasons for the crash. There will be one\n   file generated for each thread in the ndbmtd, in the case of ndbd there\n   will be only one file since there is only one file.\n\n   Jams\n   ----\n   jam() and its cousins is a set of macros used for tracing what happened\n   at the point of a crash. Each jam call executes a set of instructions\n   that inserts the line number of the jam macro into an array kept per\n   thread. There is some overhead in the jams, but it helps quite a lot in\n   debugging crashes of the NDB data nodes. At crash time we can see a few\n   thousand of the last decisions made just before the crash. This together\n   with the signal logs makes for a powerful tool to root out bugs in NDB\n   data nodes.\n\n   Trace Id\n   --------\n   Each signal also carries a signal id, this id can be used to trace certain\n   activities that go on for a longer time. This tracing can happen even in a\n   live system.\n*/\n</pre>\n</div>\n\n<p>&nbsp;</p>\n","authorId":null,"subject":"hidden","tags":null,"img":null,"summary":"string","lastUpdated":"2020-12-18T06:47:52.891+0000"}