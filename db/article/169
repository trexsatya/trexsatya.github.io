{"name":"Docker - Containers","id":169,"content":"<div class=\"box1\">\n<p>Let&#39;s say you have created an app using Java-8, you have one (or more) JAR files from your app, and now you want to this app available for use. <em>What would you have to do</em>?</p>\n\n<ul>\n\t<li>You need to deploy the JAR file onto the server (maybe a physical machine or something else). (This step could simply mean copying the JAR file to right place on the server)</li>\n\t<li>Then, you need to make sure that Java version 8 is installed on the server, if not you may want to run a script that will install Java version 8.</li>\n\t<li>Then you need to run the command to start the java app.</li>\n\t<li>If your app is some kind of HTTP server based app, then it might also need to expose a port for the external world. You might also need to set up a certificate for HTTPS.</li>\n</ul>\n</div>\n\n<p><br />\nThere could be additional steps for different types of apps and scenarios. <strong>So, how would you do that?</strong></p>\n\n<p>The <em>traditional approach</em> has been to use script (bash or batch) files which perform these tasks or manually doing these (in the worst case). [But you can easily think and see that its a very bad approach.]</p>\n\n<p>Also, instead of directly running the scripts on servers, you can use some sort of <em>Configuration Management platform</em> (e.g. Puppet, Chef, Ansible etc) to deploy dependencies, run scripts. Especially useful when you have multiple servers where you want to deploy your application.</p>\n\n<p>A better idea is to <span style=\"color:#8e44ad;\"><strong><em>bundle your app with everything it requires</em></strong></span>. You could use configuration management tools on top of it then.&nbsp;How can you bundle app and all it requires?</p>\n\n<ul>\n\t<li>&nbsp;Well, one (traditional) way is to create a <strong>Virtual Machine</strong> image which bundles the app with all requirements. And then deploy these VMs on servers.</li>\n\t<li>&nbsp;Another way is to use <strong>Container</strong> technology. Containers also bundle the app and all its dependencies.</li>\n</ul>\n\n<p><span style=\"color:#8e44ad;\"><strong>How is Container different from VM then?</strong></span></p>\n\n<p>If you want to understand on the simplest level from the usage perspective, you can say <em>Containers are cheaper and more efficient than VMs for the task of bundling and shipping an app</em>.</p>\n\n<p>If you want to understand on the technical level, VMs represent an abstract form of physical machine hardware, a VM will have an entire Operating System of its own. Containers represent an abstract form of an Operating System for a single process/application, and shares OS kernel on the machine it is running with other Containers.</p>\n\n<p><em>Docker is a technology that facilitates Containers. This technology is developed by Docker Inc and nowadays is hugely popular. But Docker is not the only technology for Containers, there are others too.</em></p>\n\n<p>Links for more understanding:&nbsp;</p>\n\n<p><a href=\"https://www.docker.com/resources/what-container\">https://www.docker.com/resources/what-container</a></p>\n\n<p><a href=\"https://stackoverflow.com/questions/28089344/docker-what-is-it-and-what-is-the-purpose\">https://stackoverflow.com/questions/28089344/docker-what-is-it-and-what-is-the-purpose</a></p>\n\n<h2>How to Use Docker</h2>\n\n<p>Remember it&#39;s not wise to use Docker directly for deploying your applications in production. Because there is a better technology built on top of Docker which makes life even easier. But to understand docker and for hobby projects, you can try docker directly:-</p>\n\n<p>To use docker you need to install Docker Engine and Docker tools. Docker engine is a daemon process running in the background which will respond to docker commands (from docker tools).</p>\n\n<p>To read detailed steps of creating an app using Docker technology:&nbsp;<a href=\"https://docs.docker.com/get-started/\">https://docs.docker.com/get-started/</a></p>\n\n<h2><br />\nTypical Usage Scenario</h2>\n\n<ol>\n\t<li>Setup all things required for Docker. Installation Step.</li>\n\t<li>Create your app, package your app in a Docker image</li>\n\t<li>Optionally you can share, upload your Docker image</li>\n\t<li>Run Docker image to make your app up and running.</li>\n</ol>\n\n<p>Install docker on the system (e.g. To install on Ubuntu based machine&nbsp;<a href=\"https://docs.docker.com/install/linux/docker-ce/ubuntu/\" target=\"_blank\">Docker Installation For Ubuntu</a>)</p>\n\n<p>Now you have two options to use Docker to package your app in a Docker image:</p>\n\n<ul>\n\t<li>You can use it in a way that you use Git or other VCSs. You pull the image, make some changes, commit and push.</li>\n\t<li>You can define what all things you want to do in a file, build the image, tag, and push.</li>\n</ul>\n\n<h3>(1) Using Docker in a Git-style way</h3>\n\n<p>Find an image on Docker Hub (this is a public repository of docker images if you know Maven it is just like Maven Repo&nbsp; is for maven artifacts)</p>\n\n<p>Pull an image, Run on the host (any kind of host physical, cloud etc)</p>\n\n<p>Build an image by performing tasks manually</p>\n\n<pre>\n<code class=\"language-bash\">#Get base image\ndocker pull ubuntu:14.04\n\n#Run docker, do your work e.g. installation of something etc; this will change the image\ndocker run -it ubuntu:14.04 /bin/bash\n\napt-get install curl\n\n#Commit changes to image\ndocker ps -a # to get the container ID\ndocker commit &lt;container ID&gt; &lt;repositoy&gt;/&lt;image-tag-name&gt;:&lt;version&gt;\n\n#verify that image is created\ndocker images #will show a list</code></pre>\n\n<h3><br />\n(2) Building image by a docker configuration file</h3>\n\n<p>You should put everything that your app requires in a single folder. Create a file named Dockerfile.</p>\n\n<pre>\n<code class=\"language-bash\">#inside file named Dockerfile\n\nFROM ubuntu:14.04 #tell base image to pull\nRUN apt-get install curl #to perform installation, this will make changes and commit\nRUN apt-get update &amp;&amp; apt-get install vim #if you want, two changes in single commit, otherwise each RUN creates a new commit</code></pre>\n\n<pre>\n<code class=\"language-bash\">docker build -t &lt;repositoy&gt;/&lt;image-tag-name&gt;:&lt;version&gt; &lt;path-to-folder-having-everything-required-by-docker-build&gt;</code></pre>\n\n<p>Dockerfile may contain commands which will be run when the image is run (not at docker build time)</p>\n\n<p>CMD command&nbsp;</p>\n\n<p>ENTRYPOINT command</p>\n\n<p>CMD is used to set a default command that will be run when docker container is run. All but last CMD commands will be ignored, so there is no point in writing multiple CMD commands in Dockerfile. CMD command will be overridden if docker container is run with a command e.g.</p>\n\n<pre>\n<code class=\"language-bash\">docker run -it &lt;image&gt; /bin/bash</code></pre>\n\n<p>ENTRYPOINT command is used to make container executable. The command will be run when docker container is run and will not be overridden.</p>\n\n<p>However, if you want to pass parameters to ENTRYPOINT command, then you can use a combination of ENTRYPOINT and CMD e.g.</p>\n\n<pre>\n<code>ENTRYPOINT [\"/bin/echo\", \"Hello\"]\nCMD [\"world\"]</code></pre>\n\n<p>When docker image is run, it will print &quot;Hello world&quot;. But you can pass a parameter to replace &quot;world&quot; when running Docker container, <code>&#39;docker run -it &lt;image&gt; Name&#39;</code></p>\n\n<p>Now if you want to make it available to any system in the world so that you could then run the docker image anywhere, you need to tag your image and push it to a publicly available docker repository.</p>\n\n<h3>Tagging</h3>\n\n<pre>\n<code class=\"language-bash\">docker tag &lt;repo-name-1&gt;/tag-1:&lt;version&gt; &lt;repo-name-2&gt;/tag-2:&lt;version&gt; </code></pre>\n\n<h3><br />\nPushing</h3>\n\n<p>Before pushing you will need to login to Docker repository (For that you need to have an account and access to the repository, Docker Hub is the public repository).</p>\n\n<pre>\n<code class=\"language-bash\">sudo docker login\nsudo docker push &lt;tag-name&gt;:&lt;version&gt;</code></pre>\n\n<p><span style=\"color:#8e44ad;\"><strong>You can see the Dockerfile used to create the image for the frontend and backend parts of this website here.&nbsp;</strong></span></p>\n\n<p><a href=\"https://github.com/trexsatya/cupitor/blob/master/frontend/vue2/Dockerfile\">https://github.com/trexsatya/cupitor/blob/master/frontend/vue2/Dockerfile</a>&nbsp;(This implies a docker image will have an NGINX server configured to serve some static files.)</p>\n\n<p><a href=\"https://github.com/trexsatya/cupitor/blob/master/backend/api/springboot/Dockerfile\">https://github.com/trexsatya/cupitor/blob/master/backend/api/springboot/Dockerfile</a>&nbsp;(For a Java application)</p>\n\n<h3>Running the Docker Image and Your Application</h3>\n\n<p>If your application is complete in one docker image only, then you can use Docker command to run it. If your application has multiple parts, something like micro-services which need to coordinate with each other, you have multiple options. You can use Docker-Compose or Docker-Compose along with Docker Swarm.</p>\n\n<pre>\n<code class=\"language-bash\">#docker run --name=&lt;container-name&gt; --publish=&lt;port-on-machine&gt;:&lt;port-in-container&gt; &lt;image-name&gt;:&lt;TAG&gt;\n\ndocker run --name=cupitor-backend --publish=8080:8080 cupitor-backend-springboot:latest --network app-tier</code></pre>\n\n<p>You can access the shell inside a running container to introspect the system that is running your app.</p>\n\n<pre>\n<code class=\"language-bash\">docker exec -it &lt;container-id&gt; bash #to get access to terminal inside container</code></pre>\n\n<p><strong>To use Docker Compose:</strong></p>\n\n<p>Basically, you need to create a docker-compose.yml file which tells the name of docker images your application requires, what ports these services expose, do they need to be on the same network, environment variables etc.</p>\n\n<p>You can see the Docker compose file used for this website itself. This website is built with 3 services: frontend, backend, and a MongoDB-server.&nbsp;<a href=\"https://github.com/trexsatya/cupitor\">https://github.com/trexsatya/cupitor</a></p>\n\n<p><strong>Why Docker Swarm?</strong></p>\n\n<p>Docker Swarm is for production-grade deployment of an application. It also uses a docker-compose file. Swarm can manage and deploy containers on multiple machines forming a cluster.</p>\n\n<p><a href=\"https://docs.docker.com/get-started/part4/\">https://docs.docker.com/get-started/part4/</a></p>\n\n<h2>More Detailed View on What Makes Docker Different From VM</h2>\n\n<p><strong>First of all, what are containers?</strong></p>\n\n<p>They are often quoted as cheap/lightweight VMs, but in reality they are just isolated groups of processes running on a single host. Technologies like Docker just makes it easy to use them.&nbsp;</p>\n\n<p>Read this <a href=\"https://ericchiang.github.io/post/containers-from-scratch/\" target=\"_blank\">Link</a> for detail.</p>\n\n<p>Docker images are simply tarballs. The digest is sha256sum of the tarball. This tarball contains directory structure like typical Linux&nbsp; OS.</p>\n\n<p id=\"chroot\"><strong>chroot</strong></p>\n\n<p>This tool allows you to&nbsp;restrict a process&#39;s&nbsp;view of the file system. After running chroot on aa folder e.g. `<code>chroot &lt;folder_name&gt; &lt;shell or command&gt;</code>`, if you execute Python interpreter, we&#39;ll be executing&nbsp; <code>&lt;folder_name&gt;/usr/bin/python</code>, not the host&#39;s python.&nbsp;That interpreter depends on shared libraries and device files that have been intentionally included in the archive/folder. Instead of shell you could run any.</p>\n\n<p>But process under chroot should not be able to kill process on host. So we need restrictions. That&#39;s where namespaces come in.</p>\n\n<p><strong>Namespace</strong></p>\n\n<p>Namespaces allow us to create restricted views of systems like the process tree, network interfaces, and mounts.</p>\n\n<p>Processes need not be completely isolated from host and each other. They might require to share.&nbsp;For instance it may be useful for two programs to have isolated PID namespaces, but share a network namespace (e.g.&nbsp;<a href=\"http://kubernetes.io/docs/user-guide/pods/\">Kubernetes pods</a>).</p>\n\n<p><code>nsenter</code> command can be used to share namespaces.</p>\n\n<p>Using <code>mount </code>command we can expose files on host as&nbsp;read-only to the chrooted shell. This command can be used for NFS, in-memory file systems etc.</p>\n\n<p><strong>Cgroups (Control Groups)</strong></p>\n\n<p>Allow kernel imposed isolation on resources like memory and CPU.</p>\n\n<p>The kernel exposes cgroups through the&nbsp;<code>/sys/fs/cgroup</code>&nbsp;directory. If your machine doesn&rsquo;t have one you may have to&nbsp;<a href=\"https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Resource_Management_Guide/sec-memory.html#memory_example-usage\">mount the memory cgroup</a>&nbsp;to follow along. If you create a directory in this cgroup directory, kernel automatically fills in it the files which can be used to cnfigure resources e.g.&nbsp;<code>/sys/fs/cgroup/memory/demo/memory.limit_in_bytes</code></p>\n\n<p>The&nbsp;<code>tasks</code>&nbsp;file is special, it contains the list of processes which are assigned to the cgroup. To join the cgroup we can write our own PID.</p>\n\n<p>cgroups can&rsquo;t be removed until every processes in the&nbsp;<code>tasks</code>&nbsp;file has exited or been reassigned to another group. Exit the shell and remove the directory with&nbsp;<code>rmdir</code>&nbsp;(don&rsquo;t use&nbsp;<code>rm -r</code>).</p>\n\n<p><strong>Security in containers</strong></p>\n\n<p>Containers are extremely effective ways of running arbitrary code from the internet as root, and this is where the low overhead of containers hurts us. Containers are significantly easier to break out of than a VM. As a result many technologies used to improve the security of containers, such as SELinux, seccomp, and capabilities involve limiting the power of processes already running as root.</p>\n\n<p>Capabilities are a set of discrete powers that together make up everything root can do. This ranges from things like setting the system clock, to kill arbitrary processes. You can use <code>setcap </code>command to give capabilities to scripts.</p>\n\n<h2>Memory Configurations</h2>\n\n<p>By default, a container has no resource constraints and can use as much of a given resource as the host&rsquo;s kernel scheduler will allow. Docker provides ways to control how much memory, CPU, or block IO a container can use, setting runtime configuration flags of the&nbsp;<code>docker run</code>&nbsp;command.</p>\n\n<p>Docker containers can be limited to use a maximum amount of memory and any container using more memory will be killed. But JVM in itself tries to allocate some memory on startup.&nbsp;What if container has less memory than JVM&rsquo;s allocated memory? It will be killed immediately. There is a setting in JVM which can cap automatically to max memory of container rather than host system. In other words, JVM becomes container aware.</p>\n\n<pre>\n<code class=\"language-bash\">docker run -m 300MB java java \\\n-XX:+UnlockExperimentalVMOptions \\\n-XX:+UseCGroupMemoryLimitForHeap \\\n-XX:MaxRAMFraction=1 \n-XshowSettings:vm \\\n-version</code></pre>\n\n<p>&nbsp;</p>\n\n<h2>Docker Troubleshooting</h2>\n\n<p>Running Docker on Windows is cumbersome, especially on older versions of Windows.</p>\n\n<p>If you are using Docker toolbox on Windows and trying to forward port from docker to host, remember, all the ports from docker are mapped onto IP of Toolbox which you can find using this command</p>\n\n<pre>\n<code class=\"language-bash\">$ docker-machine ip #This returns IP of Linux VM which is used by Toolbox or Docker machine to run docker</code></pre>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n","authorId":null,"subject":"devops","tags":null,"img":"/images/docker.jpg","summary":"Docker give you capability to package your whole application and ship it to any machine to run the application without any configuration headache.","lastUpdated":"2018-10-20T05:20:39.012+0000"}