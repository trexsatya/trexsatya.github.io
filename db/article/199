{"name":"Scalability","id":199,"content":"<div class=\"box1\" style=\"\">Suddenly your&nbsp;website becomes popular. You had deployed your website application on one server which was good enough until now. But now your server is giving up due to heavy load. Is your application, deployment architecture prepared to handle this kind of scenario? This is the question of <strong><em>Scaling</em></strong>.<br />\n<span style=\"color:#27ae60;\"><strong><em>How&nbsp;does your system/application respond&nbsp;to increasing load?</em></strong></span></div>\n\n<p><br />\nYou can use more powerful hardware e.g. more powerful CPU, more RAMs, more and better disk spaces. This is called scaling <em>vertically</em>.<br />\nOr, you can deploy multiple instances of your application on multiple servers (maybe on different machines). This is called scaling <em>horizontally</em>.</p>\n\n<p>&nbsp; &nbsp;<img alt=\"\" src=\"/images/scaling.png\" style=\"width: 541px; height: 248px;\" /></p>\n\n<p>&nbsp; &nbsp;<strong>Vertical Scaling</strong> - Increase hardware (RAM, processor, disk space). These are the Disk technologies in increasing order of power: SATA &lt; SAS &nbsp;&lt; SSD (Solid State Drives don&#39;t rotate and are more efficient)</p>\n\n<p><strong>&nbsp; Horizontal Scaling</strong> - You could instead use multiple machines (servers) to serve your application. How would you set this up?&nbsp;<br />\n&nbsp; &nbsp;Suppose you have got 5 machines, they will have different IP addresses. But your website has single URL and that will be mapped to single IP address by any DNS server. How would you use 5 different servers?</p>\n\n<ol>\n\t<li>You can configure the DNS server (generally provided by some web hosting company e.g. GoDaddy) to use multiple addresses (A Records) in the round-robin fashion (by default).</li>\n\t<li>You could use Load Balancer (LB) which selects one of many servers (It can select based on many parameters, it watches the servers to see if they are healthy or dead or overloaded, it redirects the request to the healthy server if someone&#39;s dead).&nbsp;There are two types of Load Balancers</li>\n</ol>\n\n<ul style=\"margin-left: 40px;\">\n\t<li><u><strong>Layer-4 LB</strong></u> works on Layer 4 (Layer 3) of OSI stack (which are 7 in total, by the way, <a href=\"https://en.wikipedia.org/wiki/OSI_model\">See More..</a>). This kind of LBs inspect first few packets coming from the client (web browsers etc)&nbsp;in the TCP stream, they change the <em>destination address</em> to one of the application servers which can respond to client&#39;s request. When the selected application server responds, it changes the <em>source address</em>&nbsp;back to its own, so the clients get an illusion that the request is served by the one single machine (on which&nbsp; LB is running).<br />\n\tThese type of LB use generally hardware (with proprietary embedded software) to inspect packets and perform NAT (Network Address Translation). This is generally not preferred nowadays, we have more configurable alternatives.</li>\n\t<li><u><strong>Layer-7 LB</strong></u>&nbsp;works on Layer 7 (and Layer 5,6) of OSI stack. This LB software inspects the actual content (/headers) of the request and decides based on that. e.g. LB could send video request to one server, image request to another server, HTML requests to another server (So you can have dedicated servers for different types of content).&nbsp;</li>\n</ul>\n\n<p>&nbsp; &nbsp; &nbsp;Load Balancers come in both Software and Hardware form. Examples are:</p>\n\n<p style=\"margin-left: 40px;\">&nbsp; &nbsp; <strong>Software Load Balancers</strong>: ELB (e.g. Amazon EC2), HAProxy, LVS (Linux Virtual Server)<br />\n&nbsp;&nbsp; &nbsp;<strong>Hardware Load Balancers</strong>: provided by Barracuda, Cisco, Citrix, F5</p>\n\n<p style=\"margin-left: 40px;\">These Load Balancers are generally implemented by Reverse Proxy servers. <strong><em>Reverse Proxies</em></strong> are servers which can perform different functions including Load Balancing. The functions include:</p>\n\n<ul>\n\t<li style=\"margin-left: 40px;\">Load Balancing</li>\n\t<li style=\"margin-left: 40px;\">Web Acceleration: Can implement some kind of caching, Can perform SSL encryption/decryption, so that application&nbsp;servers are relieved from this task.</li>\n\t<li style=\"margin-left: 40px;\">Security/Anonymity: It allows you to hide your internal LAN from the outside world.</li>\n</ul>\n\n<p>So now using the Load Balancer(s), you can allow different servers to serve your application. But there are scenarios when it creates problems.</p>\n\n<p><strong>Sticky Sessions</strong>: HTTP Sessions are generally stored on servers (identified by some Session ID random hash numbers). But in the scenario where there are multiple servers, suppose a customer logs into the website, selects some shopping items to the cart and this session information gets&nbsp;stored on server-1, after few minutes he adds another item to cart, but this HTTP request is redirected by LB to server-2, server-2 sees that this user is trying to add a shopping item, but server-2 does not know about this user&#39;s session (which was actually stored on server-1), hence user is forced to log in again.&nbsp;&nbsp;<br />\n<strong>How do we solve this problem?</strong></p>\n\n<ol>\n\t<li>We can use some kind of shared storage:&nbsp;FC (Fiber channels), Database e.g MySQL, NFS etc</li>\n\t<li>We can use cookies to store the identity of the server on which has the session on it.&nbsp;</li>\n</ol>\n\n<p><a href=\"http://www.haproxy.org/download/1.2/doc/architecture.txt\">Read more about configuring HAProxies here.</a></p>\n\n<h2>Distributed Systems:</h2>\n\n<p>This setup we just discussed can be called <em>a distributed system</em>, there are many other things and scenarios in distributed systems.</p>\n\n<p>Problems in a distributed system which need to be solved (by different techniques in different use-cases):<br />\n&nbsp;&nbsp; &nbsp;Load Balancing<br />\n&nbsp;&nbsp; &nbsp;Redundancy<br />\n&nbsp;&nbsp; &nbsp;High Availability</p>\n\n<p>Redundancy: If the content on your web application is stored in only one place, it is not completely safe. Any tragic accident happens and it will be lost for always. You would not want to lose customer data if you are running a successful business. So what do we do? We duplicate the data in multiple places (on many levels).</p>\n\n<p>This duplication can happen at storage Disk level, server level (same data is duplicated on multiple machines), location level (same data is duplicated in multiple geolocations).&nbsp;</p>\n\n<p>Redundancy at Disk Level: (<a href=\"https://en.wikipedia.org/wiki/RAID\">RAID</a> )</p>\n\n<p style=\"margin-left: 40px;\">RAID0 (data chunked and chunks written in parallel to different disks), RAID1 (data duplicated and written in parallel)(costly)<br />\nRAID10 is a combination, RAID5, RAID6 are middle grounds.</p>\n\n<p><strong>CAP Theorem:</strong></p>\n\n<p>CAP theorem tells you about a limitation that is faced by any distributed system. You can have only two of these three:</p>\n\n<ol>\n\t<li><strong>C</strong>onsistency</li>\n\t<li><strong>A</strong>vailability</li>\n\t<li><strong>P</strong>artition Tolerance</li>\n</ol>\n\n<p><u><strong>P</strong><em>artition Tolerance</em></u> means that the system continues to operate despite arbitrary partitioning due to network failures. This is required by any distributed system because we cannot avoid network failures.</p>\n\n<p><u><strong>A</strong><em>vailability</em></u> means that every request receives a response, without a guarantee that it contains the most recent version of the information</p>\n\n<p><u><strong>C</strong><em>onsistency</em></u> means that every read receives the most recent write or an error.</p>\n\n<p>So we have a compromise between Availability and Consistency, and for different business purposes, we can choose different options out of these two. e.g. In case of a banking software, we cannot tolerate leave out consistency, if someone you have transferred some money it must show to the transferee. On the other hand, it might be OK if the post that you shared on social media is not seen by everyone for a fraction of time.</p>\n\n<p>&nbsp;&nbsp; &nbsp;Consistency can be achieved in different flavors:<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;<strong>Weak consistency</strong><br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;After a write, reads may or may not see it. The best effort approach is taken.</p>\n\n<p>&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;This approach is seen in systems such as Memcached.&nbsp;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;Weak consistency works well in real time use cases such as VoIP, video chat, and real-time multiplayer games.&nbsp;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;For example, if you are on a phone call and lose reception for a few seconds,&nbsp;<br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;when you regain connection you do not hear what was spoken during connection loss.<br />\n&nbsp;&nbsp; &nbsp;<br />\n&nbsp; &nbsp; &nbsp; &nbsp;<strong>Eventual consistency</strong><br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;After a write, reads will eventually see it (typically within milliseconds). Data is replicated asynchronously.</p>\n\n<p>&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;This approach is seen in systems such as DNS and email. Eventual consistency works well in highly available systems.<br />\n&nbsp;&nbsp; &nbsp;<br />\n&nbsp; &nbsp; &nbsp; &nbsp;<strong>Strong consistency</strong><br />\n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;After a write, reads will see it. Data is replicated synchronously.</p>\n\n<p>&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;This approach is seen in file systems and RDBMSes. Strong consistency works well in systems that need transactions.<br />\n&nbsp;</p>\n\n<p>To achieve Availability, we have two options:</p>\n\n<ol>\n\t<li>Master - Master configuration (aka Active-Active )</li>\n\t<li>Master-Slave configuration (aka Active-Passive)</li>\n</ol>\n\n<p>In the first one, we use two (or more) &nbsp;servers. If one server goes down, all the requests coming to it are redirected to another server by Load Balancer.&nbsp;</p>\n\n<p>In the second option also we use two (or more) servers, but the second one stays in passive mode until the first one fails.</p>\n\n<p><strong>Scaling the Database Layer:</strong></p>\n\n<p>There are many techniques to scale a relational database:&nbsp;<strong>master-slave replication</strong>,&nbsp;<strong>master-master replication</strong>,&nbsp;<strong>federation</strong>,&nbsp;<strong>sharding</strong>,&nbsp;<strong>denormalization</strong>, and&nbsp;<strong>SQL tuning</strong>.</p>\n\n<p>Master serves to read and writes both, Slaves serve read-only. Master propagates/replicates writes to other masters or Slaves (Slaves can also propagate to other slaves). When the master fails, some master or slave takes over (implementing this logic is complex).</p>\n\n<p>Remember replication and synchronization among nodes increase complexity (and latency).</p>\n\n<p><u>Federation or Functional Partitioning</u>: DB per functional unit. e.g. one for Users, one for Products etc.</p>\n\n<p><u>Sharding or Data Partitioning</u>: More than one DB for the same type of data e.g. one DB for users with names starting with A-M, one DB for users with names starting with N-Z etc.</p>\n\n<p>&nbsp; &nbsp; &nbsp; This is complex to implement, <em>re-balancing</em>&nbsp;(like re-hashing in HashMaps) is needed.</p>\n\n<p>Partitioning&nbsp;results in less read and write traffic, less replication, and more cache hits. Joining data across partitions is difficult and costly.</p>\n\n<p><strong>Scaling at Computation Layer:</strong></p>\n\n<p>Map-Reduce (Distributed Computing: in simple words, it&#39;s just Divide &amp; Conquer performed on multiple servers)</p>\n\n<p>Caching (Don&#39;t do same computation again)</p>\n\n<h2>Noisy Neighbour Problem (DDoS Attack)</h2>\n\n<p>You need to handle sudden burst (suddenly millions of requests coming in at the same time unexpectedly, this could have resulted from a Distributed Denial of Service Attack or simply because your API became popular suddenly or your content went viral).</p>\n\n<p>In any case, if you do not handle such situations, your availability will be affected. So to maintain High Availability you also need to handle use some sort of <strong>Rate Limiting or Throttling</strong> to ensure that a client is not able to bombard your service with requests. Even with auto-scaling rate limiter is required because scaling takes some time, and by the time your service scales, servers which have been bombarded may crash.</p>\n\n<p>First line of defence here can be Load Balancer, you can configure max connections or max number of threads. But in your service there will be some slow operations and some fast opertions, load balancer does not know that. You might want to have different configuration for different endpoints.</p>\n\n<p>Remember that the Load Balancer does not distribute requests uniformly across servers. Also some server may process requests slowly for various reasons (example slow disk, overheating, etc...), so the servers need to co-ordinate with each other while Rate Limiting.</p>\n\n<p><a href=\"https://www.youtube.com/watch?v=FU4WlwfS3G0\" target=\"_blank\">Watch this video.</a></p>\n\n<p>There are different algorithms for Rate Limiting: Token Bucket, Sliding Window.</p>\n\n<p>You can easily design Rate Limiting solution for single server. But to make it distributed, servers must know how many requests for given second has been served by other servers. So how do servers know this information. There are many approaches:</p>\n\n<ul>\n\t<li>Mesh network (every server tells info about itself to every other server), good and simple for small clusters with a few servers.</li>\n\t<li>Gossip communication. Servers selects a peer randomly and spreads information.</li>\n\t<li>Distributed cache.</li>\n\t<li>Co-ordination service: with Consensus algorithms. All servers need to tell their stats to this service. This service selects a leader which calculates stats and tells every server.</li>\n\t<li>Random Leader selection</li>\n</ul>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n","authorId":null,"subject":"architecture","tags":null,"img":null,"summary":"When the application gets bigger, number of clients/users increase, we have to either unleash the power of Hardware or come up with a clever solution","lastUpdated":"2018-07-14T17:54:08.047+0000"}