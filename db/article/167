{"name":"Artificial Neural Networks","id":167,"content":"<p>Given enough features, any output, any decision, any estimation / prediciton can be expressed as&nbsp;mathematical functions.</p>\n\n<p>We set this mathematical expression randomly, and try to give it the correct shape, such that the output of this mathematical function is closest possible to the input.&nbsp;</p>\n\n<p>delta = expected-output-for-input-x minus function-output-for-input-x</p>\n\n<p>We want this delta&nbsp;to be as small as possible, maybe zero. To build our function, we can convert this delta into a function which we can minimize mathematically easily.</p>\n\n<p>&nbsp; &nbsp; We define a cost function as <strong>&#39;average of delta over all inputs&#39;&nbsp; </strong><span class=\"math\">(1/2n)\\sum_x\\|y(x) - f(x)\\|^2</span>&nbsp; This is simply average of dot product of the two vectors.</p>\n\n<p>Now we want to minize this cost function, but we can&#39;t do it simply by calculas, as this is not about one or two variables. There could be thousand of variables for the function.&nbsp;</p>\n\n<p>So one way is to use Gradient Descent algorithm to minimize this cost function. (The fundamental idea is that suppose the cost is much higher initially, we find the derivative of Cost w.r.t. all the variables and subtract it from the cost, hopefully it will lead us to global minima of the function).</p>\n\n<p>To understand how Gradient Descent works, there are many intutions that we can develop. Here are some mathematical&nbsp;explainations<iframe allow=\"autoplay; encrypted-media\" allowfullscreen=\"\" frameborder=\"0\" height=\"315\" src=\"https://www.youtube.com/embed/TEB2z7ZlRAw\" width=\"560\"></iframe></p>\n\n<p>&nbsp;</p>\n\n<p>Convolutional Neural Nets:</p>\n\n<p>Visit Links:&nbsp;<a href=\"https://cs231n.github.io/\" target=\"_blank\">https://cs231n.github.io/</a>&nbsp; and&nbsp;<a href=\"http://www.deeplearningbook.org/\" target=\"_blank\">http://www.deeplearningbook.org/</a>&nbsp;and&nbsp;<a href=\"http://neuralnetworksanddeeplearning.com/\" target=\"_blank\">http://neuralnetworksanddeeplearning.com/</a></p>\n\n<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Convolve word evolves from&nbsp;<i>convolvere</i>&nbsp;(meaning &#39;roll together&#39;). In mathematics, it convolution means integrating two functions into one, the output expresses how the shape of one function is modified by the other.</p>\n\n<p>The ideas is to get an abstract view of a large image, this is done by applying a filter to each subregion of image. Take a matrix (called <strong>filter</strong>) and superimpose on the top-left corner of the image. Apply some mathematical operations to reduce all the superimposed pixels into single number. Ouput that. Move the sliding&nbsp;&nbsp;filter to next pixel. Do the same. And so on.</p>\n\n<p>In this process the image shrinks. Because if we apply f x f sliding matrix to image of size n x n, then the size of output would be (n-f+1 x n-f+1).</p>\n\n<p>To prevent this we can pad the original image by zeroes. To make output size equal to original image size: pad by (f-1)/2 pixels.&nbsp;</p>\n\n<p>Instead of taking one step we can take multiple steps while sliding the filter.&nbsp;Ouput size: floor((n+2p-f)/s+1)</p>\n\n<p>An example convolution function would be&nbsp; <strong>slice -&gt;&nbsp; sum(slice*Filter) + bias</strong></p>\n\n<p>To reduce the amount of computation we can downsample, (not the image but the abstract view of it, i.e. output of convolutional layers). To downsample it we use Pooling layers.&nbsp; Pooling also provides translation-invariance, i.e. if the image were&nbsp;translated (along origin of axis), output won&#39;t vary much.</p>\n\n<p>Max Pooling:&nbsp;&nbsp;It partitions the input image into a set of non-overlapping rectangles and, for each such sub-region, outputs the maximum.</p>\n\n<p>A sequence of ConvLayers and PoolingLayers gives us a feature vector which we can feed into a Fully-Connected Deep Neural Network.</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n","authorId":null,"subject":"ai","tags":null,"img":null,"summary":null,"lastUpdated":"2020-04-02T05:49:55.845+0000"}