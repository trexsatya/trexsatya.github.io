{"name":"Python for AI, ML","id":160,"content":"<p>For Basics Of Python Visit&nbsp;<a reactlink=\"/article/116\">Python For Algorithms</a></p>\n\n<p>Install Python Module called &quot;Numpy&quot;: &gt; pip install numpy</p>\n\n<pre>\n<code class=\"language-python\">import numpy as np\n\na = np.array([1, 2, 3])   # Create a rank 1 array\nprint(type(a))            # Prints \"&lt;class 'numpy.ndarray'&gt;\"\nprint(a.shape)            # Prints \"(3,)\"\nprint(a[0], a[1], a[2])   # Prints \"1 2 3\"\na[0] = 5                  # Change an element of the array\nprint(a)                  # Prints \"[5, 2, 3]\"\n\nb = np.array([[1,2,3],[4,5,6]])    # Create a rank 2 array\nprint(b.shape)                     # Prints \"(2, 3)\"\nprint(b[0, 0], b[0, 1], b[1, 0])   # Prints \"1 2 4\"</code></pre>\n\n<pre>\n<code class=\"language-python\">a = np.zeros((2,2))   # Create an array of all zeros\nprint(a)              # Prints \"[[ 0.  0.]\n                      #          [ 0.  0.]]\"\n\nb = np.ones((1,2))    # Create an array of all ones\nprint(b)              # Prints \"[[ 1.  1.]]\"\n\nc = np.full((2,2), 7)  # Create a constant array\nprint(c)               # Prints \"[[ 7.  7.]\n                       #          [ 7.  7.]]\"\n\nd = np.eye(2)         # Create a 2x2 identity matrix\nprint(d)              # Prints \"[[ 1.  0.]\n                      #          [ 0.  1.]]\"\n\ne = np.random.random((2,2))  # Create an array filled with random values\nprint(e)                     # Might print \"[[ 0.91940167  0.08143941]\n                             #               [ 0.68744134  0.87236687]]\"</code></pre>\n\n<pre>\n<code class=\"language-python\">#Slicing for numpy arrays is same as normal arrays/lists\n#arr[i:j:k], i: start, j:stop, k:steps\n\na = np.array([[1, 2, 3, 4], \n              [5, 6, 7, 8], \n              [9,10,11,12]])\n\n# Use slicing to pull out the subarray consisting of the first 2 rows\n# and columns 1 and 2; b is the following array of shape (2, 2):\n# [[2 3]\n#  [6 7]]\nb = a[:2, 1:3]\n\n# A slice of an array is a view into the same data, so modifying it\n# will modify the original array.\n\n# Two ways of accessing the data in the middle row of the array.\n# Mixing integer indexing with slices yields an array of lower rank,\n# while using only slices yields an array of the same rank as the\n# original array:\nrow_r1 = a[1, :]    # Rank 1 view of the second row of a\nrow_r2 = a[1:2, :]  # Rank 2 view of the second row of a\nprint(row_r1, row_r1.shape)  # Prints \"[5 6 7 8] (4,)\"\nprint(row_r2, row_r2.shape)  # Prints \"[[5 6 7 8]] (1, 4)\"</code></pre>\n\n<pre>\n<code class=\"language-python\">#Two types of indexing\n#Basic\nndarray[x,y,z,..]\n\n#Advanced\nndarray[ rowsToSelect, columnsToSelect]\n\nndarray[ rowsToSelect] #e.g a[[1,2]] \n#multiple copies created if index is reffered multiple times e.g. a[[1,1]]\n\nndarray[np.arange(n), columnsToSelect]\n</code></pre>\n\n<pre>\n<code class=\"language-python\">#For two vectors * is element-wise multiplication not matrix-multiplication\na.dot(b), or numpy.dot(a,b) #is matrix multiplication\n\nnp.sum(a) #sum of all elements\nnp.sum(axis=0) #sum of each column\nnp.sum(axis=1) #sum of each row\n\na.T #transpose of a\n</code></pre>\n\n<h2>Pandas</h2>\n\n<p><strong>Input</strong></p>\n\n<p>From read_* where * = csv / excel / sql / clipboard / json / html</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<pre>\n<code class=\"language-python\">#DataFrame Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n#Consider it as in memory DB table\nDataFrame(data=dict_with_columns_and_column_values)\nDataFrame(np.array(list of rows), \n          columns = list_of_column_names)\n\nDataFrame.from_dict(dic_with_row_names_and_row_values, \n                    orient='index',\n                    columns= list_of_column_names)\n</code></pre>\n\n<p>Common Operations</p>\n\n<pre>\n<code class=\"language-python\">#Transpose\ndata_frame.T()\n\n#Indexing\ndata_frame.iat(row, column)\n\n#Indexing just the rows\ndata_frame.iat(index_number_or_list_of_numbers_slice)\n\n#It can also accept function which returns any of the number, list_of_numbers, slice\n\n#Indexing both rows and columns\n#Same way but two arguments\n\n#Remember python slicing supports a third argument step/stride e.g. 1:2:2</code></pre>\n\n<p>&nbsp;</p>\n\n<p>Interaction of pandas with numpy</p>\n\n<pre>\n<code class=\"language-python\">\n#Converting DataFrame to numpy array\ndata_frame.to_numpy()\n\n</code></pre>\n\n<p>&nbsp;</p>\n\n<p>Size &rArr;&nbsp;total number of elements</p>\n\n<p>Shape &rArr;&nbsp;Dimensions of table</p>\n\n<p>Row labels are called index</p>\n\n<p>&nbsp;</p>\n\n<p>Working with Missing Data (nulls)</p>\n\n<pre>\n<code class=\"language-python\">#to check there are .isna() .notna() methods\n\n</code></pre>\n\n<h2>SciKit</h2>\n\n<p><strong>Cross Validation</strong></p>\n\n<p>If you split your dataset into train and test, and after training your model use test to validate and then again change the parameters and then try again, you are prone to overfitting. You can divide into three: test, train, and validate. But then you lose the precious data that you could have used for training.</p>\n\n<p>Solution is Cross-validation: use one split to validate, and others to train. Adjust the parameters, use another split to validate and others to train.. and so on.</p>\n\n<pre>\n<code class=\"language-python\">#Cross validate, specify the scoring function(s), get metrics\nfrom sklearn.metrics import make_scorer\n\nclf = svm.SVC(kernel='linear', C=1, random_state=0)\nscoring = {'prec_macro': 'precision_macro',\n           'rec_macro': make_scorer(recall_score, average='macro')}\nscores = cross_validate(clf, X, y, scoring=scoring,\n                        cv=5, return_train_score=True)\nsorted(scores.keys())\n\n\nscores['train_rec_macro']\n\n#cross_validate() can also take a pipeline as argument instead of a model\n\n\n#There are many ways to split the dataset\n#Try these out\nX = [\"a\", \"b\", \"c\", \"d\"]\nkf = KFold(n_splits=2) \n#Or, use RepeatedKFold(n_splits=2, n_repeats=2, random_state=random_state); \n#Or, LeavePOut(p=2), ShuffleSplit(n_splits=5, test_size=0.25, random_state=0)\n\nfor train, test in kf.split(X):\n    print(\"%s %s\" % (train, test))\n\n#There could be situations where data is not balanced per outcome class; \n#e.g. lot more samples for positive case than for negative. In that case use \n# StratifiedKFold(n_splits=3); \n# Or, StratifiedShuffleSplit\n</code></pre>\n\n<p>Group-wise cross validation: in cases where&nbsp;the generative process has a group structure (samples collected from different subjects, experiments, measurement devices).</p>\n\n<p>If there are many samples from a particular group, and the outcome depends on the group factor; if you use the data from same group to validate, it would not be a good validation, it&#39;s just like using training data to validate. So, instead use this:</p>\n\n<pre>\n<code class=\"language-python\">from sklearn.model_selection import GroupKFold\n\nX = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 8.8, 9, 10]\ny = [\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\", \"d\", \"d\", \"d\"]\ngroups = [1, 1, 1, 2, 2, 2, 3, 3, 3, 3]\n\ngkf = GroupKFold(n_splits=3)\nfor train, test in gkf.split(X, y, groups=groups):\n    print(\"%s %s\" % (train, test))\n\n#Other options are LeavePGroupsOut(n_groups=2), \n# GroupShuffleSplit(n_splits=4, test_size=0.5, random_state=0)</code></pre>\n\n<p>If&nbsp;the samples have been generated using a time-dependent process.</p>\n\n<p>Classical splitters assume samples are IID. If they are instead autocorrelated, use this:</p>\n\n<p><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html#sklearn.model_selection.TimeSeriesSplit\" title=\"sklearn.model_selection.TimeSeriesSplit\"><code>TimeSeriesSplit</code></a>&nbsp;is a variation of&nbsp;<em>k-fold</em>&nbsp;which returns first&nbsp;k&nbsp;folds as train set and the&nbsp;(k+1)&nbsp;th fold as test set. Note that unlike standard cross-validation methods, successive training sets are supersets of those that come before them. Also, it adds all surplus data to the first training partition, which is always used to train the model.</p>\n\n<p>This class can be used to cross-validate time series data samples that are observed at fixed time intervals.</p>\n\n<p>Example of 3-split time series cross-validation on a dataset with 6 samples:</p>\n\n<pre>\n<code class=\"language-python\">from sklearn.model_selection import TimeSeriesSplit\n\nX = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\ny = np.array([1, 2, 3, 4, 5, 6])\ntscv = TimeSeriesSplit(n_splits=3)\nprint(tscv)\n\nfor train, test in tscv.split(X):\n    print(\"%s %s\" % (train, test))</code></pre>\n\n<p>&nbsp;</p>\n\n<p><strong>Hyperparameter Optimization</strong></p>\n\n<p>Instead of trying different combinations of parameters one by one manually, you can specify them all at once, and this process may be automated.</p>\n\n<pre>\n<code class=\"language-python\">param_grid = [\n  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n ]\n\nclf = GridSearchCV(\n        SVC(), tuned_parameters, scoring='precision_macro'\n    )\nclf.fit(X_train, y_train)\n\nprint(clf.best_params_)</code></pre>\n\n<p>For complete example visit this <a href=\"https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html#sphx-glr-auto-examples-model-selection-plot-grid-search-digits-py\">documentation</a></p>\n\n<p>&nbsp;</p>\n\n<p><strong>Scoring,&nbsp;Quantifying the Quality of Prediction:</strong></p>\n\n<p>See the documentation <a href=\"https://scikit-learn.org/stable/modules/model_evaluation.html\">here</a>.</p>\n\n<p>For comprehensive guid for evaluation metrics, visit <a href=\"https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc\">this blog</a>.</p>\n\n<p>Measuring the performance of classification:</p>\n\n<p><strong>Accuracy:</strong>&nbsp;Ratio of number of correct classifications per total number of tests.</p>\n\n<p><u>Only for Binary Classification</u></p>\n\n<p><strong>Precision</strong>: can be interpreted as how confident you are in saying that you would not mistake of taking a stone as a diamond.</p>\n\n<p><strong>Recall (a.k.a. Sensitivity)</strong>: can be interpreted as how confident you are in saying that you would not mistake of throwing away a diamond thinking its a stone.</p>\n\n<p>But which one to choose: precision, or recall? To solve that you can take average of those by assigning some weights/importance to precision and recall. E.g. if you give equal weights to both, then the average is called <strong>F1-Score</strong>, and if you give recall more importance (b times as precision), then the average is called&nbsp;<strong>Fb-Score.</strong></p>\n\n<pre>\n<code class=\"language-python\">f1_score(y_true, y_pred, \\*[, labels, …])\n\nfbeta_score(y_true, y_pred, \\*, beta[, …])\n\n#If you want to plot precision recall for different thresholds for classification; e.g. if probability output of model is &gt; 0.5 then it belongs to class-1 otherwise class-2\nprecision_recall_curve(y_true, probas_pred, \\*)\n\nprecision_recall_fscore_support(y_true, …)</code></pre>\n\n<p>Macro average of precision would be sum of precisions for all classes divided by number of classes. Similarly for recall.</p>\n\n<p><strong>Specificity:</strong>&nbsp;TN/(TN+FP). how confident you are that you correctly identified negative cases.</p>\n\n<p><strong>RoC</strong> (Receiver Operator Characteristic) graphs provide simple way to summarize the effects of choosing different thresholds for classification. It plots Sensitivity (on y-axis)&nbsp;i.e True Positive rate against (1-Specificity) (on x-axis) i.e. True Negative rate. Precision-Recall curves is also a RoC curve.</p>\n\n<p><strong>AUC</strong> (Area Under Curve) of RoC allows us to compare two RoC curves (e.g. two RoC&#39;s for 2 different models). More area implies better.</p>\n\n<pre>\n<code class=\"language-python\">#RoC\nsklearn.metrics.roc_curve(y_true, y_score, *, pos_label=None, sample_weight=None, drop_intermediate=True)</code></pre>\n\n<p>&nbsp;</p>\n\n<p>In case of Binary classification, Matthews Correlation Coefficient (MCC) can be&nbsp;informative than Fb-Score.</p>\n\n<p><u>For Multilabel&nbsp;classification</u></p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>Accuracy Score</p>\n\n<p>F1 Score</p>\n\n<p>Log Loss</p>\n\n<p>Precision Score</p>\n\n<p>Recall Score</p>\n\n<p>RoC AUC Score</p>\n\n<p>Confusion Matrix</p>\n\n<p>&nbsp;</p>\n\n<p>In cases where ranking of documents is important e.g. in web-search results you shouldn&#39;t calculate precision, recall on whole list of documents retrurned. Instead you can plot Precision p(r) as function of Recall r.</p>\n\n<pre>\n<code class=\"language-python\">average_precision_score(y_true, y_score, \\*)</code></pre>\n\n<p>&nbsp;</p>\n","authorId":null,"subject":"ai","tags":null,"img":null,"summary":null,"lastUpdated":"2020-06-17T04:54:04.054+0000"}