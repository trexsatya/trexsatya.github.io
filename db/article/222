{"name": "Databases", "id": 222, "content": "<div style=\"background:#eeeeee;border:1px solid #cccccc;padding:5px 10px;\">\n<ol>\n\t<li>Why databases are important? Why do we even need databases, why can&#39;t we just use huge files?</li>\n\t<li>If you were to create a DBMS, what are the core components that need to be developed?</li>\n\t<li>How would you optimize performance in DBMS operations?</li>\n\t<li>How would you handle concurrency (multiple users operating on same data set)? What models could you implement?</li>\n</ol>\n</div>\n\n<p><br />\nDatabases are probably the most important component of any system. And choosing correct DBMS, handling data storage and querying efficiently is of utmost importance to make your system fast.</p>\n\n<blockquote>\n<p>Note: This article has more links than explanations. Visit the links to get in depth knowledge.</p>\n</blockquote>\n\n<p>Read this article: <a href=\"http://coding-geek.com/how-databases-work/\" target=\"_blank\">how databases work</a> and <a href=\"https://stackoverflow.com/questions/172925/how-do-databases-work-internally\" target=\"_blank\">this</a>. Watch <a href=\"https://www.youtube.com/watch?v=aZjYr87r1b8\" target=\"_blank\">this video</a>.</p>\n\n<p>This guy implemented a clone of sqlite: <a href=\"https://cstack.github.io/db_tutorial/parts/part1.html\" target=\"_blank\">Read here</a>.</p>\n\n<p><strong>Database Examples:</strong>&nbsp;</p>\n\n<ul>\n\t<li>Relational: MySQL, MariaDB, PostgreSQL, Oracle, SQL-Server, MongoDB</li>\n\t<li>Document Stores: MongoDB, DocumentDB (Amazon),&nbsp;</li>\n\t<li>Column Stores: <a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_columnar_storage_disk_mem_mgmnt.html\">Redshift</a>,&nbsp;<a href=\"https://mariadb.com/products/enterprise/components/#columnstore\">MariaDB ColumnStore</a>, <a href=\"https://kudu.apache.org/docs/index.html\">Apache Kudu</a>,&nbsp;Google&nbsp;<a href=\"https://en.wikipedia.org/wiki/BigQuery\" title=\"BigQuery\">BigQuery</a></li>\n\t<li>Wide-column Stores: Cassandra DB, BigTable,&nbsp;</li>\n</ul>\n\n<h2>Why Database?</h2>\n\n<p>You should definitely use database for <strong><em>persistent storage</em></strong>. But you should be able to answer why database instead of just a file or filesystem storage.</p>\n\n<p>See this <a href=\"https://stackoverflow.com/questions/2356851/database-vs-flat-files\" target=\"_blank\">Stackoverflow thread</a>.</p>\n\n<p>Ask yourself these questions?</p>\n\n<ul>\n\t<li>Do you need to make queries and CRUD operations to file data? If yes, what sort of queries?</li>\n\t<li>Do you have relational data, relationship between records? Do you have contraints on those relationships?</li>\n\t<li>Is the file going to be accessed by only one client at a time? How do you make sure of that?</li>\n\t<li>Can you store your whole data on the same server as application server, or might you need to move it to another server because of the size?</li>\n\t<li>Do you need to manage access control to the data?</li>\n</ul>\n\n<p>Databases is specifically designed for all these scenarios. If the answer is yes in most/all cases, so the question becomes&nbsp;<strong>why not database?</strong></p>\n\n<h2>Database components</h2>\n\n<p><u>The core components:</u></p>\n\n<ul>\n\t<li><strong>The process manager</strong>: Many databases have a <strong>pool of processes/threads</strong> that needs to be managed. Moreover, in order to gain nanoseconds, some modern databases use their own threads instead of the Operating System threads.</li>\n\t<li><strong>The network manager</strong>: Network I/O is a big issue, especially for distributed databases. That&rsquo;s why some databases have their own manager.</li>\n\t<li><strong>File system manager</strong>: <strong>Disk I/O is the first bottleneck of a database</strong>. Having a manager that will efficiently handle the Operating System file system or even replace it is important.</li>\n\t<li><strong>The memory manager</strong>: To avoid the disk I/O penalty a large quantity of RAM is required. But if you handle a large amount of memory, you need an efficient memory manager. Especially when you have many queries using memory at the same time.</li>\n\t<li><strong>Security Manager</strong>: for managing the authentication and the authorizations&nbsp;of the users</li>\n\t<li><strong>Client manager</strong>: for managing the client connections</li>\n\t<li>&hellip;</li>\n</ul>\n\n<p><u>The tools:</u></p>\n\n<ul>\n\t<li><strong>Backup manager</strong>: for saving and restoring a database.</li>\n\t<li><strong>Recovery manager</strong>: for restarting the database in a <strong>coherent state</strong> after a crash</li>\n\t<li><strong>Monitor manager</strong>: for logging the activity of the database and providing tools to monitor a database</li>\n\t<li><strong>Administration manager</strong>: for storing metadata (like the names and the structures of the tables) and providing tools to manage databases, schemas, tablespaces, &hellip;</li>\n\t<li>&hellip;</li>\n</ul>\n\n<p><u>The query Manager:</u></p>\n\n<ul>\n\t<li><strong>Query parser</strong>: to check if a query is valid</li>\n\t<li><strong>Query rewriter</strong>: to pre-optimize a query</li>\n\t<li><strong>Query optimizer</strong>: to optimize a query</li>\n\t<li><strong>Query executor</strong>: to compile and execute a query</li>\n</ul>\n\n<p><u>The data manager:</u></p>\n\n<ul>\n\t<li><strong>Transaction manager</strong>: to handle transactions</li>\n\t<li><strong>Cache manager</strong>: to put data in memory before using them and put data in memory before writing them on disk</li>\n\t<li><strong>Data access manager</strong>: to access data on disk</li>\n</ul>\n\n<h2>Data-Storage Ideas For Databases</h2>\n\n<ol>\n\t<li>Simplest model: Add record&nbsp;=&gt; Append the record into a file. Update the index in memory. Index could be written to disk in background. For large index which cannot be hold into memory, use data-structure which supports offline storage e.g. B-Tree. When file grows big enough, use a new file; Keep compacting the files in background by merging them into a new file so that only the latest record for an ID is kept, old files can be deleted later. Use checksum to ensure that corrupt data is discarded.</li>\n\t<li>Same idea, make sure that the records are always sorted by key in the files (even compacted/merged files - simple merge sort can do that). Now instead of storing the whole index in memory you can keep sparse index (only few entries) and still be able to effectively find the records. So basically one index record will give us a range of keys/records. Since they need to be scanned anyway/always whenever we query and get to that index, we can compress that range of records into a block, which saves I/O bandwidth and disk-space.<br />\n\tAdd record =&gt; Add it to an in-memory data-structure which is sorted e.g. RB-Trees, B-Trees, AVL-Trees. Also append it to a log file like we did previously. When the in-memory DS gets big enough, write it to disk. Discard the corresponding log-file once the DS has been written to disk. In case of crash, use the log file to build the in-memory DS.&nbsp;<br />\n\tWhat if we query for a key that does not exist in DB? - It can be very slow. To make it fast, use Bloom Filters in-memory DS.&nbsp;</li>\n\t<li>Same as above but use B-Tree. Simply use B-Tree structure to store data on disk so we can write to disk directly. Records are mapped to fixed-size disk blocks/pages. A log file is still used but just for recovering from a cache.<br />\n\tKeep the IDs (with reference to disk) sorted. Leaf nodes store the data. If a newly-inserted record cannot be accomodated, split the page into two pages and adjust the tree pointers.<br />\n\tRemember, B-Trees require random writes compared to above strategies of sequential writes. Random writes can be slow on magenetic disks. B-Trees also leave fragmented disk blocks (when a page is split).</li>\n\t<li>Keep everything in-memory, write snapshots to disk. This allows us to avoid the overheads of encoding in-memory data structures in a form that can be written to disk, which significantly increases performance.&nbsp;This also allows possibility of data loss.</li>\n</ol>\n\n<p>Implemented ACID in approach-2 is more challenging than in Approach-3.</p>\n\n<h2>Database with Different Purposes &amp; Design</h2>\n\n<p>OLTP: Online Transaction Processing (<em>typical read-update scenarios</em>). Row-storage&nbsp;DBs perform better since whole record/row sits on disk together =&gt; easy to read and write.&nbsp;</p>\n\n<p>OLAP:&nbsp;Online Analytical Processing. Column-storage DBs perform better since a disk block stores column values for many records e.g. names for x number of persons. This allows using some compression algo to compress data for specific types of columns.&nbsp;Column compression allows more rows from a column to fit in the same amount of L1 cache which gives more performancce combined with&nbsp;<a href=\"https://www.infoq.com/articles/columnar-databases-and-vectorization/\">vectorized processing</a>. Imposing sorting on column store further increases the possibility of compression since many duplicate values will be together.</p>\n\n<p>Key-Value Storage: Stores Key-Value pair where Key = rowID, value = data.</p>\n\n<p>Wide-Column Storage: Stores Key-Value pair where Key =&nbsp;&nbsp;(rowID, columnID, timestamp), value = data. Many columns are combined into a column-family which are then mapped to disk blocks. First part of the Key is used to partition the data across different servers. These are actually more oriented than column oriented since they store all columns for a particular set of columns (column family) together with rowID.</p>\n\n<p>Document Stores: are very efficient for 1-m relationships. But m-1, m-m relationships become challenging. They have limited/less-efficient&nbsp;support for joins. Document Stores are not schema-less, rather they are schema-on-read instead of schema-on-write which is analogous to dynamic typed programming as opposed to static-typed programming languages.</p>\n\n<p>If you are using RDBMS and need analytics, materialized views will help. These are table like objects which contain the result of a query, they need to be configured to be refreshed.</p>\n\n<h2>How Does DBMS Execute Queries?</h2>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<h2>Performance Problems of DB</h2>\n\n<p>Analyse queries:</p>\n\n<pre>\n<code class=\"language-sql\">EXPLAIN ANALYZE -- MySQL Example\nSELECT first_name, last_name, SUM(amount) AS total\nFROM staff INNER JOIN payment\n  ON staff.staff_id = payment.staff_id\n     AND\n     payment_date LIKE '2005-08%'\nGROUP BY first_name, last_name;\n\n-&gt; Table scan on &lt;temporary&gt;  (actual time=0.001..0.001 rows=2 loops=1)\n    -&gt; Aggregate using temporary table  (actual time=58.104..58.104 rows=2 loops=1)\n        -&gt; Nested loop inner join  (cost=1757.30 rows=1787) (actual time=0.816..46.135 rows=5687 loops=1)\n            -&gt; Table scan on staff  (cost=3.20 rows=2) (actual time=0.047..0.051 rows=2 loops=1)\n            -&gt; Filter: (payment.payment_date like '2005-08%')  (cost=117.43 rows=894) (actual time=0.464..22.767 rows=2844 loops=2)\n                -&gt; Index lookup on payment using idx_fk_staff_id (staff_id=staff.staff_id)  (cost=117.43 rows=8043) (actual time=0.450..19.988 rows=8024 loops=2)</code></pre>\n\n<p>&nbsp;</p>\n\n<p><strong><em>Slowness despite having indexes?</em></strong></p>\n\n<p>An index lookup requires three steps:</p>\n\n<ol>\n\t<li>the tree traversal;</li>\n\t<li>following the leaf node chain;</li>\n\t<li>fetching the table data.</li>\n</ol>\n\n<p>The tree traversal is the only step that has an upper bound for the number of accessed blocks&mdash;the index depth. The other two steps might need to access many blocks&mdash;they cause a slow index lookup. <a href=\"https://use-the-index-luke.com/sql/anatomy/slow-indexes\" target=\"_blank\">Read here</a>.</p>\n\n<p>One index on a table is not a big deal. You automatically have an index on columns (or combinations of columns) that are primary keys or declared as unique. (<a href=\"https://stackoverflow.com/questions/41410482/what-are-the-disadvantages-to-indexes-in-database-tables\" target=\"_blank\">Read this stackoverflow thread</a>). <span style=\"color:#27ae60;\"><em>More indexes means more work when insertion or deletion to table is happening.</em></span></p>\n\n<p>Some DBs like MySQL have clustered and non-clustered indexes. Clustered means indexes have both index and data pages (It&#39;s not just index, it also has table data). While there can be only one clustered index on a table, a table can have up to 999 nonclustered indexes. <a href=\"https://stackoverflow.com/questions/1251636/what-do-clustered-and-non-clustered-index-actually-mean\" target=\"_blank\">Read this stackoverflow thread</a>. Basically in clustered indexes, the leaf level of B+ Tree is the table itself. It means there is only one clustered index possible. That&#39;s why we need non-clustered indexes, which can have different key order.</p>\n\n<p>If the table has no clustered index it is called a heap.</p>\n\n<p>Non clustered indexes can be built on either a heap or a clustered index. They always contain a row locator back to the base table. In the case of a heap this is a physical row identifier (rid) and consists of three components (File:Page:Slot). In the case of a Clustered index the row locator is logical (the clustered index key) key.</p>\n\n<p><strong><em>Which index is faster? Clustered or Non-clustered?</em></strong> Just remember that the searching process using non-clustered indexes involves bookmark/pointer visiting also, and if query optimizer decides that the cost of visiting bookmarks is high, it can avoid usage of index altogether. <a href=\"https://www.youtube.com/watch?v=ITcOiLSfVJQ\" target=\"_blank\">Watch this video</a>. By default indexes created for Primary Key constraints are Clustered, and for other sorts Non-clustered.&nbsp; <a href=\"https://sqlperformance.com/2017/03/sql-indexes/performance-myths-clustered-vs-non-clustered\">Read this as well</a>.</p>\n\n<p>&nbsp;</p>\n\n<h2>Database Transactions</h2>\n\n<p>In a Database Management System, a transaction is a single unit of logic or work, sometimes made up of multiple operations. Any logical calculation done in a consistent mode in a database is known as a transaction. One example is a transfer from one bank account to another: the complete transaction requires subtracting the amount to be transferred from one account and adding that same amount to the other.</p>\n\n<p>The requirements for this are ACID:</p>\n\n<p><strong>A</strong>tomic: Either all statements executed or none (everything reverted). Handles even unusual situations like power failures, errors and crashes.</p>\n\n<p><strong>C</strong>onsistency: any data written to the database must be valid according to all defined rules, including <a class=\"mw-redirect\" href=\"https://en.wikipedia.org/wiki/Integrity_constraints\" title=\"Integrity constraints\">constraints</a>, <a class=\"mw-redirect\" href=\"https://en.wikipedia.org/wiki/Cascading_rollback\" title=\"Cascading rollback\">cascades</a>, <a href=\"https://en.wikipedia.org/wiki/Database_trigger\" title=\"Database trigger\">triggers</a>, and any combination thereof. In case of clustered setup, this means that any data written is updated to all data nodes.</p>\n\n<p><strong>I</strong>solation: Transactions are executed concurrently. So we need some sort of concurrency control.</p>\n\n<p><strong>D</strong>urability: Once a transaction has been committed, it must remain so. Simply, it means that transaction has taken its effect into disk (non-volatile memory).</p>\n\n<p>&nbsp;</p>\n\n<p><strong>How is ACID achieved?</strong></p>\n\n<p>&nbsp;</p>\n\n<p><strong>How is Isolation achieved?</strong></p>\n\n<p>Isolation mechanism is similar to achieving thread-safety in concurrent processing.</p>\n\n<p>When two transactions (T1, T2) run concurrently, they can run into different situations.</p>\n\n<ol>\n\t<li>T1 updates something, T2 reads updated data, but T1 reverts the change that it made. =&gt; <strong>Dirty Read</strong><br />\n\tSolve the problem with isolation-level&nbsp;READ COMMITTED.<br />\n\t&nbsp;</li>\n\t<li>T2&nbsp;reads a row with id=1, T1&nbsp;updates the row with id=1 and commits, T2 reads the row again. T2 got different data from reading the row within the same transaction, which can cause problems in the result. =&gt; <strong>Non-Repeatable Read.&nbsp;</strong><br />\n\tBut why would a transaction read a row twice in the same transaction? - It&#39;s actually not required, it&#39;s just to demonstrate the problem, that the calculation/decision would be based on stale data.<br />\n\tSolve the problem with&nbsp;REPEATABLE READ.<br />\n\t&nbsp;</li>\n\t<li>Similar problem as above but instead on a range or set of rows. T1 reads a range, T2 commits a change which affects that range (new row inserted/deleted),&nbsp;&nbsp;T1 reads the range again - gets different result. =&gt; <strong>Phantom Read</strong>.&nbsp;&nbsp;</li>\n\t<li>T1 reads a row with id=1, T2 reads the same row, T1 updates it and commit, T2&nbsp;updates and commits; update done by T1 is lost =&gt; <strong>Lost Update</strong>.<br />\n\t<br />\n\tNon Repeatable-Read,&nbsp; Phantom Read and Lost Update problems can be solved in two ways:<br />\n\t&nbsp;- Using Locks.&nbsp;<br />\n\t&nbsp;- Multiversion Concurrency Control (MVCC) / <em>Snapshot Isolation</em>. &nbsp;T2 will work on a snapshot and will ultimately commit only if DBMS can ensure that the result would have been same if transactions were serialised (i.e. first T2&nbsp;then T1), otherwise the commit will fail.<br />\n\t&nbsp;</li>\n\t<li>There is a constraint C (based on some data which is going to be used by both T1 &amp; T2). T1 and T2 execute concurrently, T1 reads data from a snapshot, T2 reads data from snapshot, T1 commits changes, T2 commits changes. DBMS cannot detect that there was a write conflict because writes are happening on different sets of data. And now the constraint is violated =&gt; <a href=\"https://stackoverflow.com/questions/48417632/why-write-skew-can-happen-in-repeatable-reads\"><strong>Write Skew</strong></a> problem.<br />\n\tSolve with SERIALIZABLE or locks.\n\t<pre>\n<code class=\"language-sql\">SELECT .. FOR UPDATE -- Locks all the records that match the criteria; Both/all concurrent transactions must use lock</code></pre>\n\t</li>\n</ol>\n\n<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</p>\n\n<p><strong>Note</strong>: MVCC in Oracle is wrongly named Serializable.</p>\n\n<p>&nbsp;</p>\n\n<h2>Database Designing</h2>\n\n<h3>Normalization vs Denormalization:</h3>\n\n<p>Normalization is done in order to structure data to reduce possibility of problems with data inconsistency.</p>\n\n<p>DeNormalization is done in order to avoid performance impact introduced by highly structured data which results into heavy join operations while querying data. You willingly introduce data redundancy in order to increase performance.&nbsp;</p>\n\n<p>It&#39;s classic Time-Space tradeoff decision.&nbsp;</p>\n\n<h3>Different Normal Forms:</h3>\n\n<p><em>Candidate Key: A set of attributes which could be used as a Primary Key (to uniquely identify the records in the table).</em></p>\n\n<p><em>Non-Trivial Multivalued Dependency X&nbsp;<img alt=\"{\\displaystyle \\twoheadrightarrow }\" aria-hidden=\"true\" src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/4b9c2ec85d900b1485fad362aabc2349a4d54ad3\" />&nbsp;Y: a dependency where Y is not subset of X and X+Y is not equal to all attributes in the table.</em></p>\n\n<p>&nbsp;</p>\n\n<p>1NF: Each field contains only one value, not a set of values.&nbsp;</p>\n\n<p>2NF: Every non-candidate-key attribute depends upon whole candidate key (not just part of it).&nbsp;</p>\n\n<p>3NF: Doesn&#39;t have transitional dependencies within same table.</p>\n\n<p>BCNF: For every functional dependency&nbsp;<i>X &rarr; Y,&nbsp;</i>either&nbsp;Y &sube; X or X is a super-key. Sometimes achieving BCNF is not feasible.</p>\n\n<p>4NF: For every Non-Trivial Multivalued Dependency X&nbsp;<img alt=\"{\\displaystyle \\twoheadrightarrow }\" aria-hidden=\"true\" src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/4b9c2ec85d900b1485fad362aabc2349a4d54ad3\" />&nbsp;Y, X is a super key (i.e. a candidate key or super-set of a candidate key). A subset of attributes does not depend upon subset of Super-Key.</p>\n\n<p>What&#39;s the process of normalization?</p>\n\n<p>The most practical way is to have some initial tables, add examples as per business scenario, then see which tables contain duplicate data.&nbsp;When you see duplicates, decompose into more tables.</p>\n\n<h3>Creating Indexes:</h3>\n\n<p>Decide which index would work for use-cases.</p>\n\n<p>Keep in mind that indexes require extra effort during writes.</p>\n\n<p><strong>Design For Purpose:</strong></p>\n\n<p>Use STAR schema model for analytics.</p>\n\n<p>&nbsp;</p>\n\n<h2>Distributed Databases</h2>\n\n<p>One DB server is not enough as application grows.&nbsp;</p>\n\n<p>Solution: Don&#39;t store all the data at one server, partition it, replicate it. Which comes at a cost, you now have to make sure that the data is consistent across servers.</p>\n\n<p><strong>Replication:</strong></p>\n\n<p>Single-Leader, Multi-Leader, Leaderless. Leader = Master = Primary; Followers = Slaves = Secondaries&nbsp;= Hot-standbys</p>\n\n<p>Writes are done only on Leaders. Reads can be done from any of them.</p>\n\n<p>Writes to Leader must be replicated to Followers synchronously or asynchronously.&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<h3>Is data loss possible in distributed databases?</h3>\n\n<p>If not properly configured and the whole cluster fails at the same time, then yes it&#39;s possible.</p>\n\n<p>&nbsp;</p>\n\n<p><strong><u>MySQL Cluster:</u></strong></p>\n\n<p>MySQL Cluster =&nbsp;<b>NDB Cluster + MySQL Server.&nbsp;</b>NDB Cluster is the actual distributed DBMS (without SQL support) here, MySQL Server provides SQL interface over it, adds authorisation support, asynchronous data replication to other nodes.</p>\n\n<p>MySQL Cluster does the horizontal sharding automatically behind the scenes based on hashing of primary keys. This means that whenever a new node is added or removed, data should be <a href=\"https://dev.mysql.com/doc/mysql-cluster-excerpt/8.0/en/mysql-cluster-online-add-node-basics.html\">re-partitioned</a>. It can be done with rolling start of different nodes or without rolling start.</p>\n\n<p>&nbsp;</p>\n\n<p><span style=\"font-size:16px;\"><strong>Is MySQL Cluster Consistent?</strong></span></p>\n\n<p>It depends: Single MySQL Cluster is CP (from CAP). It guarantees <strong>Strong Consistency</strong>. Data is synchronously replicated using 2PC Commit.</p>\n\n<blockquote>\n<p><span style=\"left: 391.2px; top: 158.324px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.829379);\">MySQL Cluster is implemented as a</span><span style=\"left: 601.314px; top: 158.324px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.804029);\">strongly <span class=\"highlight selected\">consisten</span>t,</span><span style=\"left: 715.034px; top: 158.324px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.80443);\">active/active, multi</span><span style=\"left: 823.6015px; top: 158.32441406249995px; font-size: 14px; font-family: sans-serif;\">-</span><span style=\"left: 828.319px; top: 158.324px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.755729);\">master database </span><span style=\"left: 391.2px; top: 179.124px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.793524);\">ensuring </span><span style=\"left: 443.529px; top: 179.124px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.768164);\">updates </span><span style=\"left: 490.346px; top: 179.124px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.772073);\">can be </span><span style=\"left: 530.459px; top: 179.124px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.783579);\">made to any </span><span style=\"left: 603.237px; top: 179.124px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.77805);\">node </span><span style=\"left: 634.316px; top: 179.124px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.775148);\">and </span><span style=\"left: 658.312px; top: 179.124px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.795787);\">are instantly available to </span><span style=\"left: 800.712px; top: 179.124px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.782657);\">the rest of the</span><span style=\"left: 881.755px; top: 179.124px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.801475);\">cluster, </span><span style=\"left: 391.2px; top: 199.924px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.824144);\">without any </span><span style=\"left: 462.026px; top: 199.924px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.8015);\">replication </span><span style=\"left: 526.143px; top: 199.924px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.801985);\">lag</span></p>\n\n<p><span style=\"left: 391.2px; top: 229.124px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.795097);\">Tables are au</span><span style=\"left: 466.709px; top: 229.124px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.798969);\">tomatically sharded across a pool of low cost commodity data nodes, enabling </span><span style=\"left: 391.2px; top: 250.324px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.822316);\">the database to scale horizontally, accessed both from SQL and directly via NoSQL APIs. </span><span style=\"left: 391.2px; top: 271.124px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.79254);\">New nodes can be added on</span><span style=\"left: 549.7309999999999px; top: 271.1244140624999px; font-size: 14px; font-family: sans-serif;\">-</span><span style=\"left: 554.449px; top: 271.124px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.788546);\">line, instantly scaling database capacity and performance, even </span><span style=\"left: 391.2px; top: 291.5244140624998px; font-size: 14px; font-family: sans-serif;\">f</span><span style=\"left: 395.925px; top: 291.524px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.797267);\">or the heaviest write loads.</span></p>\n\n<p><span style=\"left: 391.2px; top: 433.5244140624999px; font-size: 14px; font-family: sans-serif;\">U</span><span style=\"left: 401.431px; top: 433.524px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.788589);\">nlike other distributed databases, </span><span style=\"left: 591.449px; top: 433.524px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.840565);\">MySQL Cluster preserves</span><span style=\"left: 742.932px; top: 433.524px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.98928);\">ACID</span><span style=\"left: 777.5598333333334px; top: 433.5244140624999px; font-size: 14px; font-family: sans-serif;\">-</span><span style=\"left: 782.278px; top: 433.524px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.754682);\">guarantees,</span><span style=\"left: 849.929px; top: 433.524px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.819622);\">the fl</span><span style=\"left: 879.438px; top: 433.524px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.860881);\">exibility </span><span style=\"left: 391.2px; top: 454.324px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.854778);\">of </span><span style=\"left: 406.55px; top: 454.324px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.855156);\">JOIN operations</span><span style=\"left: 503.337px; top: 454.324px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.757193);\">and</span><span style=\"left: 527.333px; top: 454.324px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.787511);\">maintain</span><span style=\"left: 576.9021666666666px; top: 454.32441406249984px; font-size: 14px; font-family: sans-serif;\">s</span><span style=\"left: 585.957px; top: 454.324px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.794957);\">referential integrity</span><span style=\"left: 699.242px; top: 454.324px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.775833);\">between tables</span><span style=\"left: 786.566px; top: 454.324px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.798507);\">on different nodes, </span><span style=\"left: 896.731px; top: 454.324px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.806909);\">on </span><span style=\"left: 391.2px; top: 474.724px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.79695);\">different shards, </span><span style=\"left: 486.804px; top: 474.724px; font-size: 14px; font-family: sans-serif; transform: scaleX(0.783);\">even in different data centers</span><span style=\"left: 651.222px; top: 474.7244140624998px; font-size: 14px; font-family: sans-serif;\">.</span></p>\n\n<p><span style=\"left: 651.222px; top: 474.7244140624998px; font-size: 14px; font-family: sans-serif;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - <a href=\"https://www.mysql.com/products/cluster/mysql-cluster-datasheet.pdf\" target=\"_blank\">MySQL Docs</a> </span></p>\n</blockquote>\n\n<p>MySQL NDB (Network Database Cluster) is different from MySQL Replication (which is a master-slave configuration; asynchronous replication). NDB has option for async replication though.</p>\n\n<p>There must be a separate management node which will (normally) act as arbitrator. There can be multiple management servers.</p>\n\n<p>There should be multiple data nodes (for storage and replication).</p>\n\n<p>There should be multiple SQL nodes (instances of MySQL servers with support for&nbsp;<a class=\"link\" href=\"https://dev.mysql.com/doc/refman/5.7/en/mysql-cluster.html\" title=\"Chapter\u00a021\u00a0MySQL NDB Cluster 7.5 and NDB Cluster 7.6\"><code class=\"literal\">NDBCLUSTER</code></a> storage engine and started with the <code class=\"option\">--ndb-cluster</code> option to enable the engine and the <code class=\"option\">--ndb-connectstring</code> option to enable it to connect to an NDB Cluster management server.)</p>\n\n<p><strong>Is NDB Cluster Transaction safe?</strong></p>\n\n<p><span class=\"emphasis\"><em>Yes</em></span>. For tables created with the <a class=\"link\" href=\"https://dev.mysql.com/doc/refman/5.7/en/mysql-cluster.html\" title=\"Chapter\u00a021\u00a0MySQL NDB Cluster 7.5 and NDB Cluster 7.6\"><code class=\"literal\">NDB</code></a> storage engine, transactions are supported. Currently, NDB Cluster supports only the <a class=\"link\" href=\"https://dev.mysql.com/doc/refman/5.7/en/innodb-transaction-isolation-levels.html#isolevel_read-committed\"><code class=\"literal\">READ COMMITTED</code></a> transaction isolation level.</p>\n\n<p>In order for a table to be shared between nodes in an NDB Cluster, the table must be created using <code class=\"literal\">ENGINE=NDB</code></p>\n\n<p>Arbitrator: In Split-Brain scenarios, when network partitioning somehow caused the cluster to divide into two (or more) parts where the nodes in one set can see each other but not the nodes in other part. In this scenario, each set of nodes tries to behave as a full cluster, this is handled by Arbitrator.</p>\n\n<p>Arbitrator selects the node set as cluster, whichever contacts it first; and instructs other sets to shut down.</p>\n\n<p>&nbsp;</p>\n\n<h2>Database Sharding</h2>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<h2>Does MongoDB provide ACID?</h2>\n\n<p>Read this: <a href=\"https://stackoverflow.com/questions/11292215/where-does-mongodb-stand-in-the-cap-theorem\" target=\"_blank\">Stackoverflow thread</a>.</p>\n\n<p>Summary: MongoDB is strongly consistent when you use a single connection or the correct <a href=\"https://docs.mongodb.com/manual/reference/write-concern/\" rel=\"noreferrer\" target=\"_blank\">Write</a>/<a href=\"https://docs.mongodb.com/manual/reference/read-concern/#read-concern-levels\" rel=\"noreferrer\" target=\"_blank\">Read Concern Level</a> (<a href=\"http://techidiocy.com/write-concern-mongodb-performance-comparison/\" rel=\"noreferrer\" target=\"_blank\">Which will cost you execution speed</a>). Reads from secondaries have to be approved by majority, which ensures strong consistency, if majority concern level is set for both read and write (<a href=\"https://docs.mongodb.com/manual/core/replica-set-members/#replica-set-secondary-members\" target=\"_blank\">See doc</a>). As soon as you don&#39;t meet those conditions (especially when you are reading from a secondary-replica) MongoDB becomes Eventually Consistent.</p>\n\n<p>Sometimes consistency is sacrificed to achieve availability. E.g. when Primary goes down, new Primary is elected, every write done by old Primary but not sync&#39;ed to secondaries will be reverted and saved to a rollback-file, as soon as it reconnects, the old primary is now a secondary with inconsistent data.</p>\n\n<p>MongoDB supports <a href=\"https://www.mongodb.com/basics/acid-transactions\">multi-document transactions</a> by locking (for concurrent changes) documents involved.</p>\n\n<p>Updates of a single document are Atomic always.</p>\n\n<h2>&nbsp;</h2>\n\n<h2>Migrations</h2>\n\n<p>Schema-changes: MySQL copies entire table on&nbsp;ALTER TABLE command which makes it slower.</p>\n\n<p>Flyway schema migration tool.</p>\n\n<p>&nbsp;</p>\n\n<h2>Database Connection Pools</h2>\n\n<p>Database clients can maintain a <a href=\"https://stackoverflow.blog/2020/10/14/improve-database-performance-with-connection-pooling/#:~:text=Instead%20of%20opening%20and%20closing,of%20clients%20accessing%20it%20grow.\" target=\"_blank\">pool of DB connections</a>, because physical establishment of a DB connection is a costly process (You have to open up network sessions, authenticate, have authorisation checked, and so on).</p>\n\n<p>Libraries which provide connection pools: <a href=\"https://www.mchange.com/projects/c3p0/\" target=\"_blank\">C3P0</a>, <a href=\"https://commons.apache.org/proper/commons-dbcp/download_dbcp.cgi\">Apache Commons DBCP Component</a>, <a href=\"https://brettwooldridge.github.io/HikariCP/\">HikariCP</a></p>\n\n<p>There can be many cinguration parameters, some common ones are: max pool size, min pool size,max prepared statement size, max idle connections, idleTimeout etc.</p>\n\n<p>&nbsp;</p>\n\n<hr />\n<p>An example of Dynamic Procedure in Oracle SQL:</p>\n\n<pre>\n<code class=\"language-sql\">create or replace PROCEDURE proc_name (table_name IN VARCHAR2 DEFAULT 'default_tbl_name')\n As\n   TYPE cur_typ IS REF CURSOR;\n   cur_var cur_typ;\n\n   counter number(10);\n   str_var VARCHAR2(100);\n   query_str VARCHAR2(300);\n   num_var Number(30);\n\nBEGIN\n    begin\n        execute immediate 'alter table ' || table_name || ' add col_name number(30)'; --Add some column if required\n    exception\n        when others \n        THEN dbms_output.put_line(SQLCODE);\n    end;\n    \n    num_var :=0;\n    query_str := 'SELECT a FROM ' || table_name || ' where some_col_name = :col';\n\n    str_var := 'my_column';\n    open item for query_str using str_var; -- parameter binding is index-based\n\n    LOOP\n        fetch item into num_var; -- index based assignment\n        query_str2 := 'SELECT ....';\n\n        execute immediate 'merge into ....';\n\n       counter := counter +1;\n\n       IF (counter =1000)\n       THEN\n            counter :=0;\n            COMMIT;\n       END IF;\n    END LOOP;\n    close item; --close cursor\n\n    commit;\nEND;\n</code></pre>\n\n<p>&nbsp;</p>\n\n<p>Links:</p>\n\n<p>http://www.bailis.org/papers/acidrain-sigmod2017.pdf</p>\n\n<p>https://www.cockroachlabs.com/blog/what-write-skew-looks-like/</p>\n\n<p>https://planetscale.com/blog/challenges-of-supporting-foreign-key-constraints</p>\n\n<p>&nbsp;</p>\n<style type=\"text/css\">@import url('https://fonts.googleapis.com/css?family=Montserrat');\n  .content h1, .content h3, .content h2 {\n    font-family: \"Montserrat\" !important;\n    text-shadow: 7px 7px 0px #d2d2d22e;\n    text-align: center;\n    padding: 1em;\n    /* margin-bottom: 1em; */\n    text-decoration: underline;\n    font-size: 2em !important;\n    line-height: 2em !important;\n    color: #986729;\n  }\n</style>\n", "authorId": 1, "subject": "architecture", "tags": [], "img": "", "summary": "", "lastUpdated": "2024-05-07 20:35:34.096830"}