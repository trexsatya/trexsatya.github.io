{"name":"Solving Data Science Problems","id":176,"content":"<p>What to look for? Or, what are the problem statements?</p>\n\n<ol>\n\t<li>Prediction: If X=(X1, X2,..) changes, does the Y also change? If yes, can we predict the change? Correlation measures the strength of such relationship, Regression quantifies the nature of relationship.</li>\n\t<li>Multilabel classification</li>\n\t<li>Binary Classification</li>\n\t<li>Prediction in time</li>\n</ol>\n\n<p>&nbsp;</p>\n\n<p>How would you select the features?</p>\n\n<ul>\n\t<li>Do you need to transform feature data first?</li>\n\t<li>Do features contain redundant information?</li>\n\t<li>What is the strength of their&nbsp;correlation?</li>\n\t<li>Are there even enough (observed) values for a&nbsp; feature?</li>\n\t<li>Should we include synthetic features?</li>\n\t<li>Is this feature even significant for the model?</li>\n\t<li>How would you identify outliers? Using Regression? Using Box Plots?</li>\n\t<li>Would you check for autocorrelation?</li>\n\t<li>Would you do PCA for feature selection?</li>\n\t<li>Would you use Random Forest for finding important features?</li>\n</ul>\n\n<p>Look out for extreme cases, avoid extrapolation. E.g. in case of housing price prediction, if you try to predict price for vacant space for x-sq-feet; and the training data didn&#39;t have such examples, the output would be absurd.</p>\n\n<p><strong>USING DISCRIMINANT ANALYSIS FOR FEATURE SELECTION</strong></p>\n\n<p>If the predictor variables are normalized prior to running LDA, the discriminator weights are measures of variable importance, thus providing a computationally efficient method of feature selection.</p>\n\n<hr />\n<p>Transformation:</p>\n\n<p>To transform categorical variables (enums) into numbers:</p>\n\n<ul>\n\t<li>Dummy (Binary) Variables / One-hot Encoding: Binary number where,&nbsp;for example, 0010 means third value (out of 4 unique values) of enum for this feature.</li>\n\t<li>Group variables: if there are too many, but most of them have few occurence.</li>\n\t<li>Convert the ordered factor variables into sorted numbers?</li>\n</ul>\n\n<p>Would you use KNN for feature engineering?</p>\n\n<p>&nbsp;</p>\n\n<p>There are other encodings:&nbsp;</p>\n\n<ul>\n\t<li><strong><em>Reference coding /&nbsp;</em></strong>treatment coding<strong><em>:</em></strong>&nbsp;one level of a factor is used as a reference and other factors are compared to that level.</li>\n\t<li><strong><em>Deviation coding / </em></strong>sum contrasts<strong><em>:</em></strong>&nbsp;A type of coding that compares each level against the overall mean as opposed to the reference level.</li>\n</ul>\n\n<p>&nbsp;</p>\n\n<p>How would you handle imbalanced data &mdash; data in which the outcome of interest (purchase on a website, insurance fraud, etc.) is rare?</p>\n\n<p><strong><em>Undersample / </em></strong>Downsample<strong><em>:</em></strong>&nbsp;Use fewer of the prevalent class records in the classification model.</p>\n\n<p><strong><em>Oversample</em></strong>&nbsp;/&nbsp;Upsample:&nbsp;Use more of the rare class records in the classification model, bootstrapping if necessary.</p>\n\n<p><strong><em>Up weight or down weight:</em></strong>&nbsp;Attach more (or less) weight to the rare (or prevalent) class in the model.</p>\n\n<p><strong><em>Data generation:</em></strong>&nbsp;Like bootstrapping, except each new bootstrapped record is slightly different from its source. SMOTE algo&nbsp;creates a synthetic record that is a randomly weighted average of the original record and the neighboring record, where the weight is generated separately for each predictor. The number of synthetic oversampled records created depends on the oversampling ratio required to bring the data set into approximate balance, with respect to outcome classes.</p>\n\n<p><strong><em>Z-score:</em></strong>&nbsp;The value that results after standardization.</p>\n\n<p><strong>K:</strong> The number of neighbors considered in nearest-neighbor calculation.</p>\n\n<p>&nbsp;</p>\n\n<hr />\n<p>How would you select the model?</p>\n\n<ul>\n\t<li>Does this type of model do better than that type of model? Would you use RMSE, RSE,&nbsp; p-value,&nbsp; t-statistics, ROC, AUC?</li>\n\t<li>Does this feature enhance the model? Is it worth it?</li>\n\t<li>What metric would you use for comparing models?</li>\n\t<li>Should&nbsp;we&nbsp; combine&nbsp; models? Boosting? Ensemble?</li>\n</ul>\n\n<p>There are methods for selection: Forward Selection (start with zero feature, add one by one as long as it improves significantly), Backward Selection/Elimination (start with all features, drop one by one till it impacts performance significantly), Ridge Regression, Lasso Regression which are form of Penalized Selection (applies the penalty by reducing coefficients)</p>\n\n<p>&nbsp;</p>\n\n<hr />\n<p>How would you tune the model?</p>\n\n<ul>\n\t<li>Would you use statistics on some features to guide your model?</li>\n\t<li>Would you use the coefficients of regression model/equation to gain&nbsp;insight and guide the improvement of model?</li>\n\t<li>Would you include interactions between variables?</li>\n\t<li>Would you worry about or gain insight from the analysis on whether assumptions are satisfied? E.g. Outliers, Influential Value, Leverage/hat-value,&nbsp;<strong><em>Heteroskedasticitys</em></strong></li>\n</ul>\n\n<h5>&nbsp;</h5>\n\n<p>How would you explore the predictions of the mode?</p>\n\n<p>&nbsp;</p>\n\n<hr />\n<h5>KEY TERMS FOR INTERPRETING THE REGRESSION EQUATION</h5>\n\n<dl>\n\t<dt><strong><em>Correlated variables</em></strong></dt>\n\t<dd>\n\t<p>When the predictor variables are highly correlated, it is difficult to interpret the individual coefficients.<a data-primary=\"correlated variables\" data-type=\"indexterm\" id=\"idm45909261144472\"></a></p>\n\t</dd>\n\t<dt><strong><em>Multicollinearity /&nbsp;</em></strong>collinearity</dt>\n\t<dd>\n\t<p>When the predictor variables have perfect, or near-perfect, correlation, the regression can be unstable or impossible to compute<a data-primary=\"multicollinearity\" data-type=\"indexterm\" id=\"idm45909261141816\"></a></p>\n\t</dd>\n\t<dt><strong><em>Confounding variables</em></strong></dt>\n\t<dd>\n\t<p>An important predictor that, when omitted, leads to spurious relationships in a regression equation.<a data-primary=\"confounding variables\" data-type=\"indexterm\" id=\"idm45909261137400\"></a></p>\n\t</dd>\n\t<dt><strong><em>Main effects</em></strong></dt>\n\t<dd>\n\t<p>The relationship between a predictor and the outcome variable, independent from other variables.<a data-primary=\"main effects\" data-type=\"indexterm\" id=\"idm45909261134776\"></a></p>\n\t</dd>\n\t<dt><strong><em>Interactions</em></strong></dt>\n\t<dd>\n\t<p>An interdependent relationship between two or more predictors and the response.</p>\n\t</dd>\n</dl>\n\n<p>Multicollinearity occurs when:</p>\n\n<ul>\n\t<li>\n\t<p>A variable is included multiple times by error.</p>\n\t</li>\n\t<li>\n\t<p><em>P</em>&nbsp;dummies, instead of&nbsp;<em>P</em>&nbsp;&ndash; 1 dummies, are created from a factor variable (see&nbsp;<a data-type=\"xref\" href=\"https://learning.oreilly.com/library/view/practical-statistics-for/9781491952955/ch04.html#FactorsRegression\">&ldquo;Factor Variables in Regression&rdquo;</a>).</p>\n\t</li>\n\t<li>\n\t<p>Two variables are nearly perfectly correlated with one another.</p>\n\t</li>\n</ul>\n\n<p><em>NOTE: Multicollinearity is not such a problem for nonregression methods like trees, clustering, and nearest-neighbors, and in such methods it may be advisable to retain&nbsp;P&nbsp;dummies (instead of&nbsp;P&nbsp;&ndash; 1). That said, even in those methods, nonredundancy in predictor variables is still a virtue.</em></p>\n\n<p><em><strong>Confounding Variables</strong></em></p>\n\n<p>With correlated variables, the problem is one of commission: including different variables that have a similar predictive relationship with the response.<a data-primary=\"confounding variables\" data-type=\"indexterm\" id=\"idm45909260869256\"></a><a data-primary=\"regression\" data-secondary=\"interpreting the regression equation\" data-tertiary=\"confounding variables\" data-type=\"indexterm\" id=\"idm45909260868552\"></a>&nbsp;With&nbsp;<em>confounding variables</em>, the problem is one of omission: an important variable is not included in the regression equation. Naive interpretation of the equation coefficients can lead to invalid conclusions.</p>\n\n<p>When two variables are interacting, it means&nbsp; that factors combined&nbsp; have more impact on&nbsp; output variable. E.g. how much the area of house increase the price depends on the location of house. So, are of house and location are interacting variables. (Perhaps the most common approach is the use&nbsp;<em>tree models</em>, as well as their descendents,&nbsp;<em>random forest</em>&nbsp;and&nbsp;<em>gradient boosted trees</em>.<a data-primary=\"gradient boosted trees\" data-type=\"indexterm\" id=\"idm45909260637960\"></a>&nbsp;This class of models automatically searches for optimal interaction terms)</p>\n\n<p>In regression, <strong>outliers</strong> are found by&nbsp;standardized residuals (&ldquo;the number of standard errors away from the regression line.&rdquo;)</p>\n\n<p>Influential Variables can be found by hat-value (values above&nbsp;2(P+1)/n indicate a high-leverage data value)&nbsp;or Cook&#39;s value (an observation has high influence if Cook&rsquo;s distance exceeds&nbsp;4/(n&minus;P&minus;1)).&nbsp;An&nbsp;<em>influence plot</em>&nbsp;or&nbsp;<em>bubble plot</em>&nbsp;combines standardized residuals, the hat-value, and Cook&rsquo;s distance in a single plot.</p>\n\n<p>Statisticians pay considerable attention to the distribution of the residuals. It turns out that ordinary least squares&nbsp;are unbiased, and in some cases the &ldquo;optimal&rdquo; estimator, under a wide range of distributional assumptions.<a data-primary=\"regression\" data-secondary=\"diagnostics\" data-tertiary=\"heteroskedasticity, non-normality, and correlated errors\" data-type=\"indexterm\" id=\"idm45909260233048\"></a><a data-primary=\"residuals\" data-secondary=\"distribution of\" data-type=\"indexterm\" id=\"idm45909260231864\"></a><a data-primary=\"ordinary least squares (OLS)\" data-type=\"indexterm\" id=\"idm45909260230920\"></a>&nbsp;This means that in most problems, data scientists do not need to be too concerned with the distribution of the residuals.&nbsp;One area where this may be of concern to data scientists is the standard calculation of confidence intervals for predicted values, which are based upon the assumptions about the residuals.&nbsp;<em>Heteroskedasticity</em>&nbsp;is the lack of constant residual variance across the range of the predicted values.<a data-primary=\"heteroskedasticity\" data-type=\"indexterm\" id=\"idm45909260227512\"></a>&nbsp;In other words, errors are greater for some portions of the range than for others.</p>\n\n<p><strong>WHY WOULD A DATA SCIENTIST CARE ABOUT HETEROSKEDASTICITY?</strong></p>\n\n<p>Heteroskedasticity indicates that prediction errors differ for different ranges of the predicted value, and may suggest an incomplete model. For example, the heteroskedasticity in&nbsp;<code>lm_98105</code>&nbsp;may indicate that the regression has left something unaccounted for in high- and low-range homes.</p>\n\n<p><br />\n&nbsp;</p>\n\n<hr />\n<p>How would you validate the model?</p>\n\n<p>Cross-Validation</p>\n\n<p><a href=\"https://www.kaggle.com/konradb/adversarial-validation-and-other-scary-terms\">Adversarial Validation</a>&nbsp;Use this if training and test data has been provided to you but they differ <em>(which is actually a violation of &quot;Identically Distributed&quot; assumption)</em>.</p>\n\n<p>The main goal is that our model/classifier would work as good on unseen data as it does on seen data (i.e. training data). To see that it actually does so, we use validation dataset to verify.</p>\n\n<p>To see that data in train and test set are mostly indistinguishable, we can use some binary classifier &mdash; Just combine the train and test data, assign label 0 to train examples and label 1 to test examples. Then use some basic form of cross-validation. Then use a classifier to predict the label. And see the accuracy. If the test and train data were actually indistinguishable (i.e. from same distribution) the ROC/AUC would be around 0.5 - 0.7 which is the case for random numbers.</p>\n\n<p>So, if the test and train data differ, we can just use the&nbsp;classifier mentioned above to predict label on training examples to filter out those examples which we know that are from training set but the classifier says that it is test example (which means that this training example resembles test dataset), and we use these filtered examples as validation set.</p>\n\n<p>Although keep in mind that this approach, in principle, is close to <em><strong>Data Snooping</strong></em>.</p>\n\n<p>&nbsp;</p>\n\n<p><strong>Boosting Algorithms:</strong></p>\n\n<p>The idea is to ensemble many weak/inefficient classifiers (hardly any better than random guessing) to come up with a strong classifier/model.&nbsp;</p>\n\n<p>This is done in an iterative approach. The&nbsp; examples which&nbsp;are misclassified gain weight before feeding into the next classifier.</p>\n\n<p>One of the most famous algorithms for this <a href=\"http://dmlc.cs.washington.edu/xgboost.html\">XGBOOST</a>&nbsp;which is a form of <a href=\"https://en.wikipedia.org/wiki/Gradient_boosting\">Gradient Boosting</a>.&nbsp;</p>\n\n<p><a href=\"https://lightgbm.readthedocs.io/en/latest/\">LightGBM</a>&nbsp;&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n","authorId":null,"subject":"ai","tags":null,"img":null,"summary":null,"lastUpdated":"2020-06-16T16:38:32.829+0000"}