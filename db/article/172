{"name": "ElasticSearch", "id": 172, "content": "<h2>What is ElasticSearch?</h2>\n\n<p>Elasticsearch is the distributed real-time search and analytics engine built on topo of Lucene library. <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/elasticsearch-intro.html\" target=\"_blank\">ElasticSearch documentation</a> is quite explanatory. Lucene handles the on-disk storage, indexing, and searching of documents, while ElasticSearch handles document updates, the API, and distribution.</p>\n\n<p>&nbsp;</p>\n\n<h2>Why ElasticSearch?</h2>\n\n<p>Ask yourself these questions:</p>\n\n<ul>\n\t<li>Do you need to search withing huge datastore?</li>\n\t<li>Do you need concurrent access/search/query to your huge datastore?</li>\n\t<li>Do you need to scale your searching capability on demand?</li>\n\t<li>Do you need to perform some sort of analytics on your datastore?</li>\n</ul>\n\n<p>Elasticsearch can help you with all these requirements.</p>\n\n<h2>Can We Use ElasticSearch to store our data primarily?</h2>\n\n<p>It depends. The resiliency of elasticsearch storage could be suitable for some but not for others. See <a href=\"https://www.elastic.co/guide/en/elasticsearch/resiliency/current/index.html\" target=\"_blank\">ElasticSearch documentation</a> on this matter.</p>\n\n<h2>How Does ElasticSearch Work?</h2>\n\n<p>Because index construction is a somewhat expensive process, Elasticsearch provides a faster, more strongly consistent database backed by a write-ahead log. Document creation, reads, updates, and deletes talk directly to this strongly-consistent database, which is asynchronously indexed into Lucene. Search queries lag behind the &ldquo;true&rdquo; state of Elasticsearch records, but should eventually catch up. One can force a flush of the transaction log to the index, ensuring changes written before the flush are made visible.</p>\n\n<p>ElasticSearch uses <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/optimistic-concurrency-control.html\" target=\"_blank\">Optimistic Concurrency Control</a>&nbsp;(OCC) for synchronized updation of a single document.</p>\n\n<p><em>[OCC in short: Every change will have a serial number, and ElasticSearch will never allow a change with lower serial number to override a higher one.]</em></p>\n\n<p>Elasticsearch uses a data structure called an inverted index that supports very fast full-text searches. An inverted index lists every unique word that appears in any document and identifies all of the documents each word occurs in.</p>\n\n<p>By default, Elasticsearch indexes all data in every field and each indexed field has a dedicated, optimized data structure. For example, text fields are stored in inverted indices, and numeric and geo fields are stored in BKD trees. The ability to use the per-field data structures to assemble and return search results is what makes Elasticsearch so fast.</p>\n\n<p>Elasticsearch can create indexes automatically from fields using <strong>dynamic mapping</strong>. However for advanced purposes you can control the&nbsp; mapping completely.</p>\n\n<p>It&rsquo;s often useful to index the same field in different ways for different purposes. For example, you might want to index a string field as both a text field for full-text search and as a keyword field for sorting or aggregating your data. Or, you might choose to use more than one language analyzer to process the contents of a string field that contains user input.</p>\n\n<p><strong>Distributed Architecture:</strong></p>\n\n<p>Under the covers, an Elasticsearch index is really just a logical grouping of one or more physical shards, where each shard is actually a self-contained index.</p>\n\n<p>By distributing the documents in an index across multiple shards, and distributing those shards across multiple nodes, Elasticsearch can ensure redundancy, which both protects against hardware failures and increases query capacity as nodes are added to a cluster. As the cluster grows (or shrinks), Elasticsearch automatically migrates shards to rebalance the cluster.</p>\n\n<p>There are two types of shards: primaries and replicas. Each document in an index belongs to one primary shard. A replica shard is a copy of a primary shard. Replicas provide redundant copies of your data to protect against hardware failure and increase capacity to serve read requests like searching or retrieving a document.</p>\n\n<p>The number of primary shards in an index is fixed at the time that an index is created, but the number of replica shards can be changed at any time, without interrupting indexing or query operations.</p>\n\n<p>Visit this <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/scalability.html\" target=\"_blank\">documentation for scalability</a>.</p>\n\n<p>You cannot just keep increasing shards for indexing. More shards means more overhead in maintaining index, larger the shard size means more time to move it around while rebalancing the cluster.</p>\n\n<p>Querying lots of small shards makes the processing per shard faster, but more queries means more overhead, so querying a smaller number of larger shards might be faster. In short&hellip;\u200bit depends.</p>\n\n<p>As a starting point:</p>\n\n<div class=\"itemizedlist\">\n<ul class=\"itemizedlist\" type=\"disc\">\n\t<li class=\"listitem\">Aim to keep the average shard size between a few GB and a few tens of GB. For use cases with time-based data, it is common to see shards in the 20GB to 40GB range.</li>\n\t<li class=\"listitem\">Avoid the gazillion shards problem. The number of shards a node can hold is proportional to the available heap space. As a general rule, the number of shards per GB of heap space should be less than 20.</li>\n</ul>\n</div>\n\n<p>&nbsp;</p>\n\n<p>For performance reasons, the nodes within a cluster need to be on the same network. But you cannot confine your cluster to a particular data centrer, what if data center goes down? The answer? Cross-cluster replication (CCR) where the other cluster could be in another data centers which can serve as a hot backup. Also CCR can be used to take advantage of geo-proximity for read operations.</p>\n\n<p>&nbsp;</p>\n\n<p>Cross-cluster replication is active-passive. The index on the primary cluster is the active leader index and handles all write requests. Indices replicated to secondary clusters are read-only followers.</p>\n\n<p>(Network Partitioning)</p>\n\n<p>What can happen when one part of the cluster is down? - Increased load</p>\n\n<p>What can happen when one part of the cluster is not able to communicate with the other part? - Data inconsistency, Data not found for sometime until cluster heals.</p>\n\n<p>We cannot wait for the whole cluster to write a document, otherwise we will just keep waiting in face of network partitioning. So ES uses Consensus algorithm. &quot;If majority of nodes have written, it&#39;s okay.&quot; Master gathers consensus.</p>\n\n<p>Quoram based Consensus algorithms are used for cluster state management and metadata management, not for data management (because Quorams need many copies and it increases space requirement)</p>\n\n<ul>\n\t<li>Paxos</li>\n\t<li>Viewstamped Replication</li>\n\t<li>Raft</li>\n\t<li>Zab</li>\n</ul>\n\n<p>Write Flow:</p>\n\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Write request goes to Node-x, Node-x finds which node has primary shard, sends the request to that node if it itself does not have the primary. Primary does the operation. Primary sends the request to replicas in parallel. Primary acknowledges the write after it gets response from all the replicas requested.</p>\n\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>What if one of the replicas is down?</strong></p>\n\n<p>Node A has primary shard, we write document-D it goes to primary shard, but the replica node B is down at that time. But after some time, node A goes down, and B comes up. If B is promoted to master, and we query for document-D, it won&#39;t return anything. How to handle this? Master decides which nodes have primary shard and which have replica shards. Master writes this info into global state (along with other metadata). Since version 5, ES uses Allocation ID to uniquely identify shard copies. Master tracks subset of copies that are in-sync and persists this in global cluster state, changes are backed by consensus layer. This is to track most recent copies of primary shard, so that we do not elect a stale shard node. Primary acknowledges a write only if master has acknowledged that the node that is down has been removed from election candidate because it is stale.</p>\n\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>How to update the sleeping replica when it comes back?</strong></p>\n\n<p>Initial approach: When sleeping replica comes back it co-ordinates with primary and compares files and their checksums to see if any update is required. But there is a problem, what if the size of data to sync is large and while the synching process is going on the primary is again being modified simultaneously. Again, inconsistency. To solve that, ES basically recreated a shard each time the sleeping replica comes back up.</p>\n\n<p>Advanced approach: Maintain a operation log with sequence numbers. So when replica comes back up, history of operations is sent. But we still have problem, history is sent asynchronously to replicas, so replicas may have divergent histories. What happens if the primary goes down, and the replicas are not consistent with each other? To solve this checkpoints (local) are created on shards. Replicas will be always kept consistent till checkpoints. If checkpoint is #5, it means shard has all oeprations #1 to #5. A global checkpoint is maintained in cluster state. So, we just have to resync some part of history. The shards which have advanced beyond global checkpoint need to be rolled back.</p>\n\n<h2>How Reliable is ElasticSearch as a Database?</h2>\n\n<p>Problems in a large cluster (in hundreds of nodes):</p>\n\n<p>Power Failures - ES depends on Lucene to handle this.</p>\n\n<p>Hard Drive failures -</p>\n\n<p style=\"margin-left: 40px;\">Complete Failure - ES handles this by replication.</p>\n\n<p style=\"margin-left: 40px;\">Ran out of space - Disk space aware allocation decider. ES monitors for this.</p>\n\n<p style=\"margin-left: 40px;\">Data Corruption - is more complicated. ES might detect corruption but not always. It gets worse if corrupted data is replicated. You can use snapshot/restore API to manually restore a backup to corrupted shard index. A more costly option is <code>&quot;index.shard.check_on_startup&quot;: &quot;fix&quot;</code> setting which forces ES to check every index file on startup.</p>\n\n<p>Cluster Consistency Issues:</p>\n\n<p style=\"margin-left: 40px;\">Complete Node Failure -</p>\n\n<p style=\"margin-left: 40px;\">Node is unresponsive -</p>\n\n<p style=\"margin-left: 40px;\">Network Failure -</p>\n\n<p>&nbsp;</p>\n\n<h2>Can We Run Multiple ES Instances/Nodes On A Single Server</h2>\n\n<p>Distributed systems are basically made for horizontal scaling, add more smaller machines when needed. So running multiple instances on single server doesn&#39;t make sense &quot;generally&quot;. However, it is possible and in some constrained cases it might be required (basically when you have very powerful hardware and you want to squeeze out everything from it). But you do need to run tests to verify that this setup isn&#39;t counterproductive.</p>\n\n<p>Read this thread: <a href=\"https://discuss.elastic.co/t/can-i-run-multiple-elasticsearch-nodes-on-the-same-machine/67\" target=\"_blank\">Elasticsearch thread</a>.</p>\n\n<p><a href=\"https://github.com/elastic/elasticsearch-formal-models\" target=\"_blank\">Check out the TLA+ Formal Specification of Elasticsearch.</a>&nbsp;and <a href=\"https://www.youtube.com/watch?v=qYDcbcOVurc&amp;list=PLWLcqZLzY8u_Osnz-YPOVrptG1ys73OkR&amp;index=7&amp;t=0s\" target=\"_blank\">this video</a>.</p>\n", "authorId": 1, "subject": "architecture", "tags": [], "img": "", "summary": "", "lastUpdated": "2024-05-01 10:27:43.827174"}