1
00:00:02,920 --> 00:00:10,400
Wow, what a place! But ... Jack? Jack?

2
00:00:10,560 --> 00:00:16,160
-Hello! Here I am! -Yeah, where did you end up?

3
00:00:16,320 --> 00:00:19,320
It happens crazy stuff in my channel sometimes.

4
00:00:19,480 --> 00:00:22,200
Do you have hundreds of thousands of followers?

5
00:00:22,360 --> 00:00:26,280
YES! The trick is to go up at regular intervals at night-

6
00:00:26,480 --> 00:00:28,480
-To cover different groups.

7
00:00:28,640 --> 00:00:33,920
I am the third largest account in New Zealand after the bird Hector.

8
00:00:34,080 --> 00:00:35,840
Wow!

9
00:00:36,000 --> 00:00:40,480
You don't really understand social media. We go through them!

10
00:00:40,640 --> 00:00:43,840
-But ... -listen, I like you.

11
00:00:44,000 --> 00:00:48,520
But your channel is a bit ... So, you need a little more ...

12
00:00:48,680 --> 00:00:54,680
So, weave it in ... with a little LA! A little music, young people.

13
00:00:54,840 --> 00:00:57,840
It should be fun! A naj's background.

14
00:00:58,000 --> 00:00:59,960
You are absolutely right, Edwin!

15
00:01:00,120 --> 00:01:03,080
And some iconic outfits.

16
00:01:08,360 --> 00:01:09,920
Anyway ...

17
00:01:10,080 --> 00:01:15,080
And grind on your message. You talk about a hundred things at the same time.

18
00:01:15,240 --> 00:01:18,320
But these are interesting things we are talking about.

19
00:01:18,480 --> 00:01:23,280
Choose a thing and focus on it. It should be clear and easy.

20
00:01:23,440 --> 00:01:27,560
You want more followers, huh? Who is following you now?

21
00:01:27,720 --> 00:01:32,920
Yes ... we will see ... Ulla-Britt in Växjö, librarian.

22
00:01:33,080 --> 00:01:36,320
Look, she has fun pictures of her cat!

23
00:01:36,480 --> 00:01:39,480
After all, one's followers tend to be as one's self.

24
00:01:39,640 --> 00:01:45,520
When adults try to be "down with the kids" it can be Cringe.

25
00:01:45,680 --> 00:01:49,280
Social media is about being able to reflect sig-

26
00:01:49,440 --> 00:01:53,120
-and relate to the one who becomes the sender.

27
00:01:53,280 --> 00:01:55,800
It becomes a bit of an age issue.

28
00:01:55,960 --> 00:02:00,840
Speaking of it, I get spinning here. Can we sit down somewhere else?

29
00:02:01,000 --> 00:02:04,360
-We go out of here. -Okay.

30
00:02:20,240 --> 00:02:24,840
-I don't get him out of the channel. -It does not matter.

31
00:02:27,600 --> 00:02:30,800
Strange that you haven't heard of any business-

32
00:02:30,960 --> 00:02:34,960
-As you want you to advertise some product.

33
00:02:35,120 --> 00:02:37,560
Funny you should say that, Åsa!

34
00:02:37,720 --> 00:02:43,680
I have received a contract with Perfect Beautiful Amazing Digital Media AB.

35
00:02:43,840 --> 00:02:46,200
They take care of my social media.

36
00:02:46,360 --> 00:02:50,560
Social media is a good thing to take up in "Källkoll".

37
00:02:50,720 --> 00:02:53,800
They affect the information flow of our time.

38
00:02:53,960 --> 00:02:58,880
Many get their information almost only from social media.

39
00:02:59,040 --> 00:03:03,360
Just! You talk about social media platform-

40
00:03:03,520 --> 00:03:07,680
-And then you may mean mainly the technology-

41
00:03:07,840 --> 00:03:12,240
-The then, what it looks like under the hood on Youtube and so on.

42
00:03:12,400 --> 00:03:15,440
But one also means the world that arises-

43
00:03:15,600 --> 00:03:19,920
-When we interact with each other.

44
00:03:20,080 --> 00:03:24,760
Type algorithms? They are scary accurate sometimes.

45
00:03:24,920 --> 00:03:27,080
How do they work purely technically?

46
00:03:27,240 --> 00:03:30,920
My name is Ania and I work in the magazine "New Technology".

47
00:03:31,080 --> 00:03:34,520
Algorithms work a bit like a recipe.

48
00:03:34,680 --> 00:03:42,120
If you think the information is the ingredients in a prescription-

49
00:03:42,280 --> 00:03:45,640
-And then you will get a certain result.

50
00:03:45,800 --> 00:03:49,360
A certain cake, if we are to take such a comparison.

51
00:03:49,520 --> 00:03:52,840
In social media contexts you can say-

52
00:03:53,000 --> 00:03:58,840
-Do the algorithms above all help to sort content-

53
00:03:59,000 --> 00:04:04,800
-And determines what you as a user see first in your feed-

54
00:04:04,960 --> 00:04:08,600
-or what kind of content you see more of.

55
00:04:08,760 --> 00:04:13,240
It may be based on who you are and what you have liked and followed-

56
00:04:13,400 --> 00:04:18,400
-But also what your friends have followed and commented, or where you live.

57
00:04:18,560 --> 00:04:23,280
The important thing is that the algorithms are not created from nowhere.

58
00:04:23,440 --> 00:04:27,240
People create them. They are not always perfect.

59
00:04:27,400 --> 00:04:33,960
You think they make a perfectly individualized content for you.

60
00:04:34,120 --> 00:04:39,480
But it can fail and I think many have noticed in ads.

61
00:04:39,640 --> 00:04:43,280
One wonders: "Why do I get this one?"

62
00:04:43,440 --> 00:04:47,720
Then it has collected a lot of different information and determined:

63
00:04:47,880 --> 00:04:51,280
"This is probably the kind of cake that fits."

64
00:04:51,440 --> 00:04:53,040
Okay!

65
00:04:53,200 --> 00:04:59,400
But algorithms are made by humans and bring our prejudices with them.

66
00:04:59,560 --> 00:05:01,040
How?

67
00:05:01,200 --> 00:05:04,320
Close your eyes and imagine a shoe.

68
00:05:04,480 --> 00:05:07,400
Okay. Did it look like this?

69
00:05:07,560 --> 00:05:10,680
Like this? Or maybe like this?

70
00:05:10,840 --> 00:05:16,400
We may not know why, but everyone prefers one shoe over another.

71
00:05:16,560 --> 00:05:20,480
Imagine you should teach a computer what a shoe is.

72
00:05:20,640 --> 00:05:23,280
You may teach it your own opinions.

73
00:05:23,440 --> 00:05:26,280
So prejudice enters machine learning.

74
00:05:26,440 --> 00:05:30,920
YouTube has been accused of their suggestions for the next video

75
00:05:31,080 --> 00:05:32,920
-Have radicalized people.

76
00:05:33,080 --> 00:05:37,160
People start with cooking and become conspiracy theorists.

77
00:05:37,320 --> 00:05:41,520
Isn't it a way that technology interacts with our thoughts?

78
00:05:41,680 --> 00:05:45,000
Exactly, it's hard to get to the bottom with such.

79
00:05:45,160 --> 00:05:49,520
The technology companies are not so open with how the algorithms work.

80
00:05:49,680 --> 00:05:53,840
You do not know the recipe for something to be virtually.

81
00:05:54,000 --> 00:05:58,280
-For then would we take advantage of it? -The precise, gejma system.

82
00:05:58,440 --> 00:06:01,240
But sometimes it has been obvious.

83
00:06:01,400 --> 00:06:08,280
On Facebook, angry clicks have led to viral things.

84
00:06:08,440 --> 00:06:12,800
What social media are there? Which ones are the biggest?

85
00:06:12,960 --> 00:06:15,760
-Facebook. -If you're type 40.

86
00:06:15,920 --> 00:06:18,640
I would say tictok.

87
00:06:18,800 --> 00:06:23,720
Tiktok is on the rise, but they are not really there yet.

88
00:06:23,880 --> 00:06:30,240
TikTok has, like all such apps, had problems with suspicions about security-

89
00:06:30,400 --> 00:06:34,320
-And how they use the data and the connection to China.

90
00:06:34,480 --> 00:06:39,440
In a country where mass surveillance is common, both on the street and internet-

91
00:06:39,600 --> 00:06:43,160
-Can you assume that this also applies to the data tictoch collects.

92
00:06:43,320 --> 00:06:48,040
It has been discovered that the app took advantage of a vulnerability in Apple's phones-

93
00:06:48,200 --> 00:06:53,680
-As has been able to read everything that the user has copied.

94
00:06:53,840 --> 00:06:58,680
Facebook has also had some about security and user data.

95
00:06:58,840 --> 00:07:04,280
My friend made a quiz called "This is your digitial life".

96
00:07:04,440 --> 00:07:09,280
It made them get hold of information about me-

97
00:07:09,440 --> 00:07:12,760
-When was used by Cambridge Analytica.

98
00:07:12,920 --> 00:07:18,160
This was a big betrayal. I apologize for happening.

99
00:07:18,320 --> 00:07:21,800
It is our responsibility to protect people's data.

100
00:07:21,960 --> 00:07:25,520
Russian hackers tried to influence the American election.

101
00:07:25,680 --> 00:07:31,360
A Russian impact campaign approved at the highest level.

102
00:07:31,520 --> 00:07:38,440
You say you can get into my Facebook account.

103
00:07:38,600 --> 00:07:42,280
And there we have your Facebook.

104
00:07:42,440 --> 00:07:48,440
Facebook and TikTok are not the only ones who have these problems.

105
00:07:48,600 --> 00:07:52,560
Like Clubhouse, the new app where you talk to each other.

106
00:07:52,720 --> 00:07:58,280
There you have wondered how the app uses your contacts.

107
00:07:58,440 --> 00:08:00,080
The app is Invite Only.

108
00:08:00,240 --> 00:08:05,160
To invite someone, you must give Clubhouse access to the phone book.

109
00:08:05,320 --> 00:08:11,000
Clubhouse went out this week and said that you can take out your contacts now.

110
00:08:11,160 --> 00:08:16,680
It is a generational issue. It depends on the platform you are on.

111
00:08:16,840 --> 00:08:21,640
A survey on Swedes' internet habits was conducted last year.

112
00:08:21,800 --> 00:08:26,680
40 % of those who answered, who were students had tested Tiktok.

113
00:08:26,840 --> 00:08:33,320
Compare that with 12 % of those who work and 3 % of pensioners.

114
00:08:33,480 --> 00:08:36,720
Big difference! Instagram and snapchat-

115
00:08:36,880 --> 00:08:43,440
-As much more common than Facebook in the group 16- to 25-year-olds.

116
00:08:43,600 --> 00:08:48,720
Then something else will come. Who is it old then?

117
00:08:48,880 --> 00:08:52,200
-It's still us. -Do not worry.

118
00:08:52,360 --> 00:08:56,200
There will be some social pedometers you can entertain with.

119
00:08:56,360 --> 00:08:58,120
I checked with people.

120
00:08:58,280 --> 00:09:02,640
Instagram, tiktok, snapchat about it likes.

121
00:09:02,800 --> 00:09:07,240
It's Instagram and Snapchat. It's the ones I use.

122
00:09:07,400 --> 00:09:10,600
It is not much. We are elderly.

123
00:09:10,760 --> 00:09:14,680
Instagram I run a lot, both for work and privately.

124
00:09:14,840 --> 00:09:19,240
I have also started with Tiktok, work -related.

125
00:09:19,400 --> 00:09:23,400
-What do you know? -Facebook. Basically.

126
00:09:23,560 --> 00:09:28,400
Social media is fantastic, but there are some problems.

127
00:09:28,560 --> 00:09:33,560
Yes, the younger generation is tired of everything being screamed out.

128
00:09:33,720 --> 00:09:37,720
Now you want to be in private rooms and share things with each other.

129
00:09:37,880 --> 00:09:42,600
You can say this, social media companies have built system-

130
00:09:42,760 --> 00:09:48,480
-O which are created to target advertising as accurately as possible.

131
00:09:48,640 --> 00:09:54,400
Yes Yes. And we users, we are pretty much on our stuff.

132
00:09:54,560 --> 00:09:57,920
We post pictures of what we are doing.

133
00:09:58,080 --> 00:10:01,040
There is a risk that it will be exploited.

134
00:10:04,160 --> 00:10:05,880
What is going on?

135
00:10:06,040 --> 00:10:10,760
I'm big in New Zealand. The third largest account.

136
00:10:10,920 --> 00:10:14,120
Or, I was the third largest account.

137
00:10:14,280 --> 00:10:18,120
The account for the bird Hector has required a cleansing of bottoms.

138
00:10:18,280 --> 00:10:20,800
Whatever? So?

139
00:10:20,960 --> 00:10:25,680
This means that thousands of my safaris were bottoms!

140
00:10:28,200 --> 00:10:31,680
It's a disaster! Why does this happen to me?

141
00:10:31,840 --> 00:10:35,000
It may have to do with that company.

142
00:10:35,160 --> 00:10:39,720
Perfect Beautiful Amazing Digital Media AB?

143
00:10:39,880 --> 00:10:45,800
Have they boosted my followers so that I should seem like a bigger star?

144
00:10:45,960 --> 00:10:51,960
So that they can use me as advertising, like their items girl? Oh my God!

145
00:10:52,120 --> 00:10:59,000
It has happened before that followers have been revealed as fake.

146
00:10:59,160 --> 00:11:03,240
You can make quite healthy if you want.

147
00:11:03,520 --> 00:11:08,840
Buy tiktok-likes.

148
00:11:09,000 --> 00:11:13,640
But they can be used for more everyday things.

149
00:11:13,800 --> 00:11:20,640
In Uppsala, it was booking bottoms that stole and sold further training times.

150
00:11:20,800 --> 00:11:24,040
What we see is private individuals-

151
00:11:24,200 --> 00:11:29,120
-Or has built bottoms, "script", which monitors our site.

152
00:11:29,280 --> 00:11:34,160
When new times are released, they are booked faster than a man can.

153
00:11:34,320 --> 00:11:36,320
Bottar is a weapon with.

154
00:11:36,480 --> 00:11:42,000
Countries that affect each other through propaganda can use them as weapons.

155
00:11:42,160 --> 00:11:46,600
What has happened to you may have to do with purchased fake followers.

156
00:11:46,760 --> 00:11:52,200
Purchased users? I have to go and crisis!

157
00:11:53,240 --> 00:11:56,080
It can be a bug. Right?

158
00:11:56,240 --> 00:12:01,640
Yes, like on Twitter when people were blocked because they used the word "Memphis"?

159
00:12:01,800 --> 00:12:06,720
Accounts that used the word "Memphis" have been limited due to a bug.

160
00:12:06,880 --> 00:12:10,160
It is something. Hope it is.

161
00:12:10,320 --> 00:12:17,480
-Can you make a summary piece? -How are we going to do it? We solve it.

162
00:12:17,640 --> 00:12:20,360
Bot comes from the word robot.

163
00:12:20,520 --> 00:12:27,360
It is a computer program to do automated data-

164
00:12:27,520 --> 00:12:33,240
-Att from chatting in a customer service to spreading propaganda.

165
00:12:33,400 --> 00:12:36,800
It depends on what it is programmed for.

166
00:12:36,960 --> 00:12:41,080
When trying to make it look like you have a lot of followers-

167
00:12:41,240 --> 00:12:44,840
-Is, for example, purchased likes or bottom-

168
00:12:45,000 --> 00:12:50,040
-So you want it to look like you naturally have those followers.

169
00:12:50,200 --> 00:12:56,920
Then you deceive the algorithm that you are more interesting than you are.

170
00:12:57,080 --> 00:12:59,440
What, then, is an algorithm?

171
00:12:59,600 --> 00:13:03,560
An algorithm is like a recipe that has been created-

172
00:13:03,720 --> 00:13:08,680
-To tell the computer how it should act in different situations.

173
00:13:08,840 --> 00:13:13,120
If it is a matter of how your social media feed looks out-

174
00:13:13,280 --> 00:13:16,640
-What will come at the top and prioritized-

175
00:13:16,800 --> 00:13:22,000
-To it is based on an algorithm that is a recipe for you.

176
00:13:22,160 --> 00:13:27,720
When it comes to your feed then it is a recipe that applies to you.

177
00:13:27,880 --> 00:13:33,360
That recipe has been built up by what you have previously done on the platform.

178
00:13:33,520 --> 00:13:37,320
How you have interacted and things you've liked.

179
00:13:37,480 --> 00:13:40,320
The companies want your details-

180
00:13:40,480 --> 00:13:44,000
-To be able to direct advertising but also other things.

181
00:13:44,160 --> 00:13:46,760
We offer a lot of information.

182
00:13:46,920 --> 00:13:51,320
We offer what we post, pictures, personal information ...

183
00:13:51,480 --> 00:13:57,080
Basically, algorithms are programming and can be used for a lot of things.

184
00:13:57,240 --> 00:14:01,880
-All right! It went well. -Yes, he may have points anyway.

185
00:14:02,040 --> 00:14:08,280
Perhaps. Anyway, we must try to help him with his loss of loss.

186
00:14:08,440 --> 00:14:11,480
You mean "bot-case"?

187
00:14:21,320 --> 00:14:25,320
Swedish texting: Iyuno Media Group for UR